{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pdfs/sample.pdf\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "# How many pages?\n",
    "print(len(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pdf[100].split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToSentences(raw_text, nlp):\n",
    "    doc = nlp(raw_text)\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "paragraphs = [p for p in text.split('\\n\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = []\n",
    "for p in paragraphs:\n",
    "    sens += splitToSentences(p, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7857"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "import time\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('models/tanda_roberta_large_asnq_wikiqa/ckpt/')\n",
    "model = RobertaForSequenceClassification.from_pretrained('models/tanda_roberta_large_asnq_wikiqa/ckpt/')\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLossAndProbas(q, a):\n",
    "    try:\n",
    "        inputs = tokenizer.encode_plus(q, a, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        probas = torch.sigmoid(logits).detach().numpy()\n",
    "        return loss.detach().numpy(), probas[0, 1]\n",
    "    except:\n",
    "        print('too long answer')\n",
    "        return 10, 0\n",
    "\n",
    "def findBestAnswer(sens, question, nlpmodel, part_length=500):\n",
    "    print('Working on question: {} ...'.format(question))\n",
    "    st = time.time()\n",
    "    res = []\n",
    "    sm = 0\n",
    "    j = 0\n",
    "    end = False\n",
    "    for i, s in enumerate(sens):\n",
    "        l, p = getLossAndProbas(question, sens)\n",
    "        if l < 2.0:\n",
    "            res.append([i, j, l, p])\n",
    "        print('processing phrase {} - loss {:.2f} - proba {:.2f}'.format(i, l, p))\n",
    "    print('Time lapse {:.2f}'.format(time.time() - st))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on question: what is topic modeling? ...\n",
      "processing phrase 0 - loss 4.86 - proba 0.08\n",
      "processing phrase 1 - loss 4.77 - proba 0.09\n",
      "processing phrase 2 - loss 5.48 - proba 0.06\n",
      "processing phrase 3 - loss 5.34 - proba 0.07\n",
      "processing phrase 4 - loss 4.90 - proba 0.08\n",
      "processing phrase 5 - loss 5.18 - proba 0.07\n",
      "processing phrase 6 - loss 6.67 - proba 0.04\n",
      "processing phrase 7 - loss 5.55 - proba 0.06\n",
      "processing phrase 8 - loss 4.77 - proba 0.09\n",
      "processing phrase 9 - loss 4.30 - proba 0.11\n",
      "processing phrase 10 - loss 3.48 - proba 0.15\n",
      "processing phrase 11 - loss 3.31 - proba 0.15\n",
      "processing phrase 12 - loss 3.18 - proba 0.16\n",
      "processing phrase 13 - loss 3.20 - proba 0.16\n",
      "processing phrase 14 - loss 3.05 - proba 0.17\n",
      "processing phrase 15 - loss 3.06 - proba 0.17\n",
      "processing phrase 16 - loss 2.40 - proba 0.21\n",
      "processing phrase 17 - loss 2.50 - proba 0.21\n",
      "processing phrase 18 - loss 2.85 - proba 0.18\n",
      "processing phrase 19 - loss 2.95 - proba 0.17\n",
      "processing phrase 20 - loss 3.53 - proba 0.14\n",
      "processing phrase 21 - loss 2.72 - proba 0.19\n",
      "processing phrase 22 - loss 2.36 - proba 0.22\n",
      "processing phrase 23 - loss 2.81 - proba 0.18\n",
      "processing phrase 24 - loss 2.48 - proba 0.20\n",
      "processing phrase 25 - loss 2.76 - proba 0.18\n",
      "processing phrase 26 - loss 3.23 - proba 0.16\n",
      "processing phrase 27 - loss 2.71 - proba 0.19\n",
      "processing phrase 28 - loss 2.81 - proba 0.18\n",
      "processing phrase 29 - loss 2.40 - proba 0.21\n",
      "processing phrase 30 - loss 2.39 - proba 0.21\n",
      "processing phrase 31 - loss 2.22 - proba 0.23\n",
      "processing phrase 32 - loss 2.80 - proba 0.18\n",
      "processing phrase 33 - loss 2.76 - proba 0.19\n",
      "processing phrase 34 - loss 2.86 - proba 0.18\n",
      "processing phrase 35 - loss 2.94 - proba 0.18\n",
      "processing phrase 36 - loss 3.05 - proba 0.17\n",
      "processing phrase 37 - loss 3.06 - proba 0.17\n",
      "processing phrase 38 - loss 3.09 - proba 0.17\n",
      "processing phrase 39 - loss 2.98 - proba 0.17\n",
      "processing phrase 40 - loss 3.31 - proba 0.15\n",
      "processing phrase 41 - loss 3.51 - proba 0.14\n",
      "processing phrase 42 - loss 3.10 - proba 0.17\n",
      "processing phrase 43 - loss 3.25 - proba 0.16\n",
      "processing phrase 44 - loss 3.41 - proba 0.15\n",
      "processing phrase 45 - loss 2.94 - proba 0.18\n",
      "processing phrase 46 - loss 2.78 - proba 0.19\n",
      "processing phrase 47 - loss 2.71 - proba 0.19\n",
      "processing phrase 48 - loss 2.86 - proba 0.18\n",
      "processing phrase 49 - loss 2.98 - proba 0.17\n",
      "processing phrase 50 - loss 2.77 - proba 0.19\n",
      "processing phrase 51 - loss 2.67 - proba 0.19\n",
      "processing phrase 52 - loss 2.68 - proba 0.19\n",
      "processing phrase 53 - loss 2.94 - proba 0.17\n",
      "processing phrase 54 - loss 2.80 - proba 0.18\n",
      "processing phrase 55 - loss 3.17 - proba 0.16\n",
      "processing phrase 56 - loss 2.92 - proba 0.18\n",
      "processing phrase 57 - loss 2.99 - proba 0.17\n",
      "processing phrase 58 - loss 3.27 - proba 0.16\n",
      "processing phrase 59 - loss 2.38 - proba 0.22\n",
      "processing phrase 60 - loss 3.47 - proba 0.15\n",
      "processing phrase 61 - loss 3.43 - proba 0.15\n",
      "processing phrase 62 - loss 3.71 - proba 0.13\n",
      "processing phrase 63 - loss 3.31 - proba 0.16\n",
      "processing phrase 64 - loss 3.58 - proba 0.14\n",
      "processing phrase 65 - loss 3.23 - proba 0.16\n",
      "processing phrase 66 - loss 3.60 - proba 0.14\n",
      "processing phrase 67 - loss 3.07 - proba 0.17\n",
      "processing phrase 68 - loss 3.68 - proba 0.14\n",
      "processing phrase 69 - loss 3.41 - proba 0.15\n",
      "processing phrase 70 - loss 3.34 - proba 0.15\n",
      "processing phrase 71 - loss 3.11 - proba 0.17\n",
      "processing phrase 72 - loss 3.18 - proba 0.16\n",
      "processing phrase 73 - loss 3.14 - proba 0.16\n",
      "processing phrase 74 - loss 3.15 - proba 0.16\n",
      "processing phrase 75 - loss 3.23 - proba 0.16\n",
      "processing phrase 76 - loss 3.09 - proba 0.17\n",
      "processing phrase 77 - loss 3.14 - proba 0.17\n",
      "processing phrase 78 - loss 3.05 - proba 0.17\n",
      "processing phrase 79 - loss 3.18 - proba 0.16\n",
      "processing phrase 80 - loss 3.38 - proba 0.15\n",
      "processing phrase 81 - loss 3.18 - proba 0.16\n",
      "processing phrase 82 - loss 3.61 - proba 0.14\n",
      "processing phrase 83 - loss 3.73 - proba 0.13\n",
      "processing phrase 84 - loss 3.81 - proba 0.13\n",
      "processing phrase 85 - loss 3.51 - proba 0.15\n",
      "processing phrase 86 - loss 3.58 - proba 0.14\n",
      "processing phrase 87 - loss 3.45 - proba 0.15\n",
      "processing phrase 88 - loss 3.09 - proba 0.17\n",
      "processing phrase 89 - loss 3.50 - proba 0.15\n",
      "processing phrase 90 - loss 3.46 - proba 0.15\n",
      "processing phrase 91 - loss 3.23 - proba 0.16\n",
      "processing phrase 92 - loss 3.39 - proba 0.15\n",
      "processing phrase 93 - loss 3.38 - proba 0.15\n",
      "processing phrase 94 - loss 3.50 - proba 0.15\n",
      "processing phrase 95 - loss 3.16 - proba 0.16\n",
      "processing phrase 96 - loss 3.12 - proba 0.17\n",
      "processing phrase 97 - loss 3.79 - proba 0.13\n",
      "processing phrase 98 - loss 3.38 - proba 0.15\n",
      "processing phrase 99 - loss 3.26 - proba 0.16\n",
      "processing phrase 100 - loss 3.41 - proba 0.15\n",
      "processing phrase 101 - loss 3.38 - proba 0.15\n",
      "processing phrase 102 - loss 3.28 - proba 0.16\n",
      "processing phrase 103 - loss 3.35 - proba 0.15\n",
      "too long answer\n",
      "processing phrase 104 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 105 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 106 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 107 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 108 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 109 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 110 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 111 - loss 10.00 - proba 0.00\n",
      "processing phrase 112 - loss 3.25 - proba 0.16\n",
      "processing phrase 113 - loss 2.98 - proba 0.17\n",
      "processing phrase 114 - loss 3.06 - proba 0.17\n",
      "processing phrase 115 - loss 3.35 - proba 0.15\n",
      "processing phrase 116 - loss 3.20 - proba 0.16\n",
      "processing phrase 117 - loss 3.25 - proba 0.16\n",
      "processing phrase 118 - loss 3.13 - proba 0.16\n",
      "processing phrase 119 - loss 2.35 - proba 0.22\n",
      "processing phrase 120 - loss 2.58 - proba 0.20\n",
      "processing phrase 121 - loss 2.27 - proba 0.22\n",
      "processing phrase 122 - loss 2.95 - proba 0.18\n",
      "processing phrase 123 - loss 2.53 - proba 0.20\n",
      "processing phrase 124 - loss 2.68 - proba 0.19\n",
      "processing phrase 125 - loss 2.85 - proba 0.18\n",
      "processing phrase 126 - loss 2.95 - proba 0.18\n",
      "processing phrase 127 - loss 2.75 - proba 0.19\n",
      "processing phrase 128 - loss 2.23 - proba 0.23\n",
      "processing phrase 129 - loss 2.46 - proba 0.21\n",
      "processing phrase 130 - loss 2.45 - proba 0.21\n",
      "processing phrase 131 - loss 2.38 - proba 0.22\n",
      "processing phrase 132 - loss 2.44 - proba 0.21\n",
      "processing phrase 133 - loss 3.15 - proba 0.16\n",
      "processing phrase 134 - loss 3.22 - proba 0.16\n",
      "processing phrase 135 - loss 3.12 - proba 0.17\n",
      "processing phrase 136 - loss 3.02 - proba 0.17\n",
      "processing phrase 137 - loss 2.46 - proba 0.21\n",
      "processing phrase 138 - loss 2.66 - proba 0.19\n",
      "processing phrase 139 - loss 2.91 - proba 0.18\n",
      "processing phrase 140 - loss 3.23 - proba 0.16\n",
      "processing phrase 141 - loss 2.86 - proba 0.18\n",
      "processing phrase 142 - loss 2.24 - proba 0.23\n",
      "processing phrase 143 - loss 1.95 - proba 0.25\n",
      "processing phrase 144 - loss 2.02 - proba 0.25\n",
      "processing phrase 145 - loss 2.18 - proba 0.23\n",
      "processing phrase 146 - loss 1.76 - proba 0.28\n",
      "processing phrase 147 - loss 2.41 - proba 0.22\n",
      "processing phrase 148 - loss 3.11 - proba 0.17\n",
      "processing phrase 149 - loss 2.78 - proba 0.19\n",
      "processing phrase 150 - loss 2.83 - proba 0.18\n",
      "processing phrase 151 - loss 2.97 - proba 0.18\n",
      "processing phrase 152 - loss 2.86 - proba 0.18\n",
      "processing phrase 153 - loss 3.00 - proba 0.17\n",
      "processing phrase 154 - loss 2.64 - proba 0.20\n",
      "processing phrase 155 - loss 2.96 - proba 0.18\n",
      "processing phrase 156 - loss 3.15 - proba 0.16\n",
      "processing phrase 157 - loss 3.29 - proba 0.16\n",
      "processing phrase 158 - loss 3.68 - proba 0.14\n",
      "processing phrase 159 - loss 3.28 - proba 0.16\n",
      "processing phrase 160 - loss 3.12 - proba 0.17\n",
      "processing phrase 161 - loss 3.29 - proba 0.16\n",
      "processing phrase 162 - loss 3.28 - proba 0.16\n",
      "processing phrase 163 - loss 3.62 - proba 0.14\n",
      "too long answer\n",
      "processing phrase 164 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 165 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 166 - loss 10.00 - proba 0.00\n",
      "processing phrase 167 - loss 1.89 - proba 0.26\n",
      "processing phrase 168 - loss 1.62 - proba 0.29\n",
      "processing phrase 169 - loss 2.43 - proba 0.21\n",
      "processing phrase 170 - loss 2.50 - proba 0.21\n",
      "too long answer\n",
      "processing phrase 171 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 172 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 173 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 174 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 175 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 176 - loss 10.00 - proba 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing phrase 177 - loss 3.07 - proba 0.17\n",
      "processing phrase 178 - loss 3.87 - proba 0.13\n",
      "processing phrase 179 - loss 3.68 - proba 0.14\n",
      "processing phrase 180 - loss 2.81 - proba 0.18\n",
      "processing phrase 181 - loss 2.95 - proba 0.18\n",
      "processing phrase 182 - loss 2.86 - proba 0.18\n",
      "processing phrase 183 - loss 2.69 - proba 0.19\n",
      "processing phrase 184 - loss 3.05 - proba 0.17\n",
      "processing phrase 185 - loss 2.46 - proba 0.21\n",
      "processing phrase 186 - loss 2.15 - proba 0.23\n",
      "processing phrase 187 - loss 1.50 - proba 0.30\n",
      "processing phrase 188 - loss 1.77 - proba 0.27\n",
      "processing phrase 189 - loss 1.79 - proba 0.27\n",
      "processing phrase 190 - loss 1.91 - proba 0.26\n",
      "processing phrase 191 - loss 2.12 - proba 0.24\n",
      "processing phrase 192 - loss 2.11 - proba 0.24\n",
      "processing phrase 193 - loss 2.07 - proba 0.24\n",
      "processing phrase 194 - loss 2.43 - proba 0.21\n",
      "processing phrase 195 - loss 2.67 - proba 0.19\n",
      "processing phrase 196 - loss 3.19 - proba 0.16\n",
      "processing phrase 197 - loss 3.43 - proba 0.15\n",
      "processing phrase 198 - loss 3.40 - proba 0.15\n",
      "processing phrase 199 - loss 4.07 - proba 0.12\n",
      "processing phrase 200 - loss 4.04 - proba 0.12\n",
      "processing phrase 201 - loss 4.18 - proba 0.11\n",
      "processing phrase 202 - loss 4.82 - proba 0.09\n",
      "processing phrase 203 - loss 6.81 - proba 0.04\n",
      "processing phrase 204 - loss 3.07 - proba 0.17\n",
      "processing phrase 205 - loss 3.51 - proba 0.14\n",
      "processing phrase 206 - loss 2.88 - proba 0.18\n",
      "processing phrase 207 - loss 3.16 - proba 0.16\n",
      "processing phrase 208 - loss 3.49 - proba 0.14\n",
      "processing phrase 209 - loss 3.41 - proba 0.15\n",
      "processing phrase 210 - loss 3.23 - proba 0.16\n",
      "processing phrase 211 - loss 3.10 - proba 0.17\n",
      "processing phrase 212 - loss 2.82 - proba 0.18\n",
      "processing phrase 213 - loss 2.74 - proba 0.19\n",
      "processing phrase 214 - loss 3.57 - proba 0.14\n",
      "processing phrase 215 - loss 3.98 - proba 0.12\n",
      "processing phrase 216 - loss 4.25 - proba 0.11\n",
      "processing phrase 217 - loss 4.12 - proba 0.11\n",
      "processing phrase 218 - loss 4.52 - proba 0.10\n",
      "processing phrase 219 - loss 4.29 - proba 0.11\n",
      "processing phrase 220 - loss 4.38 - proba 0.10\n",
      "processing phrase 221 - loss 4.27 - proba 0.11\n",
      "processing phrase 222 - loss 3.84 - proba 0.13\n",
      "processing phrase 223 - loss 4.64 - proba 0.09\n",
      "processing phrase 224 - loss 4.31 - proba 0.10\n",
      "processing phrase 225 - loss 3.75 - proba 0.13\n",
      "processing phrase 226 - loss 4.62 - proba 0.09\n",
      "processing phrase 227 - loss 3.80 - proba 0.13\n",
      "processing phrase 228 - loss 4.11 - proba 0.11\n",
      "processing phrase 229 - loss 3.78 - proba 0.13\n",
      "processing phrase 230 - loss 3.40 - proba 0.15\n",
      "processing phrase 231 - loss 3.08 - proba 0.16\n",
      "processing phrase 232 - loss 3.07 - proba 0.17\n",
      "processing phrase 233 - loss 4.22 - proba 0.11\n",
      "processing phrase 234 - loss 4.41 - proba 0.10\n",
      "processing phrase 235 - loss 4.42 - proba 0.10\n",
      "processing phrase 236 - loss 5.05 - proba 0.08\n",
      "processing phrase 237 - loss 5.21 - proba 0.07\n",
      "processing phrase 238 - loss 6.20 - proba 0.05\n",
      "processing phrase 239 - loss 6.27 - proba 0.04\n",
      "processing phrase 240 - loss 7.73 - proba 0.02\n",
      "processing phrase 241 - loss 7.10 - proba 0.03\n",
      "processing phrase 242 - loss 5.54 - proba 0.06\n",
      "processing phrase 243 - loss 5.43 - proba 0.07\n",
      "processing phrase 244 - loss 5.68 - proba 0.06\n",
      "processing phrase 245 - loss 4.79 - proba 0.09\n",
      "processing phrase 246 - loss 4.06 - proba 0.12\n",
      "processing phrase 247 - loss 4.62 - proba 0.09\n",
      "processing phrase 248 - loss 4.14 - proba 0.11\n",
      "processing phrase 249 - loss 4.09 - proba 0.11\n",
      "processing phrase 250 - loss 4.58 - proba 0.09\n",
      "processing phrase 251 - loss 4.63 - proba 0.09\n",
      "processing phrase 252 - loss 3.24 - proba 0.16\n",
      "processing phrase 253 - loss 4.02 - proba 0.12\n",
      "processing phrase 254 - loss 3.27 - proba 0.16\n",
      "processing phrase 255 - loss 2.98 - proba 0.17\n",
      "processing phrase 256 - loss 3.47 - proba 0.14\n",
      "processing phrase 257 - loss 3.08 - proba 0.16\n",
      "processing phrase 258 - loss 4.08 - proba 0.11\n",
      "processing phrase 259 - loss 3.61 - proba 0.14\n",
      "processing phrase 260 - loss 5.12 - proba 0.07\n",
      "processing phrase 261 - loss 4.32 - proba 0.10\n",
      "processing phrase 262 - loss 4.32 - proba 0.10\n",
      "processing phrase 263 - loss 4.14 - proba 0.11\n",
      "processing phrase 264 - loss 4.08 - proba 0.11\n",
      "processing phrase 265 - loss 4.23 - proba 0.11\n",
      "processing phrase 266 - loss 4.54 - proba 0.09\n",
      "processing phrase 267 - loss 6.35 - proba 0.04\n",
      "processing phrase 268 - loss 5.90 - proba 0.05\n",
      "processing phrase 269 - loss 3.83 - proba 0.12\n",
      "processing phrase 270 - loss 4.34 - proba 0.10\n",
      "processing phrase 271 - loss 3.62 - proba 0.14\n",
      "processing phrase 272 - loss 5.45 - proba 0.06\n",
      "processing phrase 273 - loss 5.36 - proba 0.07\n",
      "processing phrase 274 - loss 5.78 - proba 0.06\n",
      "processing phrase 275 - loss 6.54 - proba 0.04\n",
      "processing phrase 276 - loss 5.68 - proba 0.06\n",
      "processing phrase 277 - loss 5.00 - proba 0.08\n",
      "processing phrase 278 - loss 4.66 - proba 0.09\n",
      "processing phrase 279 - loss 3.95 - proba 0.12\n",
      "processing phrase 280 - loss 3.81 - proba 0.13\n",
      "processing phrase 281 - loss 3.55 - proba 0.14\n",
      "processing phrase 282 - loss 4.39 - proba 0.10\n",
      "processing phrase 283 - loss 4.13 - proba 0.11\n",
      "processing phrase 284 - loss 5.37 - proba 0.07\n",
      "processing phrase 285 - loss 4.91 - proba 0.08\n",
      "processing phrase 286 - loss 4.81 - proba 0.08\n",
      "processing phrase 287 - loss 5.50 - proba 0.06\n",
      "processing phrase 288 - loss 5.27 - proba 0.07\n",
      "processing phrase 289 - loss 5.67 - proba 0.06\n",
      "processing phrase 290 - loss 4.72 - proba 0.09\n",
      "processing phrase 291 - loss 5.21 - proba 0.07\n",
      "processing phrase 292 - loss 5.77 - proba 0.06\n",
      "processing phrase 293 - loss 5.28 - proba 0.07\n",
      "processing phrase 294 - loss 5.26 - proba 0.07\n",
      "processing phrase 295 - loss 5.35 - proba 0.07\n",
      "processing phrase 296 - loss 4.64 - proba 0.09\n",
      "processing phrase 297 - loss 5.47 - proba 0.06\n",
      "processing phrase 298 - loss 4.45 - proba 0.10\n",
      "processing phrase 299 - loss 3.84 - proba 0.12\n",
      "processing phrase 300 - loss 3.03 - proba 0.17\n",
      "processing phrase 301 - loss 2.06 - proba 0.24\n",
      "processing phrase 302 - loss 1.48 - proba 0.30\n",
      "processing phrase 303 - loss 0.60 - proba 0.45\n",
      "processing phrase 304 - loss 0.33 - proba 0.55\n",
      "processing phrase 305 - loss 2.52 - proba 0.20\n",
      "processing phrase 306 - loss 2.99 - proba 0.17\n",
      "processing phrase 307 - loss 3.46 - proba 0.15\n",
      "processing phrase 308 - loss 4.69 - proba 0.09\n",
      "processing phrase 309 - loss 4.38 - proba 0.10\n",
      "processing phrase 310 - loss 3.27 - proba 0.16\n",
      "processing phrase 311 - loss 2.72 - proba 0.19\n",
      "processing phrase 312 - loss 4.02 - proba 0.12\n",
      "processing phrase 313 - loss 3.52 - proba 0.14\n",
      "processing phrase 314 - loss 3.68 - proba 0.13\n",
      "processing phrase 315 - loss 3.84 - proba 0.13\n",
      "processing phrase 316 - loss 3.78 - proba 0.13\n",
      "processing phrase 317 - loss 3.11 - proba 0.16\n",
      "processing phrase 318 - loss 3.17 - proba 0.16\n",
      "processing phrase 319 - loss 3.77 - proba 0.13\n",
      "processing phrase 320 - loss 4.26 - proba 0.11\n",
      "processing phrase 321 - loss 3.68 - proba 0.13\n",
      "processing phrase 322 - loss 4.04 - proba 0.12\n",
      "processing phrase 323 - loss 3.61 - proba 0.14\n",
      "processing phrase 324 - loss 3.74 - proba 0.13\n",
      "processing phrase 325 - loss 3.51 - proba 0.14\n",
      "processing phrase 326 - loss 4.18 - proba 0.11\n",
      "processing phrase 327 - loss 3.90 - proba 0.12\n",
      "processing phrase 328 - loss 3.55 - proba 0.14\n",
      "processing phrase 329 - loss 3.19 - proba 0.16\n",
      "processing phrase 330 - loss 2.80 - proba 0.18\n",
      "processing phrase 331 - loss 3.42 - proba 0.15\n",
      "processing phrase 332 - loss 3.63 - proba 0.14\n",
      "processing phrase 333 - loss 3.49 - proba 0.14\n",
      "processing phrase 334 - loss 4.27 - proba 0.11\n",
      "processing phrase 335 - loss 3.24 - proba 0.16\n",
      "processing phrase 336 - loss 3.89 - proba 0.12\n",
      "processing phrase 337 - loss 3.61 - proba 0.14\n",
      "processing phrase 338 - loss 2.25 - proba 0.22\n",
      "processing phrase 339 - loss 2.29 - proba 0.22\n",
      "processing phrase 340 - loss 2.57 - proba 0.20\n",
      "processing phrase 341 - loss 2.69 - proba 0.19\n",
      "processing phrase 342 - loss 2.79 - proba 0.18\n",
      "processing phrase 343 - loss 3.17 - proba 0.16\n",
      "processing phrase 344 - loss 3.24 - proba 0.16\n",
      "processing phrase 345 - loss 3.33 - proba 0.15\n",
      "processing phrase 346 - loss 3.42 - proba 0.15\n",
      "processing phrase 347 - loss 3.36 - proba 0.15\n",
      "processing phrase 348 - loss 3.32 - proba 0.15\n",
      "processing phrase 349 - loss 2.72 - proba 0.19\n",
      "processing phrase 350 - loss 1.98 - proba 0.25\n",
      "processing phrase 351 - loss 2.67 - proba 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing phrase 352 - loss 3.28 - proba 0.16\n",
      "processing phrase 353 - loss 3.06 - proba 0.17\n",
      "processing phrase 354 - loss 3.50 - proba 0.14\n",
      "processing phrase 355 - loss 3.61 - proba 0.14\n",
      "processing phrase 356 - loss 4.00 - proba 0.12\n",
      "processing phrase 357 - loss 3.67 - proba 0.14\n",
      "processing phrase 358 - loss 3.02 - proba 0.17\n",
      "processing phrase 359 - loss 2.97 - proba 0.17\n",
      "processing phrase 360 - loss 3.03 - proba 0.17\n",
      "processing phrase 361 - loss 3.38 - proba 0.15\n",
      "processing phrase 362 - loss 4.51 - proba 0.10\n",
      "processing phrase 363 - loss 4.48 - proba 0.10\n",
      "processing phrase 364 - loss 4.65 - proba 0.09\n",
      "processing phrase 365 - loss 4.18 - proba 0.11\n",
      "processing phrase 366 - loss 4.60 - proba 0.09\n",
      "processing phrase 367 - loss 4.05 - proba 0.12\n",
      "processing phrase 368 - loss 4.09 - proba 0.12\n",
      "processing phrase 369 - loss 3.98 - proba 0.12\n",
      "processing phrase 370 - loss 4.08 - proba 0.11\n",
      "processing phrase 371 - loss 3.91 - proba 0.12\n",
      "processing phrase 372 - loss 3.04 - proba 0.17\n",
      "processing phrase 373 - loss 3.10 - proba 0.16\n",
      "processing phrase 374 - loss 2.92 - proba 0.17\n",
      "processing phrase 375 - loss 3.45 - proba 0.15\n",
      "processing phrase 376 - loss 3.41 - proba 0.15\n",
      "processing phrase 377 - loss 2.57 - proba 0.20\n",
      "processing phrase 378 - loss 2.47 - proba 0.21\n",
      "processing phrase 379 - loss 2.71 - proba 0.19\n",
      "processing phrase 380 - loss 2.58 - proba 0.20\n",
      "processing phrase 381 - loss 2.63 - proba 0.19\n",
      "processing phrase 382 - loss 2.52 - proba 0.20\n",
      "processing phrase 383 - loss 3.14 - proba 0.16\n",
      "processing phrase 384 - loss 3.73 - proba 0.13\n",
      "processing phrase 385 - loss 3.52 - proba 0.14\n",
      "processing phrase 386 - loss 4.26 - proba 0.11\n",
      "processing phrase 387 - loss 4.13 - proba 0.11\n",
      "processing phrase 388 - loss 4.00 - proba 0.12\n",
      "processing phrase 389 - loss 2.89 - proba 0.18\n",
      "processing phrase 390 - loss 2.84 - proba 0.18\n",
      "processing phrase 391 - loss 2.88 - proba 0.18\n",
      "processing phrase 392 - loss 2.97 - proba 0.17\n",
      "processing phrase 393 - loss 2.92 - proba 0.17\n",
      "processing phrase 394 - loss 3.23 - proba 0.16\n",
      "processing phrase 395 - loss 3.64 - proba 0.14\n",
      "processing phrase 396 - loss 3.28 - proba 0.16\n",
      "processing phrase 397 - loss 3.12 - proba 0.16\n",
      "processing phrase 398 - loss 3.18 - proba 0.16\n",
      "processing phrase 399 - loss 3.23 - proba 0.16\n",
      "processing phrase 400 - loss 2.79 - proba 0.18\n",
      "processing phrase 401 - loss 2.88 - proba 0.18\n",
      "processing phrase 402 - loss 3.82 - proba 0.13\n",
      "processing phrase 403 - loss 2.68 - proba 0.19\n",
      "processing phrase 404 - loss 2.75 - proba 0.18\n",
      "processing phrase 405 - loss 3.77 - proba 0.13\n",
      "processing phrase 406 - loss 3.33 - proba 0.15\n",
      "processing phrase 407 - loss 3.93 - proba 0.12\n",
      "processing phrase 408 - loss 4.22 - proba 0.11\n",
      "processing phrase 409 - loss 4.47 - proba 0.10\n",
      "processing phrase 410 - loss 3.98 - proba 0.12\n",
      "processing phrase 411 - loss 3.76 - proba 0.13\n",
      "processing phrase 412 - loss 3.54 - proba 0.14\n",
      "processing phrase 413 - loss 3.77 - proba 0.13\n",
      "processing phrase 414 - loss 3.42 - proba 0.15\n",
      "processing phrase 415 - loss 3.26 - proba 0.16\n",
      "processing phrase 416 - loss 3.43 - proba 0.15\n",
      "processing phrase 417 - loss 3.45 - proba 0.15\n",
      "processing phrase 418 - loss 3.37 - proba 0.15\n",
      "processing phrase 419 - loss 4.01 - proba 0.12\n",
      "processing phrase 420 - loss 4.31 - proba 0.11\n",
      "processing phrase 421 - loss 4.80 - proba 0.09\n",
      "processing phrase 422 - loss 4.89 - proba 0.08\n",
      "processing phrase 423 - loss 4.84 - proba 0.08\n",
      "processing phrase 424 - loss 3.30 - proba 0.15\n",
      "processing phrase 425 - loss 3.67 - proba 0.13\n",
      "processing phrase 426 - loss 3.43 - proba 0.15\n",
      "processing phrase 427 - loss 3.29 - proba 0.15\n",
      "processing phrase 428 - loss 2.85 - proba 0.18\n",
      "processing phrase 429 - loss 4.29 - proba 0.10\n",
      "processing phrase 430 - loss 3.81 - proba 0.13\n",
      "processing phrase 431 - loss 3.62 - proba 0.14\n",
      "processing phrase 432 - loss 4.71 - proba 0.09\n",
      "processing phrase 433 - loss 4.34 - proba 0.10\n",
      "processing phrase 434 - loss 4.43 - proba 0.10\n",
      "processing phrase 435 - loss 4.09 - proba 0.11\n",
      "processing phrase 436 - loss 5.38 - proba 0.07\n",
      "processing phrase 437 - loss 4.55 - proba 0.09\n",
      "processing phrase 438 - loss 4.81 - proba 0.08\n",
      "processing phrase 439 - loss 4.91 - proba 0.08\n",
      "processing phrase 440 - loss 4.31 - proba 0.10\n",
      "processing phrase 441 - loss 4.69 - proba 0.09\n",
      "processing phrase 442 - loss 4.73 - proba 0.09\n",
      "processing phrase 443 - loss 4.00 - proba 0.12\n",
      "processing phrase 444 - loss 4.32 - proba 0.10\n",
      "processing phrase 445 - loss 5.38 - proba 0.07\n",
      "processing phrase 446 - loss 4.62 - proba 0.09\n",
      "processing phrase 447 - loss 4.59 - proba 0.09\n",
      "processing phrase 448 - loss 4.88 - proba 0.08\n",
      "processing phrase 449 - loss 3.73 - proba 0.13\n",
      "processing phrase 450 - loss 3.28 - proba 0.15\n",
      "processing phrase 451 - loss 3.58 - proba 0.14\n",
      "processing phrase 452 - loss 3.93 - proba 0.12\n",
      "processing phrase 453 - loss 3.36 - proba 0.15\n",
      "processing phrase 454 - loss 4.63 - proba 0.09\n",
      "processing phrase 455 - loss 4.24 - proba 0.11\n",
      "processing phrase 456 - loss 4.09 - proba 0.11\n",
      "processing phrase 457 - loss 3.72 - proba 0.13\n",
      "processing phrase 458 - loss 5.09 - proba 0.08\n",
      "processing phrase 459 - loss 3.91 - proba 0.12\n",
      "processing phrase 460 - loss 3.53 - proba 0.14\n",
      "processing phrase 461 - loss 4.63 - proba 0.09\n",
      "processing phrase 462 - loss 3.80 - proba 0.13\n",
      "processing phrase 463 - loss 4.40 - proba 0.10\n",
      "processing phrase 464 - loss 4.60 - proba 0.09\n",
      "processing phrase 465 - loss 4.92 - proba 0.08\n",
      "processing phrase 466 - loss 4.08 - proba 0.12\n",
      "processing phrase 467 - loss 3.18 - proba 0.16\n",
      "processing phrase 468 - loss 3.66 - proba 0.13\n",
      "processing phrase 469 - loss 3.56 - proba 0.14\n",
      "processing phrase 470 - loss 4.98 - proba 0.08\n",
      "processing phrase 471 - loss 3.91 - proba 0.12\n",
      "processing phrase 472 - loss 3.26 - proba 0.15\n",
      "processing phrase 473 - loss 3.37 - proba 0.15\n",
      "processing phrase 474 - loss 2.99 - proba 0.17\n",
      "processing phrase 475 - loss 3.27 - proba 0.16\n",
      "processing phrase 476 - loss 3.92 - proba 0.12\n",
      "processing phrase 477 - loss 4.25 - proba 0.11\n",
      "processing phrase 478 - loss 4.18 - proba 0.11\n",
      "processing phrase 479 - loss 4.15 - proba 0.11\n",
      "processing phrase 480 - loss 3.62 - proba 0.14\n",
      "processing phrase 481 - loss 2.96 - proba 0.17\n",
      "processing phrase 482 - loss 3.10 - proba 0.17\n",
      "processing phrase 483 - loss 2.21 - proba 0.23\n",
      "processing phrase 484 - loss 2.80 - proba 0.18\n",
      "processing phrase 485 - loss 2.69 - proba 0.19\n",
      "processing phrase 486 - loss 3.02 - proba 0.17\n",
      "processing phrase 487 - loss 3.63 - proba 0.14\n",
      "processing phrase 488 - loss 3.41 - proba 0.15\n",
      "processing phrase 489 - loss 3.38 - proba 0.15\n",
      "processing phrase 490 - loss 3.28 - proba 0.16\n",
      "processing phrase 491 - loss 2.87 - proba 0.18\n",
      "processing phrase 492 - loss 2.51 - proba 0.20\n",
      "processing phrase 493 - loss 2.49 - proba 0.20\n",
      "processing phrase 494 - loss 3.33 - proba 0.15\n",
      "processing phrase 495 - loss 3.34 - proba 0.15\n",
      "processing phrase 496 - loss 3.50 - proba 0.15\n",
      "processing phrase 497 - loss 4.63 - proba 0.09\n",
      "processing phrase 498 - loss 4.00 - proba 0.12\n",
      "processing phrase 499 - loss 4.03 - proba 0.12\n",
      "processing phrase 500 - loss 4.02 - proba 0.12\n",
      "processing phrase 501 - loss 4.48 - proba 0.10\n",
      "processing phrase 502 - loss 4.30 - proba 0.11\n",
      "processing phrase 503 - loss 4.23 - proba 0.11\n",
      "processing phrase 504 - loss 4.54 - proba 0.10\n",
      "processing phrase 505 - loss 4.85 - proba 0.08\n",
      "processing phrase 506 - loss 4.84 - proba 0.08\n",
      "processing phrase 507 - loss 4.71 - proba 0.09\n",
      "processing phrase 508 - loss 4.37 - proba 0.10\n",
      "processing phrase 509 - loss 4.41 - proba 0.10\n",
      "processing phrase 510 - loss 4.50 - proba 0.10\n",
      "processing phrase 511 - loss 4.16 - proba 0.11\n",
      "processing phrase 512 - loss 4.05 - proba 0.12\n",
      "processing phrase 513 - loss 3.84 - proba 0.13\n",
      "processing phrase 514 - loss 3.07 - proba 0.17\n",
      "processing phrase 515 - loss 3.42 - proba 0.15\n",
      "processing phrase 516 - loss 3.84 - proba 0.13\n",
      "processing phrase 517 - loss 3.70 - proba 0.13\n",
      "processing phrase 518 - loss 3.58 - proba 0.14\n",
      "processing phrase 519 - loss 3.03 - proba 0.17\n",
      "processing phrase 520 - loss 2.90 - proba 0.18\n",
      "processing phrase 521 - loss 3.45 - proba 0.15\n",
      "processing phrase 522 - loss 3.53 - proba 0.14\n",
      "processing phrase 523 - loss 3.64 - proba 0.13\n",
      "processing phrase 524 - loss 3.48 - proba 0.14\n",
      "processing phrase 525 - loss 2.92 - proba 0.17\n",
      "processing phrase 526 - loss 2.56 - proba 0.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing phrase 527 - loss 2.11 - proba 0.23\n",
      "processing phrase 528 - loss 3.16 - proba 0.16\n",
      "processing phrase 529 - loss 2.96 - proba 0.17\n",
      "processing phrase 530 - loss 2.69 - proba 0.19\n",
      "processing phrase 531 - loss 3.04 - proba 0.17\n",
      "processing phrase 532 - loss 3.81 - proba 0.13\n",
      "processing phrase 533 - loss 3.70 - proba 0.13\n",
      "processing phrase 534 - loss 4.54 - proba 0.10\n",
      "processing phrase 535 - loss 4.39 - proba 0.10\n",
      "processing phrase 536 - loss 3.43 - proba 0.15\n",
      "processing phrase 537 - loss 3.76 - proba 0.13\n",
      "processing phrase 538 - loss 3.61 - proba 0.14\n",
      "processing phrase 539 - loss 4.98 - proba 0.08\n",
      "processing phrase 540 - loss 4.26 - proba 0.11\n",
      "processing phrase 541 - loss 4.49 - proba 0.10\n",
      "processing phrase 542 - loss 5.01 - proba 0.08\n",
      "processing phrase 543 - loss 4.53 - proba 0.10\n",
      "processing phrase 544 - loss 4.77 - proba 0.09\n",
      "processing phrase 545 - loss 4.99 - proba 0.08\n",
      "processing phrase 546 - loss 3.28 - proba 0.15\n",
      "processing phrase 547 - loss 4.18 - proba 0.11\n",
      "processing phrase 548 - loss 4.43 - proba 0.10\n",
      "processing phrase 549 - loss 4.80 - proba 0.09\n",
      "processing phrase 550 - loss 4.87 - proba 0.08\n",
      "processing phrase 551 - loss 4.72 - proba 0.09\n",
      "processing phrase 552 - loss 3.49 - proba 0.14\n",
      "processing phrase 553 - loss 4.24 - proba 0.11\n",
      "processing phrase 554 - loss 4.30 - proba 0.10\n",
      "processing phrase 555 - loss 4.09 - proba 0.11\n",
      "processing phrase 556 - loss 4.65 - proba 0.09\n",
      "processing phrase 557 - loss 4.13 - proba 0.11\n",
      "processing phrase 558 - loss 4.50 - proba 0.10\n",
      "processing phrase 559 - loss 4.59 - proba 0.09\n",
      "processing phrase 560 - loss 4.03 - proba 0.12\n",
      "processing phrase 561 - loss 3.52 - proba 0.14\n",
      "processing phrase 562 - loss 2.83 - proba 0.18\n",
      "processing phrase 563 - loss 2.95 - proba 0.17\n",
      "processing phrase 564 - loss 2.92 - proba 0.17\n",
      "processing phrase 565 - loss 3.42 - proba 0.15\n",
      "processing phrase 566 - loss 3.40 - proba 0.15\n",
      "processing phrase 567 - loss 4.22 - proba 0.11\n",
      "processing phrase 568 - loss 3.56 - proba 0.14\n",
      "processing phrase 569 - loss 3.80 - proba 0.13\n",
      "processing phrase 570 - loss 3.24 - proba 0.16\n",
      "processing phrase 571 - loss 3.52 - proba 0.14\n",
      "processing phrase 572 - loss 3.43 - proba 0.15\n",
      "processing phrase 573 - loss 3.23 - proba 0.16\n",
      "processing phrase 574 - loss 3.09 - proba 0.17\n",
      "processing phrase 575 - loss 3.72 - proba 0.13\n",
      "processing phrase 576 - loss 2.59 - proba 0.20\n",
      "processing phrase 577 - loss 3.19 - proba 0.16\n",
      "processing phrase 578 - loss 2.84 - proba 0.18\n",
      "processing phrase 579 - loss 2.40 - proba 0.21\n",
      "processing phrase 580 - loss 2.14 - proba 0.23\n",
      "processing phrase 581 - loss 2.22 - proba 0.22\n",
      "processing phrase 582 - loss 2.02 - proba 0.24\n",
      "processing phrase 583 - loss 2.01 - proba 0.24\n",
      "processing phrase 584 - loss 2.06 - proba 0.24\n",
      "processing phrase 585 - loss 2.68 - proba 0.19\n",
      "processing phrase 586 - loss 2.64 - proba 0.19\n",
      "processing phrase 587 - loss 2.46 - proba 0.21\n",
      "processing phrase 588 - loss 3.03 - proba 0.17\n",
      "processing phrase 589 - loss 3.26 - proba 0.16\n",
      "processing phrase 590 - loss 3.06 - proba 0.17\n",
      "processing phrase 591 - loss 4.21 - proba 0.11\n",
      "processing phrase 592 - loss 4.27 - proba 0.11\n",
      "processing phrase 593 - loss 4.15 - proba 0.11\n",
      "processing phrase 594 - loss 4.04 - proba 0.12\n",
      "processing phrase 595 - loss 4.90 - proba 0.08\n",
      "processing phrase 596 - loss 4.19 - proba 0.11\n",
      "processing phrase 597 - loss 3.67 - proba 0.14\n",
      "processing phrase 598 - loss 4.20 - proba 0.11\n",
      "processing phrase 599 - loss 3.69 - proba 0.14\n",
      "processing phrase 600 - loss 3.93 - proba 0.12\n",
      "processing phrase 601 - loss 3.81 - proba 0.13\n",
      "processing phrase 602 - loss 3.19 - proba 0.16\n",
      "processing phrase 603 - loss 3.28 - proba 0.16\n",
      "processing phrase 604 - loss 3.44 - proba 0.15\n",
      "processing phrase 605 - loss 3.69 - proba 0.14\n",
      "processing phrase 606 - loss 3.94 - proba 0.12\n",
      "processing phrase 607 - loss 3.56 - proba 0.14\n",
      "processing phrase 608 - loss 3.72 - proba 0.13\n",
      "processing phrase 609 - loss 3.20 - proba 0.16\n",
      "processing phrase 610 - loss 3.48 - proba 0.15\n",
      "processing phrase 611 - loss 3.44 - proba 0.15\n",
      "processing phrase 612 - loss 4.27 - proba 0.11\n",
      "processing phrase 613 - loss 4.07 - proba 0.11\n",
      "processing phrase 614 - loss 3.63 - proba 0.14\n",
      "processing phrase 615 - loss 4.16 - proba 0.11\n",
      "processing phrase 616 - loss 2.94 - proba 0.17\n",
      "processing phrase 617 - loss 2.94 - proba 0.17\n",
      "processing phrase 618 - loss 3.62 - proba 0.14\n",
      "processing phrase 619 - loss 3.09 - proba 0.16\n",
      "processing phrase 620 - loss 3.32 - proba 0.15\n",
      "processing phrase 621 - loss 3.00 - proba 0.17\n",
      "processing phrase 622 - loss 2.91 - proba 0.17\n",
      "processing phrase 623 - loss 2.88 - proba 0.18\n",
      "processing phrase 624 - loss 2.98 - proba 0.17\n",
      "processing phrase 625 - loss 2.38 - proba 0.21\n",
      "processing phrase 626 - loss 2.90 - proba 0.18\n",
      "processing phrase 627 - loss 3.12 - proba 0.16\n",
      "processing phrase 628 - loss 3.16 - proba 0.16\n",
      "processing phrase 629 - loss 3.65 - proba 0.14\n",
      "processing phrase 630 - loss 3.22 - proba 0.16\n",
      "processing phrase 631 - loss 3.06 - proba 0.17\n",
      "processing phrase 632 - loss 3.13 - proba 0.16\n",
      "processing phrase 633 - loss 3.49 - proba 0.14\n",
      "processing phrase 634 - loss 3.71 - proba 0.13\n",
      "processing phrase 635 - loss 3.67 - proba 0.14\n",
      "processing phrase 636 - loss 4.34 - proba 0.11\n",
      "processing phrase 637 - loss 3.93 - proba 0.12\n",
      "processing phrase 638 - loss 4.54 - proba 0.10\n",
      "processing phrase 639 - loss 3.94 - proba 0.12\n",
      "processing phrase 640 - loss 3.76 - proba 0.13\n",
      "processing phrase 641 - loss 4.04 - proba 0.12\n",
      "processing phrase 642 - loss 4.07 - proba 0.11\n",
      "processing phrase 643 - loss 4.08 - proba 0.11\n",
      "processing phrase 644 - loss 3.87 - proba 0.12\n",
      "processing phrase 645 - loss 3.51 - proba 0.14\n",
      "processing phrase 646 - loss 3.57 - proba 0.14\n",
      "processing phrase 647 - loss 4.32 - proba 0.11\n",
      "processing phrase 648 - loss 4.03 - proba 0.12\n",
      "processing phrase 649 - loss 4.53 - proba 0.10\n",
      "processing phrase 650 - loss 4.67 - proba 0.09\n",
      "processing phrase 651 - loss 4.12 - proba 0.11\n",
      "processing phrase 652 - loss 3.46 - proba 0.14\n",
      "processing phrase 653 - loss 3.08 - proba 0.16\n",
      "processing phrase 654 - loss 3.37 - proba 0.15\n",
      "processing phrase 655 - loss 3.25 - proba 0.15\n",
      "processing phrase 656 - loss 3.13 - proba 0.16\n",
      "processing phrase 657 - loss 3.23 - proba 0.16\n",
      "processing phrase 658 - loss 3.44 - proba 0.15\n",
      "processing phrase 659 - loss 4.23 - proba 0.11\n",
      "processing phrase 660 - loss 3.80 - proba 0.13\n",
      "processing phrase 661 - loss 3.88 - proba 0.12\n",
      "processing phrase 662 - loss 4.29 - proba 0.11\n",
      "processing phrase 663 - loss 4.45 - proba 0.10\n",
      "processing phrase 664 - loss 3.70 - proba 0.13\n",
      "processing phrase 665 - loss 3.40 - proba 0.15\n",
      "processing phrase 666 - loss 3.69 - proba 0.14\n",
      "processing phrase 667 - loss 3.51 - proba 0.14\n",
      "processing phrase 668 - loss 3.58 - proba 0.14\n",
      "processing phrase 669 - loss 3.84 - proba 0.13\n",
      "processing phrase 670 - loss 4.01 - proba 0.12\n",
      "processing phrase 671 - loss 4.20 - proba 0.11\n",
      "processing phrase 672 - loss 4.10 - proba 0.12\n",
      "processing phrase 673 - loss 4.05 - proba 0.12\n",
      "processing phrase 674 - loss 4.12 - proba 0.11\n",
      "processing phrase 675 - loss 4.32 - proba 0.11\n",
      "processing phrase 676 - loss 4.19 - proba 0.11\n",
      "processing phrase 677 - loss 4.45 - proba 0.10\n",
      "processing phrase 678 - loss 3.75 - proba 0.13\n",
      "processing phrase 679 - loss 3.80 - proba 0.13\n",
      "processing phrase 680 - loss 3.60 - proba 0.14\n",
      "processing phrase 681 - loss 3.38 - proba 0.15\n",
      "processing phrase 682 - loss 3.47 - proba 0.15\n",
      "processing phrase 683 - loss 3.00 - proba 0.17\n",
      "processing phrase 684 - loss 2.92 - proba 0.17\n",
      "processing phrase 685 - loss 3.42 - proba 0.15\n",
      "processing phrase 686 - loss 3.03 - proba 0.17\n",
      "processing phrase 687 - loss 2.63 - proba 0.19\n",
      "processing phrase 688 - loss 3.20 - proba 0.16\n",
      "processing phrase 689 - loss 3.53 - proba 0.14\n",
      "processing phrase 690 - loss 3.26 - proba 0.15\n",
      "processing phrase 691 - loss 2.48 - proba 0.20\n",
      "processing phrase 692 - loss 2.87 - proba 0.18\n",
      "processing phrase 693 - loss 2.86 - proba 0.18\n",
      "processing phrase 694 - loss 3.43 - proba 0.15\n",
      "processing phrase 695 - loss 3.59 - proba 0.14\n",
      "processing phrase 696 - loss 2.39 - proba 0.21\n",
      "processing phrase 697 - loss 2.81 - proba 0.18\n",
      "processing phrase 698 - loss 2.87 - proba 0.18\n",
      "processing phrase 699 - loss 3.02 - proba 0.17\n",
      "processing phrase 700 - loss 3.06 - proba 0.17\n",
      "processing phrase 701 - loss 3.66 - proba 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing phrase 702 - loss 3.62 - proba 0.14\n",
      "processing phrase 703 - loss 3.36 - proba 0.15\n",
      "processing phrase 704 - loss 3.43 - proba 0.15\n",
      "processing phrase 705 - loss 4.29 - proba 0.10\n",
      "processing phrase 706 - loss 3.71 - proba 0.13\n",
      "processing phrase 707 - loss 4.30 - proba 0.11\n",
      "processing phrase 708 - loss 3.62 - proba 0.14\n",
      "processing phrase 709 - loss 3.71 - proba 0.13\n",
      "processing phrase 710 - loss 3.73 - proba 0.13\n",
      "processing phrase 711 - loss 3.38 - proba 0.15\n",
      "processing phrase 712 - loss 3.89 - proba 0.12\n",
      "processing phrase 713 - loss 3.39 - proba 0.15\n",
      "processing phrase 714 - loss 3.52 - proba 0.14\n",
      "processing phrase 715 - loss 3.62 - proba 0.14\n",
      "processing phrase 716 - loss 4.00 - proba 0.12\n",
      "processing phrase 717 - loss 4.49 - proba 0.10\n",
      "processing phrase 718 - loss 3.66 - proba 0.14\n",
      "processing phrase 719 - loss 3.62 - proba 0.14\n",
      "processing phrase 720 - loss 3.93 - proba 0.12\n",
      "processing phrase 721 - loss 2.99 - proba 0.17\n",
      "processing phrase 722 - loss 3.48 - proba 0.15\n",
      "processing phrase 723 - loss 3.93 - proba 0.12\n",
      "processing phrase 724 - loss 4.08 - proba 0.11\n",
      "processing phrase 725 - loss 3.56 - proba 0.14\n",
      "processing phrase 726 - loss 4.04 - proba 0.12\n",
      "processing phrase 727 - loss 3.82 - proba 0.13\n",
      "processing phrase 728 - loss 3.32 - proba 0.15\n",
      "processing phrase 729 - loss 3.76 - proba 0.13\n",
      "processing phrase 730 - loss 3.76 - proba 0.13\n",
      "processing phrase 731 - loss 3.57 - proba 0.14\n",
      "processing phrase 732 - loss 4.34 - proba 0.10\n",
      "processing phrase 733 - loss 4.19 - proba 0.11\n",
      "processing phrase 734 - loss 4.37 - proba 0.10\n",
      "processing phrase 735 - loss 2.94 - proba 0.17\n",
      "processing phrase 736 - loss 2.43 - proba 0.21\n",
      "processing phrase 737 - loss 2.52 - proba 0.20\n",
      "processing phrase 738 - loss 2.91 - proba 0.17\n",
      "processing phrase 739 - loss 2.82 - proba 0.18\n",
      "processing phrase 740 - loss 3.44 - proba 0.15\n",
      "processing phrase 741 - loss 4.04 - proba 0.12\n",
      "processing phrase 742 - loss 4.29 - proba 0.11\n",
      "processing phrase 743 - loss 4.39 - proba 0.10\n",
      "processing phrase 744 - loss 3.78 - proba 0.13\n",
      "processing phrase 745 - loss 3.68 - proba 0.13\n",
      "processing phrase 746 - loss 4.25 - proba 0.11\n",
      "processing phrase 747 - loss 4.66 - proba 0.09\n",
      "processing phrase 748 - loss 4.53 - proba 0.10\n",
      "processing phrase 749 - loss 4.40 - proba 0.10\n",
      "processing phrase 750 - loss 3.95 - proba 0.12\n",
      "processing phrase 751 - loss 3.28 - proba 0.16\n",
      "processing phrase 752 - loss 2.83 - proba 0.18\n",
      "processing phrase 753 - loss 3.19 - proba 0.16\n",
      "processing phrase 754 - loss 2.98 - proba 0.17\n",
      "processing phrase 755 - loss 3.02 - proba 0.17\n",
      "processing phrase 756 - loss 2.85 - proba 0.18\n",
      "processing phrase 757 - loss 2.74 - proba 0.19\n",
      "processing phrase 758 - loss 3.20 - proba 0.16\n",
      "processing phrase 759 - loss 1.90 - proba 0.25\n",
      "processing phrase 760 - loss 3.01 - proba 0.17\n",
      "processing phrase 761 - loss 3.33 - proba 0.15\n",
      "processing phrase 762 - loss 3.53 - proba 0.14\n",
      "processing phrase 763 - loss 3.38 - proba 0.15\n",
      "processing phrase 764 - loss 2.61 - proba 0.19\n",
      "processing phrase 765 - loss 3.46 - proba 0.14\n",
      "processing phrase 766 - loss 2.95 - proba 0.17\n",
      "processing phrase 767 - loss 3.19 - proba 0.16\n",
      "processing phrase 768 - loss 3.65 - proba 0.14\n",
      "processing phrase 769 - loss 3.50 - proba 0.14\n",
      "processing phrase 770 - loss 4.39 - proba 0.10\n",
      "processing phrase 771 - loss 2.78 - proba 0.18\n",
      "processing phrase 772 - loss 3.98 - proba 0.12\n",
      "processing phrase 773 - loss 3.19 - proba 0.16\n",
      "processing phrase 774 - loss 3.35 - proba 0.15\n",
      "processing phrase 775 - loss 2.90 - proba 0.18\n",
      "processing phrase 776 - loss 2.81 - proba 0.18\n",
      "processing phrase 777 - loss 2.91 - proba 0.17\n",
      "processing phrase 778 - loss 2.87 - proba 0.18\n",
      "processing phrase 779 - loss 2.78 - proba 0.18\n",
      "processing phrase 780 - loss 2.24 - proba 0.22\n",
      "processing phrase 781 - loss 2.24 - proba 0.22\n",
      "processing phrase 782 - loss 2.68 - proba 0.19\n",
      "processing phrase 783 - loss 2.69 - proba 0.19\n",
      "processing phrase 784 - loss 2.71 - proba 0.19\n",
      "processing phrase 785 - loss 2.81 - proba 0.18\n",
      "processing phrase 786 - loss 3.18 - proba 0.16\n",
      "processing phrase 787 - loss 3.33 - proba 0.15\n",
      "processing phrase 788 - loss 3.00 - proba 0.17\n",
      "processing phrase 789 - loss 2.83 - proba 0.18\n",
      "processing phrase 790 - loss 3.15 - proba 0.16\n",
      "processing phrase 791 - loss 2.98 - proba 0.17\n",
      "processing phrase 792 - loss 3.32 - proba 0.15\n",
      "processing phrase 793 - loss 3.41 - proba 0.15\n",
      "processing phrase 794 - loss 3.73 - proba 0.13\n",
      "processing phrase 795 - loss 3.86 - proba 0.13\n",
      "processing phrase 796 - loss 2.99 - proba 0.17\n",
      "processing phrase 797 - loss 3.23 - proba 0.16\n",
      "too long answer\n",
      "processing phrase 798 - loss 10.00 - proba 0.00\n",
      "processing phrase 799 - loss 2.64 - proba 0.19\n",
      "processing phrase 800 - loss 2.91 - proba 0.18\n",
      "processing phrase 801 - loss 2.73 - proba 0.19\n",
      "processing phrase 802 - loss 3.32 - proba 0.15\n",
      "too long answer\n",
      "processing phrase 803 - loss 10.00 - proba 0.00\n",
      "processing phrase 804 - loss 3.60 - proba 0.14\n",
      "processing phrase 805 - loss 3.91 - proba 0.12\n",
      "processing phrase 806 - loss 3.29 - proba 0.15\n",
      "processing phrase 807 - loss 3.00 - proba 0.17\n",
      "processing phrase 808 - loss 2.63 - proba 0.19\n",
      "processing phrase 809 - loss 3.19 - proba 0.16\n",
      "processing phrase 810 - loss 3.32 - proba 0.15\n",
      "processing phrase 811 - loss 2.82 - proba 0.18\n",
      "processing phrase 812 - loss 3.65 - proba 0.14\n",
      "too long answer\n",
      "processing phrase 813 - loss 10.00 - proba 0.00\n",
      "too long answer\n",
      "processing phrase 814 - loss 10.00 - proba 0.00\n",
      "processing phrase 815 - loss 4.08 - proba 0.12\n",
      "processing phrase 816 - loss 3.23 - proba 0.16\n",
      "processing phrase 817 - loss 3.09 - proba 0.16\n",
      "processing phrase 818 - loss 2.63 - proba 0.20\n",
      "processing phrase 819 - loss 2.62 - proba 0.20\n",
      "processing phrase 820 - loss 1.97 - proba 0.25\n",
      "processing phrase 821 - loss 1.37 - proba 0.32\n",
      "processing phrase 822 - loss 2.70 - proba 0.19\n",
      "processing phrase 823 - loss 2.75 - proba 0.19\n",
      "processing phrase 824 - loss 2.69 - proba 0.19\n"
     ]
    }
   ],
   "source": [
    "question = 'what is topic modeling?'\n",
    "res = findBestAnswer(sens, question, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Answer with loss 0.2419998049736023 proba 0.5917137265205383: \n",
      "348\n",
      "LeaveOneOut, 257\n",
      "LinearRegression, 47-56, 81, 247\n",
      "LinearSVC, 56-59, 65, 67, 68\n",
      "load_boston, 34, 230, 317\n",
      "load_breast_cancer, 32, 38, 59, 75, 134, 144, make_blobs, 92, 119, 136, 173-183, 188, 286\n",
      "make_circles, 119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230, MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292\n",
      "\n",
      "- Answer with loss 0.308668315410614 proba 0.5660744309425354: \n",
      "(there are no false negatives for class 0). We can see that\n",
      "because all other entries in the first row of the confusion matrix are 0. We can also see\n",
      "that no  other  digits were  mistakenly  classified  as  0,  because  all  other  entries  in the\n",
      "first  column  of  the  confusion  matrix are  0 (there  are  no  false  positives  for  class  0). Some digits were confused with others, though—for example, the digit 2 (third row), three of which were classified as the digit 3 (fourth column). There was also one digit\n",
      "3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\n",
      "as 2 (thrid column, fourth row).\n",
      "\n",
      "- Answer with loss 0.5037974119186401 proba 0.4896831810474396: \n",
      "the  right ,  we  see that  worst  radius <= 16.795  creates  a  node that  contains  only  8 benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 0.5303655862808228 proba 0.47433966398239136: \n",
      "True, intercept_scaling=1, max_iter=100,\n",
      "    multi_class='ovr', n_jobs=1, penalty='l2', random_state= None,\n",
      "    solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]) This best_estimator_ in our case is a pipeline with two steps, standardscaler and\n",
      "logisticregression. To  access the  logisticregression  step, we  can  use  the\n",
      "named_steps attribute of the pipeline, as explained earlier: Logistic regression step: LogisticRegression(C=0.1, class_weight= None, dual=False, fit_intercept= True,\n",
      "                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "                  penalty='l2',\n",
      "\n",
      "- Answer with loss 0.5671939253807068 proba 0.46946048736572266: \n",
      "I  also  want  to  thank  the\n",
      "many people in my life whose love and friendship gave me the energy and support to\n",
      "undertake such a challenging task. From Sarah\n",
      "I would like to thank Meg Blanchette, without whose help and guidance this project\n",
      "would not have even existed. Thanks to Celia La and Brian Carlson for reading in the\n",
      "early days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\n",
      "to DTS, for your everlasting and endless support. Machine learning is about extracting knowledge from data.\n",
      "\n",
      "- Answer with loss 0.5740464925765991 proba 0.46304816007614136: \n",
      "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030])) print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000])) Number of features: 74849 First 20 features:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n",
      " '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n",
      " '01', '01pm', '02'] Features 20010 to 20030: ['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback',\n",
      " 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl',\n",
      " 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "\n",
      "- Answer with loss 0.5906260013580322 proba 0.4481492340564728: \n",
      "discuss methods to evaluate classification and regression performance that go beyond\n",
      "the default measures of accuracy and R2 provided by the score method. Cross-Validation\n",
      "Cross-validation is a statistical method of evaluating generalization performance that\n",
      "is more stable and thorough than using a split into a training and a test set. In cross-\n",
      "validation,  the  data  is  instead  split  repeatedly  and  multiple  models are  trained. The\n",
      "most commonly used version of cross-validation is k-fold cross-validation, where k is\n",
      "a user-specified number, usually 5 or 10.\n",
      "\n",
      "- Answer with loss 0.6114321351051331 proba 0.44382601976394653: \n",
      "And finally, thanks\n",
      "to DTS, for your everlasting and endless support. Machine learning is about extracting knowledge from data. It is a research field at the\n",
      "intersection  of  statistics,  artificial  intelligence,  and  computer  science and  is  also\n",
      "known  as  predictive  analytics  or  statistical  learning. The  application  of  machine\n",
      "learning methods has in recent years become ubiquitous in everyday life. From auto‐\n",
      "matic  recommendations  of  which  movies  to  watch,  to  what  food  to  order  or  which\n",
      "products  to  buy,  to  personalized  online  radio  and  recognizing  your  friends  in\n",
      "\n",
      "- Answer with loss 0.6325464248657227 proba 0.45481741428375244: \n",
      "As you can see, all data points were assigned the label -1, which stands for noise. This\n",
      "is  a  consequence  of  the  default  parameter  settings  for  eps  and  min_samples, which\n",
      "are  not  tuned  for  small  toy  datasets. The  cluster  assignments  for  different  values  of\n",
      "min_samples and eps are shown below, and visualized in Figure 3-37: min_samples: 2 eps: 1.000000 cluster: [-1  0  0 -1 0 -1 1 1  0  1 -1 -1] min_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\n",
      "min_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\n",
      "min_samples:\n",
      "\n",
      "- Answer with loss 0.7105879783630371 proba 0.4268055260181427: \n",
      "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores))) We obtain a mean cross-validation score of 88%, which indicates reasonable perfor‐\n",
      "mance for a balanced binary classification task. We know that LogisticRegression\n",
      "has a regularization parameter, C, which we can tune via cross-validation: from sklearn.model_selection import GridSearchCV\n",
      "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
      "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
      "\n",
      "- Answer with loss 0.7123764753341675 proba 0.4476822018623352: \n",
      "As  we  discussed  earlier,  transforming  data  using  unsupervised  learning  can  have\n",
      "many motivations. The most common motivations are visualization, compressing the\n",
      "data, and finding a representation that is more informative for further processing. One of the simplest and most widely used algorithms for all of these is principal com‐\n",
      "ponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\n",
      "zation  (NMF),  which  is  commonly  used  for  feature  extraction,  and  t-SNE,  which\n",
      "\n",
      "- Answer with loss 0.7142728567123413 proba 0.42279279232025146: \n",
      "For any\n",
      "application, we need to ask ourselves what the consequences of these mistakes might\n",
      "be in the real world. One possible mistake is that a healthy patient will be classified as positive, leading to\n",
      "additional testing. This leads to some costs and an inconvenience for the patient (and\n",
      "possibly some mental distress). An incorrect positive prediction is called a false posi‐\n",
      "tive. The other possible mistake is that a sick patient will be classified as negative, and\n",
      "will  not  receive  further  tests  and  treatment.\n",
      "\n",
      "- Answer with loss 0.7525808811187744 proba 0.42185521125793457: \n",
      "three of which were classified as the digit 3 (fourth column). There was also one digit\n",
      "3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\n",
      "as 2 (thrid column, fourth row). 0       1.00 1.00 1.00 37\n",
      "          1 0.89      0.91 0.90 43 2 0.95 0.93 0.94 44 3 0.90 0.96 0.92 45\n",
      "          4 0.97 1.00 0.99 38\n",
      "          5 0.98 0.98 0.98        48\n",
      "          6 0.96 1.00 0.98 52\n",
      "          7 1.00 0.94 0.97 48\n",
      "          8 0.93 0.90 0.91        48\n",
      "          9 0.96 0.94 0.95 47 Unsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\n",
      "sions with this class.\n",
      "\n",
      "- Answer with loss 0.7601673007011414 proba 0.4134927988052368: \n",
      "the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples. Instead of looking at the whole tree, which can be taxing, there are some useful prop‐\n",
      "erties that we can derive to summarize the workings of the tree. The most commonly\n",
      "used  summary  is  feature  importance,  which  rates  how  important\n",
      "\n",
      "- Answer with loss 0.7743133306503296 proba 0.4038320779800415: \n",
      "that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples. Instead of looking at the whole tree, which can be taxing, there are some useful prop‐\n",
      "erties that we can derive to summarize the workings of the tree. The most commonly\n",
      "used  summary  is  feature  importance,  which  rates  how  important\n",
      "\n",
      "- Answer with loss 0.8107950687408447 proba 0.40170547366142273: \n",
      "cross-validation of, 252-260\n",
      "effect of data representation choices on, 211\n",
      "evaluation and improvement, 251-252\n",
      "evaluation metrics and scoring, 275-302\n",
      "iris classification application, 13-23\n",
      "overfitting vs. underfitting, 28\n",
      "pipeline preprocessing and, 317\n",
      "selecting, 300\n",
      "selecting with grid search, 319\n",
      "theory behind, 361\n",
      "tuning parameters with grid search, 260-275 natural language processing (NLP), 325, 355\n",
      "negative class, 26\n",
      "nested cross-validation, 272\n",
      "Netflix prize challenge, 363\n",
      "neural networks (deep learning)\n",
      "\n",
      "- Answer with loss 0.8121834397315979 proba 0.4039698541164398: \n",
      "Each point along the curve in Figure 5-13 corresponds to a possible threshold of the\n",
      "decision_function. We can see, for example, that we can achieve a recall of 0.4 at a\n",
      "precision of about 0.75. The black circle marks the point that corresponds to a thresh‐\n",
      "old of 0, the default threshold for decision_function. This point is the trade-off that\n",
      "is chosen when calling the predict method. The closer a curve stays to the upper-right corner, the better the classifier. A point at\n",
      "the  upper  right  means  high  precision  and  high  recall  for  the  same  threshold.\n",
      "\n",
      "- Answer with loss 0.8584913015365601 proba 0.4012312591075897: \n",
      "We can see that\n",
      "because all other entries in the first row of the confusion matrix are 0. We can also see\n",
      "that no  other  digits were  mistakenly  classified  as  0,  because  all  other  entries  in the\n",
      "first  column  of  the  confusion  matrix are  0 (there  are  no  false  positives  for  class  0). Some digits were confused with others, though—for example, the digit 2 (third row), three of which were classified as the digit 3 (fourth column). There was also one digit\n",
      "3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\n",
      "as 2 (thrid column, fourth row).\n",
      "\n",
      "- Answer with loss 0.8691897392272949 proba 0.3875214159488678: \n",
      "To get around this problem, the splitting of the dataset during cross-validation should\n",
      "be done before doing any preprocessing. Any process that extracts knowledge from the\n",
      "dataset  should  only  ever  be  applied  to  the  training  portion  of  the  dataset,  so  any\n",
      "cross-validation should be the “outermost loop” in your processing. To  achieve  this  in  scikit-learn  with  the  cross_val_score  function  and  the  Grid\n",
      "SearchCV function, we can use the Pipeline class. The Pipeline class is a class that\n",
      "allows “gluing” together multiple processing steps into a single scikit-learn estima‐\n",
      "\n",
      "- Answer with loss 0.8918774127960205 proba 0.38987645506858826: \n",
      "has  a  built-in  list  of  English  stopwords  in the  feature_extraction.text\n",
      "module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "\n",
      "- Answer with loss 0.8935009241104126 proba 0.38788241147994995: \n",
      "LatentDirichletAllocation, 348\n",
      "LeaveOneOut, 257\n",
      "LinearRegression, 47-56, 81, 247\n",
      "LinearSVC, 56-59, 65, 67, 68\n",
      "load_boston, 34, 230, 317\n",
      "load_breast_cancer, 32, 38, 59, 75, 134, 144, make_blobs, 92, 119, 136, 173-183, 188, 286\n",
      "make_circles, 119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230, MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292\n",
      "\n",
      "- Answer with loss 0.906320333480835 proba 0.38684746623039246: \n",
      "A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a  new  way  to  represent  this  data that  summarizes  the  essential  characteristics  with\n",
      "fewer  features. A  common  application  for  dimensionality reduction  is  reduction  to\n",
      "two dimensions for visualization purposes. Another application for unsupervised transformations is finding the parts or compo‐\n",
      "nents that “make up” the data.\n",
      "\n",
      "- Answer with loss 0.9127856492996216 proba 0.38707783818244934: \n",
      ",  we  see that  worst  radius <= 16.795  creates  a  node that  contains  only  8 benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 0.913913369178772 proba 0.38141149282455444: \n",
      "that  is  too  complex  for  the  amount  of\n",
      "information we have, as our novice data scientist did, is called overfitting. Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data. On the other hand, if your model is too simple—say, “Everybody who owns a\n",
      "house buys a boat”—then you might not be able to capture all the aspects of and vari‐\n",
      "ability in the data, and your model will do badly even on the training set.\n",
      "\n",
      "- Answer with loss 0.9315787553787231 proba 0.38290733098983765: \n",
      "These  regions  are  referred  to  as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points), and they\n",
      "are defined as follows. There are two parameters in DBSCAN: min_samples and eps. If there are at least min_samples many data points within a distance of eps to a given\n",
      "data point, that data point is classified as a core sample. Core samples that are closer\n",
      "to each other than the distance eps are put into the same cluster by DBSCAN.\n",
      "\n",
      "- Answer with loss 0.9381235837936401 proba 0.3889266848564148: \n",
      "In  supervised  learning,  we  want  to  build  a  model  on  the  training  data and  then  be\n",
      "able to make accurate predictions on new, unseen data that has the same characteris‐\n",
      "tics as the training set that we used. If a model is able to make accurate predictions on\n",
      "unseen  data,  we  say  it  is  able  to  generalize  from the  training  set  to  the  test  set. We\n",
      "want to build a model that is able to generalize as accurately as possible. Usually we build a model in such a way that it can make accurate predictions on the\n",
      "training  set.\n",
      "\n",
      "- Answer with loss 0.9525642395019531 proba 0.3817853629589081: \n",
      "361\n",
      "Other Machine Learning Frameworks and Packages 362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366 Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   367\n",
      "\n",
      "- Answer with loss 0.9537962675094604 proba 0.3795314431190491: \n",
      "The data X is\n",
      "always  assumed  to be  a  NumPy  array  or  SciPy  sparse  matrix that  has  continuous (floating-point) entries. Supervised algorithms also require a y argument, which is a\n",
      "one-dimensional  NumPy  array  containing  target  values  for  regression  or classifica‐\n",
      "tion (i.e., the known output labels or responses). There are two main ways to apply a learned model in scikit-learn. To create a pre‐\n",
      "diction in the form of a new output like y, you use the predict method. To create a\n",
      "new  representation  of  the  input  data  X,  you  use  the  transform  method.\n",
      "\n",
      "- Answer with loss 0.9580352306365967 proba 0.36793503165245056: \n",
      "We discussed the parameter settings of many of the algorithms in scikit-learn\n",
      "in  Chapters  2  and  3,  and  it  is  important  to understand what the  parameters mean\n",
      "before  trying  to adjust  them. Finding  the  values  of  the  important  parameters  of  a\n",
      "model (the ones that provide the best generalization performance) is a tricky task, but\n",
      "necessary for almost all models and datasets. Because it is such a common task, there\n",
      "are standard methods in scikit-learn to help you with it. The most commonly used\n",
      "method is grid search, which basically means trying all possible combinations of the\n",
      "parameters of interest.\n",
      "\n",
      "- Answer with loss 0.9671443700790405 proba 0.36734867095947266: \n",
      "Therefore, it is important that the model\n",
      "does not produce many false positives—in other words, that it has a high precision. Precision is also known as positive predictive value (PPV). Recall is used as performance metric when we need to identify all positive samples;\n",
      "that  is,  when  it  is  important  to  avoid  false  negatives. The  cancer  diagnosis example from earlier in this chapter is a good example for this: it is important to find all peo‐\n",
      "ple that are sick, possibly including healthy patients in the prediction.\n",
      "\n",
      "- Answer with loss 0.9962359070777893 proba 0.36890313029289246: \n",
      "There are two main approaches: using a language-\n",
      "specific  list  of  stopwords,  or  discarding  words  that  appear  too  frequently. scikit- learn has  a  built-in  list  of  English  stopwords  in the  feature_extraction.text\n",
      "module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "\n",
      "- Answer with loss 1.0253195762634277 proba 0.37032631039619446: \n",
      "251 Cross-Validation 252\n",
      "Cross-Validation in scikit-learn                                                                                253\n",
      "Benefits of Cross-Validation 254 Stratified k-Fold Cross-Validation and Other Strategies 254 Grid Search 260 Simple Grid Search 261 The Danger of Overfitting the Parameters and the Validation Set                      261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring 275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification\n",
      "\n",
      "- Answer with loss 1.038636326789856 proba 0.3711284101009369: \n",
      "mean_test_score\n",
      "0   0.001 0.001         {'C': 0.001, 'gamma': 0.001}       0.366 1   0.001 0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347 0.363 2 22              0.375           0.347\n",
      "\n",
      "- Answer with loss 1.040618896484375 proba 0.36508095264434814: \n",
      "coef_ attribute, 47, 50\n",
      "comments and questions, xi\n",
      "competitions, 365\n",
      "conflation, 344\n",
      "confusion matrices, 279-286\n",
      "context, 343\n",
      "continuous features, 211, 218\n",
      "core samples/core points, 187\n",
      "corpus, 325\n",
      "cos function, 232\n",
      "CountVectorizer, 334\n",
      "cross-validation analyzing results of, 267-271\n",
      "benefits of, 254\n",
      "cross-validation splitters, 256\n",
      "grid search and, 263-275\n",
      "in scikit-learn, 253\n",
      "leave-one-out cross-validation, 257\n",
      "nested, 272\n",
      "parallelizing with grid search, 274\n",
      "principle of, 252\n",
      "purpose of, 254\n",
      "shuffle-split cross-validation, 258\n",
      "stratified k-fold, 254-256\n",
      "with groups, 259\n",
      "\n",
      "- Answer with loss 1.046698808670044 proba 0.36612212657928467: \n",
      "359 Building Your Own Estimator 360 Where to Go from Here                                                                                                361\n",
      "Theory 361\n",
      "Other Machine Learning Frameworks and Packages 362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366\n",
      "\n",
      "- Answer with loss 1.047479510307312 proba 0.359885036945343: \n",
      ", it can cap‐\n",
      "ture  clusters  of  complex  shapes, and  it  can  identify  points that  are  not  part  of any\n",
      "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
      "still scales to relatively large datasets. DBSCAN  works  by  identifying  points that  are  in  “crowded”  regions  of the  feature\n",
      "space,  where  many  data  points  are  close  together. These  regions  are  referred  to  as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty.\n",
      "\n",
      "- Answer with loss 1.0481112003326416 proba 0.36554133892059326: \n",
      "feature_names = vect.get_feature_names()\n",
      "print(\"Number of features: {}\".format(len(feature_names)))\n",
      "print(\"First 20 features:\\n{}\".format(feature_names[:20])) print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030])) print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000])) Number of features: 74849 First 20 features:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n",
      " '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n",
      " '01', '01pm', '02']\n",
      "\n",
      "- Answer with loss 1.0518572330474854 proba 0.3613821566104889: \n",
      "The  points  in  class  0  are  above  the  line  corresponding  to  class  2,  which means they  are  classified  as  “rest”  by  the  binary  classifier  for  class  2. The  points belonging to class 0 are to the left of the line corresponding to class 1, which means\n",
      "the  binary  classifier  for  class 1 also  classifies  them  as  “rest.” Therefore,  any  point  in\n",
      "this area will be classified as class 0 by the final classifier (the result of the classifica‐\n",
      "tion  confidence  formula  for  classifier  0  is  greater  than  zero,  while  it  is  smaller  than\n",
      "zero for the other two classes).\n",
      "\n",
      "- Answer with loss 1.0808699131011963 proba 0.3570089638233185: \n",
      "68\n",
      "load_boston, 34, 230, 317\n",
      "load_breast_cancer, 32, 38, 59, 75, 134, 144, make_blobs, 92, 119, 136, 173-183, 188, 286\n",
      "make_circles, 119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230, MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292 RandomForestClassifier, 84-86, 238, 290,\n",
      "\n",
      "- Answer with loss 1.0915653705596924 proba 0.350341796875: \n",
      "There  are  two  easy-to-use  choices  for  algorithm. The  default  is  'adam',\n",
      "which works well in most situations but is quite sensitive to the scaling of the data (so\n",
      "it is important to always scale your data to 0 mean and unit variance). The other one\n",
      "is  'l-bfgs',  which  is  quite  robust but  might  take  a  long  time  on  larger  models  or\n",
      "larger  datasets. There  is  also  the  more  advanced  'sgd'  option,  which  is  what  many\n",
      "deep learning researchers use. The 'sgd' option comes with many additional param‐\n",
      "\n",
      "- Answer with loss 1.1101335287094116 proba 0.34688255190849304: \n",
      "set. However,  intuitively3 we  expect  simple  models  to  generalize\n",
      "better to new data. If the rule was “People older than 50 want to buy a boat,” and this\n",
      "would explain the behavior of all the customers, we would trust it more than the rule\n",
      "involving children and marital status in addition to age. Therefore, we always want to\n",
      "find  the  simplest  model. Building  a  model that  is  too  complex  for  the  amount  of\n",
      "information we have, as our novice data scientist did, is called overfitting. Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data.\n",
      "\n",
      "- Answer with loss 1.1131198406219482 proba 0.35429489612579346: \n",
      "One-Hot-Encoding (Dummy Variables) By far the most common way to represent categorical variables is using the one-hot-\n",
      "encoding or one-out-of-N encoding, also known as dummy variables. The idea behind\n",
      "dummy  variables  is  to  replace  a  categorical  variable  with  one  or  more  new  features that can have the values 0 and 1. The values 0 and 1 make sense in the formula for\n",
      "linear  binary  classification  (and  for  all  other  models  in  scikit-learn), and  we  can\n",
      "represent any number of categories by introducing one new feature per category, as\n",
      "described here.\n",
      "\n",
      "- Answer with loss 1.114460825920105 proba 0.3456759452819824: \n",
      "Thanks to the O’Reilly folks for their endless patience. And finally, thanks\n",
      "to DTS, for your everlasting and endless support. Machine learning is about extracting knowledge from data. It is a research field at the\n",
      "intersection  of  statistics,  artificial  intelligence,  and  computer  science and  is  also\n",
      "known  as  predictive  analytics  or  statistical  learning. The  application  of  machine\n",
      "learning methods has in recent years become ubiquitous in everyday life. From auto‐\n",
      "matic  recommendations  of  which  movies  to  watch,  to  what  food  to  order  or  which\n",
      "products  to  buy,  to  personalized  online  radio  and  recognizing  your  friends  in\n",
      "\n",
      "- Answer with loss 1.1169734001159668 proba 0.355758935213089: \n",
      "0.001         {'C': 0.001, 'gamma': 0.001}       0.366 1   0.001 0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347 0.363 2 22              0.375           0.347\n",
      "\n",
      "- Answer with loss 1.1325756311416626 proba 0.3736824691295624: \n",
      "Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples. Instead of looking at the whole tree, which can be taxing, there are some useful prop‐\n",
      "erties that we can derive to summarize the workings of the tree.\n",
      "\n",
      "- Answer with loss 1.1335539817810059 proba 0.35107699036598206: \n",
      "Topic Modeling and Document Clustering One particular technique that is often applied to text data is topic modeling, which is\n",
      "an umbrella term describing the task of assigning each document to one or multiple\n",
      "topics,  usually  without  supervision. A  good  example  for  this  is  news  data,  which\n",
      "might be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\n",
      "document  is  assigned  a  single  topic,  this  is  the  task  of  clustering  the  documents,  as\n",
      "discussed  in  Chapter  3.\n",
      "\n",
      "- Answer with loss 1.1435141563415527 proba 0.34391680359840393: \n",
      "The closer a curve stays to the upper-right corner, the better the classifier. A point at\n",
      "the  upper  right  means  high  precision  and  high  recall  for  the  same  threshold. The\n",
      "curve starts at the top-left corner, corresponding to a very low threshold, classifying\n",
      "everything as the positive class. Raising the threshold moves the curve toward higher\n",
      "precision, but also lower recall. Raising the threshold more and more, we get to a sit‐\n",
      "uation where most of the points classified as being positive are true positives, leading\n",
      "to  a  very  high  precision\n",
      "\n",
      "- Answer with loss 1.1481176614761353 proba 0.3515985310077667: \n",
      "dual=False, fit_intercept= True, intercept_scaling=1, max_iter=100,\n",
      "    multi_class='ovr', n_jobs=1, penalty='l2', random_state= None,\n",
      "    solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]) This best_estimator_ in our case is a pipeline with two steps, standardscaler and\n",
      "logisticregression. To  access the  logisticregression  step, we  can  use  the\n",
      "named_steps attribute of the pipeline, as explained earlier: Logistic regression step: LogisticRegression(C=0.1, class_weight= None, dual=False, fit_intercept=\n",
      "\n",
      "- Answer with loss 1.1482821702957153 proba 0.352853387594223: \n",
      "drawbacks of, 146\n",
      "example of, 140\n",
      "feature extraction with, 147\n",
      "unsupervised nature of, 145\n",
      "visualizations with, 142\n",
      "whitening option, 150\n",
      "probabilistic modeling, 363 building your own estimators, 360\n",
      "business metrics and, 358\n",
      "initial approach to, 357\n",
      "resources, 361-366\n",
      "simple vs. complicated cases, 358\n",
      "steps of, 358\n",
      "testing your system, 359\n",
      "tool choice, 359\n",
      "production systems\n",
      "testing, 359\n",
      "tool choice, 359 analyzing, 85\n",
      "building, 84\n",
      "data representation and, 220-224\n",
      "vs. decision trees, 83\n",
      "vs. gradient boosted regression trees, 88\n",
      "parameters, 88\n",
      "predictions with, 84\n",
      "randomization in, 83\n",
      "strengths and weaknesses, 87\n",
      "\n",
      "- Answer with loss 1.1517521142959595 proba 0.3495531678199768: \n",
      "362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366 Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   367\n",
      "\n",
      "- Answer with loss 1.159773349761963 proba 0.3403254449367523: \n",
      "From  this  plot we  can  determine  that  features  in  the  Breast  Cancer  dataset are  of\n",
      "completely  different  orders  of  magnitude. This  can  be  somewhat  of  a  problem  for\n",
      "other models (like linear models), but it has devastating effects for the kernel SVM. Let’s examine some ways to deal with this issue. One  way to  resolve  this  problem  is  by  rescaling  each feature so that they  are  all\n",
      "approximately on the same scale. A common rescaling method for kernel SVMs is to\n",
      "scale the data such that all features are between 0 and 1.\n",
      "\n",
      "- Answer with loss 1.1608127355575562 proba 0.3438411056995392: \n",
      "Building  a  model that  is  too  complex  for  the  amount  of\n",
      "information we have, as our novice data scientist did, is called overfitting. Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data. On the other hand, if your model is too simple—say, “Everybody who owns a\n",
      "house buys a boat”—then you might not be able to capture all the aspects of and vari‐\n",
      "ability in the data, and your model will do badly even on the training set.\n",
      "\n",
      "- Answer with loss 1.1658298969268799 proba 0.3480461835861206: \n",
      "None, dual=False, fit_intercept= True, intercept_scaling=1, max_iter=100,\n",
      "    multi_class='ovr', n_jobs=1, penalty='l2', random_state= None,\n",
      "    solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]) This best_estimator_ in our case is a pipeline with two steps, standardscaler and\n",
      "logisticregression. To  access the  logisticregression  step, we  can  use  the\n",
      "named_steps attribute of the pipeline, as explained earlier: Logistic regression step: LogisticRegression(C=0.1, class_weight= None, dual=False, fit_intercept=\n",
      "\n",
      "- Answer with loss 1.1732442378997803 proba 0.3390903174877167: \n",
      "However,  intuitively3 we  expect  simple  models  to  generalize\n",
      "better to new data. If the rule was “People older than 50 want to buy a boat,” and this\n",
      "would explain the behavior of all the customers, we would trust it more than the rule\n",
      "involving children and marital status in addition to age. Therefore, we always want to\n",
      "find  the  simplest  model. Building  a  model that  is  too  complex  for  the  amount  of\n",
      "information we have, as our novice data scientist did, is called overfitting. Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data.\n",
      "\n",
      "- Answer with loss 1.2126333713531494 proba 0.3425218462944031: \n",
      "300\n",
      "Summary and Outlook                                                                                                  302 305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters 317 Grid-Searching\n",
      "\n",
      "- Answer with loss 1.2271126508712769 proba 0.33834144473075867: \n",
      "0.92\n",
      "Training accuracy of l1 logreg with C=1.000: 0.96\n",
      "Test accuracy of l1 logreg with C=1.000: 0.96\n",
      "Training accuracy of l1 logreg with C=100.000: 0.99\n",
      "Test accuracy of l1 logreg with C=100.000: 0.98 As you can see, there are many parallels between linear models for binary classifica‐\n",
      "tion and linear models for regression. As in regression, the main difference between\n",
      "the  models  is  the  penalty  parameter,  which  influences  the  regularization  and\n",
      "whether the model will use all available features or select only a subset.\n",
      "\n",
      "- Answer with loss 1.2342793941497803 proba 0.3321240544319153: \n",
      "The  color  encodes  the  cross-validation  accuracy,  with  light\n",
      "colors  meaning  high  accuracy  and  dark  colors  meaning  low  accuracy. You can  see\n",
      "that SVC is very sensitive to the setting of the parameters. For many of the parameter\n",
      "settings, the accuracy is around 40%, which is quite bad; for other settings the accu‐\n",
      "racy is around 96%. We can take away from this plot several things. First, the parame‐\n",
      "ters we adjusted are very important for obtaining good performance. Both parameters\n",
      "(C and gamma) matter a lot, as adjusting them can change the accuracy from 40% to\n",
      "96%.\n",
      "\n",
      "- Answer with loss 1.2353779077529907 proba 0.3385380506515503: \n",
      "We could visualize this graphically as shown in\n",
      "Figure 2-44: Figure 2-44. Visualization of logistic regression, where input features and predictions are\n",
      "shown as nodes, and the coefficients are connections between the nodes Here, each node on the left represents an input feature, the connecting lines represent\n",
      "the learned coefficients, and the node on the right represents the output, which is a\n",
      "weighted sum of the inputs. In an MLP this process of computing weighted sums is repeated multiple times, first\n",
      "computing  hidden  units\n",
      "\n",
      "- Answer with loss 1.2382146120071411 proba 0.33480581641197205: \n",
      "learn has  a  built-in  list  of  English  stopwords  in the  feature_extraction.text\n",
      "module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "\n",
      "- Answer with loss 1.2450075149536133 proba 0.3294949531555176: \n",
      "This means that building a model using a particular parameter setting on a particular cross-validation split can\n",
      "be  done  completely  independently  from  the  other  parameter  settings  and  models. This makes grid search and cross-validation ideal candidates for parallelization over\n",
      "multiple  CPU  cores  or  over  a  cluster. You  can  make  use  of  multiple  cores  in  Grid\n",
      "SearchCV  and  cross_val_score  by  setting  the  n_jobs  parameter  to the  number  of\n",
      "CPU cores you want to use. You can set n_jobs=-1 to use all available cores.\n",
      "\n",
      "- Answer with loss 1.2619316577911377 proba 0.3329377770423889: \n",
      "The cover fonts are URW Type‐\n",
      "writer  and  Guardian  Sans. The  text font  is  Adobe  Minion  Pro;  the  heading  font  is\n",
      "Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n",
      "\n",
      "- Answer with loss 1.2714284658432007 proba 0.32592982053756714: \n",
      "These questions are similar to the questions you might ask in a game of 20 Questions. Imagine you  want  to  distinguish  between the  following  four  animals:  bears,  hawks,\n",
      "penguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\n",
      "questions as possible. You might start off by asking whether the animal has feathers, a\n",
      "question that narrows down your possible animals to just two. If the answer is “yes,”\n",
      "you  can  ask  another  question  that  could  help you  distinguish  between  hawks  and\n",
      "penguins.\n",
      "\n",
      "- Answer with loss 1.2726937532424927 proba 0.3333435654640198: \n",
      "296 Regression Metrics 299 Using Evaluation Metrics in Model Selection 300\n",
      "Summary and Outlook                                                                                                  302 305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315\n",
      "\n",
      "- Answer with loss 1.2735557556152344 proba 0.3314765393733978: \n",
      "As  the  simple  k-fold  strategy  fails  here,  scikit-learn does  not  use  it  for  classifica‐\n",
      "tion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\n",
      "split the data such that the proportions between classes are the same in each fold as\n",
      "they are in the whole dataset, as illustrated in Figure 5-2: For  example,  if  90%  of  your  samples  belong  to  class A  and  10%  of  your  samples\n",
      "belong  to  class  B,  then  stratified  cross-validation ensures  that  in  each  fold,  90%  of\n",
      "samples belong to class A and 10% of samples belong to class B.\n",
      "\n",
      "- Answer with loss 1.2776648998260498 proba 0.33027341961860657: \n",
      "104-119\n",
      "overview of, 2\n",
      "data representation, 4\n",
      "examples of, 3\n",
      "generalization, 26\n",
      "goals for, 25\n",
      "model complexity vs. dataset size, 29\n",
      "overfitting vs. underfitting, 28\n",
      "overview of, 127\n",
      "sample datasets, 30-34\n",
      "uncertainty estimates, 119-127 bag-of-words representation, 327-334\n",
      "examples of, 323\n",
      "model coefficients, 338\n",
      "overview of, 355\n",
      "rescaling data with tf-idf, 336-338\n",
      "sentiment analysis example, 325\n",
      "stopwords, 334\n",
      "topic modeling and document clustering, time series predictions, 363\n",
      "tokenization, 328, 344-347\n",
      "top nodes, 72\n",
      "topic modeling, with LDA, 347-355\n",
      "training data, 17\n",
      "train_test_split function, 254\n",
      "transform method, 135, 312, 334\n",
      "transformations\n",
      "selecting, 235\n",
      "univariate nonlinear, 232-236\n",
      "unsupervised, 131\n",
      "\n",
      "- Answer with loss 1.2793322801589966 proba 0.32859688997268677: \n",
      "and  you  have  100  hidden  units,  then  there  are  100 *\n",
      "100  =  10,000  weights  between  the  input  and  the  first  hidden  layer. There  are  also\n",
      "100 *  1  =  100  weights  between  the  hidden  layer  and  the  output  layer,  for  a  total  of\n",
      "around 10,100 weights. If you add a second hidden layer with 100 hidden units, there\n",
      "will be another 100 * 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000.\n",
      "\n",
      "- Answer with loss 1.2842535972595215 proba 0.3168823719024658: \n",
      "It is always possible to set a thresh‐\n",
      "old to fulfill a particular target, like 90% recall. The hard part is to develop a model\n",
      "that  still  has  reasonable  precision  with  this  threshold—if  you  classify  everything  as\n",
      "positive, you will have 100% recall, but your model will be useless. Setting a requirement on a classifier like 90% recall is often called setting the operat‐\n",
      "ing point. Fixing an operating point is often helpful in business settings to make per‐\n",
      "formance guarantees to customers or other groups inside your organization.\n",
      "\n",
      "- Answer with loss 1.286678671836853 proba 0.3255356252193451: \n",
      "unsuper‐\n",
      "vised learning  algorithms. Unsupervised learning  subsumes  all  kinds  of  machine\n",
      "learning where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data.\n",
      "\n",
      "- Answer with loss 1.2896648645401 proba 0.3258631229400635: \n",
      "Who Should Read This Book This  book  is  for  current  and  aspiring  machine  learning  practitioners  looking  to\n",
      "implement solutions to real-world machine learning problems. This is an introduc‐\n",
      "tory book requiring no previous knowledge of machine learning or artificial intelli‐\n",
      "gence  (AI). We  focus  on  using  Python and  the  scikit-learn  library,  and  work\n",
      "through all the steps to create a successful machine learning application. The meth‐\n",
      "ods we introduce will be helpful for scientists and researchers, as well as data\n",
      "\n",
      "- Answer with loss 1.3020384311676025 proba 0.3241332173347473: \n",
      "Here the input is a scan of the handwriting, and the desired output is the actual\n",
      "digits in the zip code. To create a dataset for building a machine learning model,\n",
      "you  need  to  collect  many  envelopes. Then  you  can  read  the  zip  codes  yourself\n",
      "and store the digits as your desired outcomes. Here the input is the image, and the output is whether the tumor is benign. To\n",
      "create a dataset for building a model, you need a database of medical images. You\n",
      "also  need  an  expert  opinion,  so  a  doctor  needs  to  look  at  all  of  the  images  and\n",
      "decide which tumors are benign and which are not.\n",
      "\n",
      "- Answer with loss 1.302392601966858 proba 0.31705108284950256: \n",
      "is  lower  than  when  using  more  neigh‐\n",
      "bors,  indicating  that  using  the  single  nearest  neighbor  leads  to  a  model that  is  too\n",
      "complex. On the other hand, when considering 10 neighbors, the model is too simple\n",
      "and performance is even worse. The best performance is somewhere in the middle,\n",
      "using around six neighbors. Still, it is good to keep the scale of the plot in mind. The\n",
      "worst performance is around 88% accuracy, which might still be acceptable. There  is  also  a  regression  variant  of  the  k-nearest  neighbors  algorithm.\n",
      "\n",
      "- Answer with loss 1.3084551095962524 proba 0.3247665762901306: \n",
      "*\n",
      "100  =  10,000  weights  between  the  input  and  the  first  hidden  layer. There  are  also\n",
      "100 *  1  =  100  weights  between  the  hidden  layer  and  the  output  layer,  for  a  total  of\n",
      "around 10,100 weights. If you add a second hidden layer with 100 hidden units, there\n",
      "will be another 100 * 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000.\n",
      "\n",
      "- Answer with loss 1.3165643215179443 proba 0.3189722001552582: \n",
      "that  worst  radius <= 16.795  creates  a  node that  contains  only  8 benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 1.3199219703674316 proba 0.32000213861465454: \n",
      "There  are  also\n",
      "100 *  1  =  100  weights  between  the  hidden  layer  and  the  output  layer,  for  a  total  of\n",
      "around 10,100 weights. If you add a second hidden layer with 100 hidden units, there\n",
      "will be another 100 * 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000.\n",
      "\n",
      "- Answer with loss 1.3205090761184692 proba 0.3216840624809265: \n",
      "But the same is not true for the next algorithm we will look at,\n",
      "DBSCAN. DBSCAN Another  very  useful clustering  algorithm  is  DBSCAN  ( which  stands  for  “density-\n",
      "based spatial clustering of applications with noise”). The main benefits of DBSCAN\n",
      "are that it does not require the user to set the number of clusters a priori , it can cap‐\n",
      "ture  clusters  of  complex  shapes, and  it  can  identify  points that  are  not  part  of any\n",
      "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
      "still scales to relatively large datasets.\n",
      "\n",
      "- Answer with loss 1.3245530128479004 proba 0.315756231546402: \n",
      "To  achieve  this  in  scikit-learn  with  the  cross_val_score  function  and  the  Grid\n",
      "SearchCV function, we can use the Pipeline class. The Pipeline class is a class that\n",
      "allows “gluing” together multiple processing steps into a single scikit-learn estima‐ tor. The Pipeline class itself has fit, predict, and score methods and behaves just\n",
      "like any other model in scikit-learn. The most common use case of the Pipeline\n",
      "class  is  in  chaining  preprocessing  steps  (like  scaling  of  the  data)  together  with  a\n",
      "supervised model like a classifier.\n",
      "\n",
      "- Answer with loss 1.3267371654510498 proba 0.3218580484390259: \n",
      "In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data. A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a  new  way  to  represent  this  data\n",
      "\n",
      "- Answer with loss 1.3307108879089355 proba 0.3220881521701813: \n",
      "this\n",
      "means storing the dataset, so we can compute neighbors during prediction: To make predictions on the test data, we call the predict method. For each data point\n",
      "in  the  test  set,  this  computes  its  nearest  neighbors  in  the  training  set and  finds  the\n",
      "most common class among these: For two-dimensional datasets, we can also illustrate the prediction for all possible test\n",
      "points  in  the  xy-plane. We  color  the  plane  according  to  the  class that  would  be\n",
      "assigned to a point in this region.\n",
      "\n",
      "- Answer with loss 1.3342392444610596 proba 0.31889331340789795: \n",
      "Cross-Validation\n",
      "Cross-validation is a statistical method of evaluating generalization performance that\n",
      "is more stable and thorough than using a split into a training and a test set. In cross-\n",
      "validation,  the  data  is  instead  split  repeatedly  and  multiple  models are  trained. The\n",
      "most commonly used version of cross-validation is k-fold cross-validation, where k is\n",
      "a user-specified number, usually 5 or 10. When performing five-fold cross-validation,\n",
      "the data is first partitioned into five parts of (approximately) equal size, called folds.\n",
      "\n",
      "- Answer with loss 1.3343143463134766 proba 0.31500256061553955: \n",
      "we  can  determine  that  features  in  the  Breast  Cancer  dataset are  of\n",
      "completely  different  orders  of  magnitude. This  can  be  somewhat  of  a  problem  for\n",
      "other models (like linear models), but it has devastating effects for the kernel SVM. Let’s examine some ways to deal with this issue. One  way to  resolve  this  problem  is  by  rescaling  each feature so that they  are  all\n",
      "approximately on the same scale. A common rescaling method for kernel SVMs is to\n",
      "scale the data such that all features are between 0 and 1.\n",
      "\n",
      "- Answer with loss 1.3391547203063965 proba 0.31975430250167847: \n",
      "scikit- learn has  a  built-in  list  of  English  stopwords  in the  feature_extraction.text\n",
      "module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "\n",
      "- Answer with loss 1.3458436727523804 proba 0.31480836868286133: \n",
      "This  can  be  somewhat  of  a  problem  for\n",
      "other models (like linear models), but it has devastating effects for the kernel SVM. Let’s examine some ways to deal with this issue. One  way to  resolve  this  problem  is  by  rescaling  each feature so that they  are  all\n",
      "approximately on the same scale. A common rescaling method for kernel SVMs is to\n",
      "scale the data such that all features are between 0 and 1. We will see how to do this\n",
      "using  the  MinMaxScaler  preprocessing  method  in  Chapter  3,  where  we’ll  give  more\n",
      "details.\n",
      "\n",
      "- Answer with loss 1.3467803001403809 proba 0.31658726930618286: \n",
      "One way to solve this problem would be to build a\n",
      "classifier where each person is a separate class. However, there are usually many dif‐\n",
      "ferent people in face databases, and very few images of the same person (i.e., very few\n",
      "training examples per class). That makes it hard to train most classifiers. Additionally, A simple solution is to use a one-nearest-neighbor classifier that looks for the most\n",
      "similar  face  image  to  the  face you  are  classifying. This  classifier could  in  principle\n",
      "work  with  only  a  single  training\n",
      "\n",
      "- Answer with loss 1.3472198247909546 proba 0.31709912419319153: \n",
      "All  three  have  a  way  of  controlling  the  granularity  of  clustering. k-means  and\n",
      "agglomerative clustering allow  you  to  specify  the  number  of  desired  clusters,  while\n",
      "DBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\n",
      "ences cluster size. All three methods can be used on large, real-world datasets, are rel‐\n",
      "atively easy to understand, and allow for clustering into many clusters. Each of the algorithms has somewhat different strengths. k-means allows for a char‐\n",
      "acterization of the clusters using the cluster means.\n",
      "\n",
      "- Answer with loss 1.3474030494689941 proba 0.3174954950809479: \n",
      "Additionally, inspecting your data is a good way to find abnormalities and peculiari‐\n",
      "ties. Maybe some of your irises were measured using inches and not centimeters, for\n",
      "example. In the real world, inconsistencies in the data and unexpected measurements\n",
      "are very common. One of the best ways to inspect data is to visualize it. One way to do this is by using a\n",
      "scatter plot. A scatter plot of the data puts one feature along the x-axis and another\n",
      "along  the  y-axis,  and  draws  a  dot  for  each  data  point.\n",
      "\n",
      "- Answer with loss 1.3552391529083252 proba 0.31654664874076843: \n",
      "who we  know  are  not  interested  in  buying  a  boat.2 The\n",
      "goal  is  to  send  out  promotional  emails  to  people who  are  likely  to  actually  make  a\n",
      "purchase, but not bother those customers who won’t be interested. After looking at the data for a while, our novice data scientist comes up with the fol‐\n",
      "lowing rule: “If the customer is older than 45, and has less than 3 children or is not\n",
      "divorced, then they want to buy a boat.” When asked how well this rule of his does,\n",
      "our data scientist answers, “It’s 100 percent accurate!”\n",
      "\n",
      "- Answer with loss 1.3576486110687256 proba 0.32033571600914: \n",
      "param_gamma params mean_test_score\n",
      "0   0.001 0.001         {'C': 0.001, 'gamma': 0.001}       0.366 1   0.001 0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347 0.363\n",
      "\n",
      "- Answer with loss 1.3594326972961426 proba 0.3129815459251404: \n",
      "plt.ylabel(\"Feature magnitude\")\n",
      "plt.yscale(\"log\") From  this  plot we  can  determine  that  features  in  the  Breast  Cancer  dataset are  of\n",
      "completely  different  orders  of  magnitude. This  can  be  somewhat  of  a  problem  for\n",
      "other models (like linear models), but it has devastating effects for the kernel SVM. Let’s examine some ways to deal with this issue. One  way to  resolve  this  problem  is  by  rescaling  each feature so that they  are  all\n",
      "approximately on the same scale. A common rescaling method for kernel SVMs is to\n",
      "scale the data such that all features are between 0 and 1.\n",
      "\n",
      "- Answer with loss 1.3629262447357178 proba 0.32128629088401794: \n",
      "323\n",
      "Types of Data Represented as Strings                                                                          323 Example Application: Sentiment Analysis of Movie Reviews 325 Representing Text Data as a Bag of Words                                                                 327 Applying Bag-of-Words to a Toy Dataset 329 Bag-of-Words for Movie Reviews 330 Stopwords 334 Rescaling the Data with tf–idf                                                                                      336\n",
      "Investigating Model Coefficients                                                                                  338\n",
      "Bag-of-Words with More Than One Word (n-Grams)                                            339\n",
      "Advanced Tokenization, Stemming, and Lemmatization                                        344\n",
      "\n",
      "- Answer with loss 1.3715152740478516 proba 0.3173188269138336: \n",
      "we  will  now  dive  more\n",
      "deeply into evaluating models and selecting parameters. We will focus on the supervised methods, regression and classification, as evaluating\n",
      "and selecting models in unsupervised learning is often a very qualitative process (as\n",
      "we saw in Chapter 3). To evaluate our supervised models, so far we have split our dataset into a training set\n",
      "and a test set using the train_test_split function, built a model on the training set\n",
      "by calling the fit method, and evaluated it on the test set using the score method,\n",
      "which  for  classification  computes  the  fraction  of  correctly  classified  samples.\n",
      "\n",
      "- Answer with loss 1.3738486766815186 proba 0.3161747455596924: \n",
      "plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
      "            'Line class 2'], loc=(1.01, 0.3)) You can see that all the points belonging to class 0 in the training data are above the\n",
      "line corresponding to class 0, which means they are on the “class 0” side of this binary\n",
      "classifier. The  points  in  class  0  are  above  the  line  corresponding  to  class  2,  which means they  are  classified  as  “rest”  by  the  binary  classifier  for  class  2. The  points belonging to class 0 are to the left of the line corresponding to class 1, which means\n",
      "the  binary  classifier  for  class\n",
      "\n",
      "- Answer with loss 1.3762305974960327 proba 0.31570523977279663: \n",
      "to  assess  generalization  performance,  and discuss methods to evaluate classification and regression performance that go beyond\n",
      "the default measures of accuracy and R2 provided by the score method. Cross-Validation\n",
      "Cross-validation is a statistical method of evaluating generalization performance that\n",
      "is more stable and thorough than using a split into a training and a test set. In cross-\n",
      "validation,  the  data  is  instead  split  repeatedly  and  multiple  models are  trained. The\n",
      "most commonly used version of cross-validation is k-fold cross-validation, where k is\n",
      "a user-specified number, usually 5 or 10.\n",
      "\n",
      "- Answer with loss 1.3802214860916138 proba 0.3082520067691803: \n",
      "Raising the threshold moves the curve toward higher\n",
      "precision, but also lower recall. Raising the threshold more and more, we get to a sit‐\n",
      "uation where most of the points classified as being positive are true positives, leading\n",
      "to  a  very  high  precision but  lower  recall. The  more  the  model  keeps  recall  high  as\n",
      "precision goes up, the better. Looking at this particular curve a bit more, we can see that with this model it is possi‐\n",
      "ble  to  get  a  precision  of  up  to  around  0.5  with  very  high  recall.\n",
      "\n",
      "- Answer with loss 1.3877676725387573 proba 0.3155706524848938: \n",
      "If this\n",
      "is done by using a rule-based heuristic, like dropping common suffixes, it is usually\n",
      "referred  to  as  stemming. If  instead  a  dictionary  of known  word forms is  used  (an\n",
      "explicit and human-verified system), and the role of the word in the sentence is taken\n",
      "into account, the process is referred to as lemmatization and the standardized form of the  word  is  referred  to  as the  lemma. Both  processing  methods,  lemmatization  and\n",
      "stemming,  are  forms  of  normalization that  try  to  extract  some  normal  form  of  a\n",
      "word.\n",
      "\n",
      "- Answer with loss 1.3882927894592285 proba 0.31573811173439026: \n",
      "325 Representing Text Data as a Bag of Words                                                                 327 Applying Bag-of-Words to a Toy Dataset 329 Bag-of-Words for Movie Reviews 330 Stopwords 334 Rescaling the Data with tf–idf                                                                                      336\n",
      "Investigating Model Coefficients                                                                                  338\n",
      "Bag-of-Words with More Than One Word (n-Grams)                                            339\n",
      "Advanced Tokenization, Stemming, and Lemmatization                                        344\n",
      "\n",
      "- Answer with loss 1.398380994796753 proba 0.3083289563655853: \n",
      "indicates  that  going  from  three  to  two  clusters meant  merging  some  very\n",
      "far-apart points. We  see  this  again  at  the  top  of  the  chart,  where  merging  the  two\n",
      "remaining clusters into a single cluster again bridges a relatively large distance. Unfortunately,  agglomerative  clustering  still  fails  at  separating  complex  shapes  like\n",
      "the two_moons dataset. But the same is not true for the next algorithm we will look at,\n",
      "DBSCAN. DBSCAN Another  very  useful clustering  algorithm  is  DBSCAN  (\n",
      "\n",
      "- Answer with loss 1.39885675907135 proba 0.3105930984020233: \n",
      "we  introduced  the  Pipeline  class,  a  general-purpose  tool  to  chain\n",
      "together multiple processing steps in a machine learning workflow. Real-world appli‐\n",
      "cations of machine learning rarely involve an isolated use of a model, and instead are\n",
      "a sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\n",
      "into a single Python object that adheres to the familiar scikit-learn interface of fit,\n",
      "predict,  and  transform. In  particular  when  doing  model  evaluation  using  cross-\n",
      "validation and parameter selection using grid search, using the Pipeline class to cap‐\n",
      "ture all the processing steps is essential for proper evaluation.\n",
      "\n",
      "- Answer with loss 1.4011120796203613 proba 0.31584474444389343: \n",
      "We  can  see  the  names  of  the\n",
      "steps by looking at the steps attribute: Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler(copy= True, feature_range=(0, 1))),\n",
      " ('svc', SVC(C=100, cache_size=200, class_weight= None, coef0=0.0,\n",
      " \n",
      "     decision_function_shape =None, degree=3, gamma='auto',\n",
      "             kernel='rbf', max_iter=-1, probability=False,\n",
      "             random_state= None, shrinking= True, tol=0.001,\n",
      "             verbose=False))] The steps are named minmaxscaler and svc. In general, the step names are just low‐\n",
      "ercase versions of the class names.\n",
      "\n",
      "- Answer with loss 1.4050637483596802 proba 0.3158993721008301: \n",
      "299 Using Evaluation Metrics in Model Selection 300\n",
      "Summary and Outlook                                                                                                  302 305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters\n",
      "\n",
      "- Answer with loss 1.4187955856323242 proba 0.3102771043777466: \n",
      "Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised  learning  problem. In  this  problem,  we  want  to  predict  one  of  several\n",
      "options (the species of iris). This is an example of a classification problem. The possi‐\n",
      "ble  outputs  (different  species  of  irises)  are  called  classes. Every  iris  in the  dataset\n",
      "belongs to one of three classes, so this problem is a three-class classification problem. The desired output for a single data point (an iris) is the species of this flower.\n",
      "\n",
      "- Answer with loss 1.4189916849136353 proba 0.308317631483078: \n",
      "For each\n",
      "bin,  they  predict  a  constant  value. As  features  are  constant  within  each  bin,  any\n",
      "model must predict the same value for all points within a bin. Comparing what the\n",
      "models  learned  before  binning  the  features and  after,  we  see  that  the  linear  model\n",
      "became much more flexible, because it now has a different value for each bin, while\n",
      "the decision tree model got much less flexible. Binning features generally has no ben‐\n",
      "eficial effect for tree-based models, as these models can learn to split up the data any‐\n",
      "\n",
      "- Answer with loss 1.421269416809082 proba 0.31534886360168457: \n",
      "308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters 317 Grid-Searching Which Model To Use                                                                         319\n",
      "Summary and Outlook                                                                                                  320\n",
      "\n",
      "- Answer with loss 1.4281458854675293 proba 0.31071069836616516: \n",
      "that can have the values 0 and 1. The values 0 and 1 make sense in the formula for\n",
      "linear  binary  classification  (and  for  all  other  models  in  scikit-learn), and  we  can\n",
      "represent any number of categories by introducing one new feature per category, as\n",
      "described here. Let’s  say  for  the  workclass feature we  have  possible  values  of  \"Government\n",
      "Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\n",
      "rated\". To encode these four possible values, we create four new features, called \"Gov\n",
      "ernment Employee\",  \"Private Employee\",  \"Self Employed\",\n",
      "\n",
      "- Answer with loss 1.4288127422332764 proba 0.30721187591552734: \n",
      "which is called y. The array X is a two-dimensional array of features, with one row per\n",
      "data point and one column per feature. The array y is a one-dimensional array, which\n",
      "here contains one class label, an integer ranging from 0 to 2, for each of the samples. We split our dataset into a training set, to build our model, and a test set, to evaluate\n",
      "how well our model will generalize to new, previously unseen data. We chose the k-nearest neighbors classification algorithm, which makes predictions\n",
      "for a new data point by considering its closest neighbor(s) in the training set.\n",
      "\n",
      "- Answer with loss 1.4303319454193115 proba 0.314421683549881: \n",
      "print(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test))) Grid-Search with accuracy\n",
      "Best parameters: {'gamma': 0.0001} Best cross-validation score (accuracy)): 0.970 Test set AUC: 0.992 Test set accuracy: 0.973 # using AUC scoring instead:\n",
      "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"\\nGrid-Search with AUC\") print(\"Best parameters:\", grid.best_params_) print(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_)) print(\"Test set AUC: {:.3f}\".format\n",
      "\n",
      "- Answer with loss 1.4334030151367188 proba 0.29998984932899475: \n",
      "when  the  goal  is  to  limit  the  number  of\n",
      "false  positives. As  an  example,  imagine  a  model  for  predicting  whether  a  new  drug\n",
      "will  be  effective  in  treating  a  disease  in  clinical  trials. Clinical  trials  are  notoriously\n",
      "expensive, and a pharmaceutical company will only want to run an experiment if it is\n",
      "very sure that the drug will actually work. Therefore, it is important that the model\n",
      "does not produce many false positives—in other words, that it has a high precision. Precision is also known as positive predictive value (PPV).\n",
      "\n",
      "- Answer with loss 1.43583345413208 proba 0.2985787093639374: \n",
      "Once a particular goal is set—say, a particular recall or precision value\n",
      "for a class—a threshold can be set appropriately. It is always possible to set a thresh‐\n",
      "old to fulfill a particular target, like 90% recall. The hard part is to develop a model\n",
      "that  still  has  reasonable  precision  with  this  threshold—if  you  classify  everything  as\n",
      "positive, you will have 100% recall, but your model will be useless. Setting a requirement on a classifier like 90% recall is often called setting the operat‐\n",
      "ing point.\n",
      "\n",
      "- Answer with loss 1.438578724861145 proba 0.3112275004386902: \n",
      "25 Classification and Regression 25\n",
      "Generalization, Overfitting, and Underfitting                                                             26\n",
      "Relation of Model Complexity to Dataset Size                                                         29\n",
      "Supervised Machine Learning Algorithms                                                                   29 Some Sample Datasets 30\n",
      "k-Nearest Neighbors 35 Linear Models 45 Naive Bayes Classifiers 68 Decision Trees 70\n",
      "Ensembles of Decision Trees 83 Kernelized Support Vector Machines\n",
      "\n",
      "- Answer with loss 1.4397008419036865 proba 0.3058614134788513: \n",
      "employ  A/B  testing,  a  form  of  blind  user  study. In A/B testing, without their knowledge a selected portion of users will be provided with\n",
      "a  website  or  service  using  algorithm  A,  while  the  rest  of  the  users  will  be  provided\n",
      "with algorithm B. For both groups, relevant success metrics will be recorded for a set\n",
      "period of time. Then, the metrics of algorithm A and algorithm B will be compared,\n",
      "and a selection between the two approaches will be made according to these metrics. Using  A/B  testing  enables  us  to  evaluate  the  algorithms  “in  the  wild,”  which  might\n",
      "help  us  to  discover  unexpected  consequences  when  users  are  interacting  with  our\n",
      "model.\n",
      "\n",
      "- Answer with loss 1.442122220993042 proba 0.3020716905593872: \n",
      "(each\n",
      "email, each customer, each transaction) is a row, and each property that describes that\n",
      "data point (say, the age of a customer or the amount or location of a transaction) is a\n",
      "column. You  might  describe  users  by  their  age,  their  gender,  when  they  created  an\n",
      "account, and how often they have bought from your online shop. You might describe\n",
      "the image of a tumor by the grayscale values of each pixel, or maybe by using the size,\n",
      "shape, and color of the tumor. Each  entity  or  row  here  is  known  as  a  sample  (or  data  point)  in  machine  learning,\n",
      "while the columns—the properties that describe these entities—are called features.\n",
      "\n",
      "- Answer with loss 1.4470685720443726 proba 0.30691275000572205: \n",
      "We need to specify for each parameter which step of the pipeline it\n",
      "belongs to. Both parameters that we want to adjust,  C and  gamma, are parameters of\n",
      "SVC, the second step. We gave this step the name \"svm\". The syntax to define a param‐\n",
      "eter grid for a pipeline is to specify for each parameter the step name, followed by __\n",
      "(a double underscore), followed by the parameter name. To search over the C param‐\n",
      "eter of SVC we therefore have to use \"svm__C\" as the key in the parameter grid dictio‐\n",
      "nary, and similarly for gamma:\n",
      "\n",
      "- Answer with loss 1.4512770175933838 proba 0.30206742882728577: \n",
      "Agglomerative Clustering Agglomerative clustering  refers  to  a  collection  of  clustering  algorithms that  all  build\n",
      "upon the same principles : the algorithm starts by declaring each point its own cluster,\n",
      "and then merges the two most similar clusters until some stopping criterion is satis‐\n",
      "fied. The stopping criterion implemented in scikit-learn is the number of clusters,\n",
      "so  similar  clusters  are  merged  until  only  the  specified  number  of  clusters  are  left. There are several linkage criteria that specify how exactly the “most similar cluster” is\n",
      "measured.\n",
      "\n",
      "- Answer with loss 1.4571621417999268 proba 0.30980098247528076: \n",
      "Using Evaluation Metrics in Model Selection 300\n",
      "Summary and Outlook                                                                                                  302 305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters\n",
      "\n",
      "- Answer with loss 1.459634780883789 proba 0.30679240822792053: \n",
      "260 Simple Grid Search 261 The Danger of Overfitting the Parameters and the Validation Set                      261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring 275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification 276\n",
      "Metrics for Multiclass Classification 296 Regression Metrics 299 Using Evaluation Metrics in Model Selection\n",
      "\n",
      "- Answer with loss 1.4610286951065063 proba 0.29840561747550964: \n",
      "Luckily, there is\n",
      "a clever mathematical trick that allows us to learn a classifier in a higher-dimensional\n",
      "space without actually computing the new, possibly very large representation. This is\n",
      "known as the kernel trick, and it works by directly computing the distance (more pre‐ cisely, the scalar products) of the data points for the expanded feature representation,\n",
      "without ever actually computing the expansion. There are two ways to map your data into a higher-dimensional space that are com‐\n",
      "monly used with support vector machines: the polynomial kernel, which computes all\n",
      "possible polynomials up to a certain degree of the original features (like feature1\n",
      "\n",
      "- Answer with loss 1.4610354900360107 proba 0.2930506467819214: \n",
      "One possible mistake is that a healthy patient will be classified as positive, leading to\n",
      "additional testing. This leads to some costs and an inconvenience for the patient (and\n",
      "possibly some mental distress). An incorrect positive prediction is called a false posi‐\n",
      "tive. The other possible mistake is that a sick patient will be classified as negative, and\n",
      "will  not  receive  further  tests  and  treatment. The  undiagnosed  cancer might  lead  to\n",
      "serious  health  issues,  and  could  even  be  fatal.\n",
      "\n",
      "- Answer with loss 1.4629476070404053 proba 0.30082598328590393: \n",
      "we  expect  simple  models  to  generalize\n",
      "better to new data. If the rule was “People older than 50 want to buy a boat,” and this\n",
      "would explain the behavior of all the customers, we would trust it more than the rule\n",
      "involving children and marital status in addition to age. Therefore, we always want to\n",
      "find  the  simplest  model. Building  a  model that  is  too  complex  for  the  amount  of\n",
      "information we have, as our novice data scientist did, is called overfitting. Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data.\n",
      "\n",
      "- Answer with loss 1.463629961013794 proba 0.30007022619247437: \n",
      "we  can  adjust  the  number  of  folds that  are  used  in\n",
      "cross_val_score  using  the  cv  parameter. However,  scikit-learn  allows  for  much\n",
      "finer control over what happens during the splitting of the data by providing a cross-\n",
      "validation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\n",
      "validation  for  regression  and  stratified k-fold  for  classification  work  well, but  there\n",
      "are some cases where you might want to use a different strategy. Say, for example, we\n",
      "want to use the standard k-fold cross-validation on a classification dataset to repro‐\n",
      "duce someone else\n",
      "\n",
      "- Answer with loss 1.4656670093536377 proba 0.31015339493751526: \n",
      "axes[0].set_title(\"Original Data\") # scale the data using MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # visualize the properly scaled data\n",
      "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
      "                c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
      "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
      "                c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
      "axes[1].set_title(\"Scaled Data\")\n",
      "\n",
      "- Answer with loss 1.4659770727157593 proba 0.3038386404514313: \n",
      "* x[2] + w[3, 0] * x[3])\n",
      "h[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3]) ŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2] Here,  w  are  the  weights  between  the  input x and  the  hidden  layer  h,  and  v  are  the\n",
      "weights between the hidden layer h and the output ŷ. The weights v and w are learned\n",
      "from data , x are the input features, ŷ is the computed output, and h are intermediate\n",
      "computations. An important parameter that needs to be set by the user is the number\n",
      "of nodes in the hidden layer.\n",
      "\n",
      "- Answer with loss 1.4673428535461426 proba 0.3062884211540222: \n",
      "In this chapter, we will expand on two aspects of this evaluation. We will first intro‐\n",
      "duce  cross-validation,  a  more  robust  way to  assess  generalization  performance,  and discuss methods to evaluate classification and regression performance that go beyond\n",
      "the default measures of accuracy and R2 provided by the score method. Cross-Validation\n",
      "Cross-validation is a statistical method of evaluating generalization performance that\n",
      "is more stable and thorough than using a split into a training and a test set.\n",
      "\n",
      "- Answer with loss 1.4708292484283447 proba 0.30055317282676697: \n",
      "Rescaling the Data with tf–idf Instead  of  dropping  features  that  are  deemed  unimportant,  another  approach  is  to\n",
      "rescale features by how informative we expect them to be. One of the most common\n",
      "ways  to  do  this  is  using  the  term  frequency–inverse  document  frequency  (tf–idf) method. The intuition of this method is to give high weight to any term that appears\n",
      "often in a particular document, but not in many documents in the corpus. If a word\n",
      "appears often in a particular document, but not in very many documents, it is likely\n",
      "to be very descriptive of the content of that document.\n",
      "\n",
      "- Answer with loss 1.4714213609695435 proba 0.30591312050819397: \n",
      "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "\n",
      "- Answer with loss 1.471429705619812 proba 0.3034111261367798: \n",
      "But  for  now,  let’s  get  to  the  algorithms  themselves. First, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\n",
      "vious chapter. k-Nearest Neighbors The  k-NN  algorithm  is  arguably  the  simplest  machine  learning  algorithm. Building the  model  consists  only  of  storing  the  training  dataset. To  make  a  prediction  for  a\n",
      "new data point, the algorithm finds the closest data points in the training dataset—its\n",
      "“nearest neighbors.” In its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\n",
      "bor, which is the closest training data point to the point we want to make a prediction\n",
      "for.\n",
      "\n",
      "- Answer with loss 1.472642183303833 proba 0.3062029778957367: \n",
      "This means when doing a split into a training and a\n",
      "test set, we want to use all the data up to a certain date as the training set and all the\n",
      "data past that date as the test set. This is how we would usually use time series predic‐\n",
      "tion: given everything that we know about rentals in the past, what do we think will happen tomorrow? We will use the first 184 data points, corresponding to the first 23\n",
      "days,  as  our  training  set,  and  the  remaining  64  data  points,  corresponding  to  the\n",
      "remaining 8 days, as our test set.\n",
      "\n",
      "- Answer with loss 1.4727299213409424 proba 0.30124858021736145: \n",
      "Therefore, it is important to keep a separate test set, which is only used for the final\n",
      "evaluation. It is good practice to do all exploratory analysis and model selection using\n",
      "the combination of a training and a validation set, and reserve the test set for a final\n",
      "evaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\n",
      "ing more than one model on the test set and choosing the better of the two will result\n",
      "in an overly optimistic estimate of how accurate the model is.\n",
      "\n",
      "- Answer with loss 1.479353666305542 proba 0.2959091067314148: \n",
      "it is often a good idea to inspect the data,\n",
      "to see if the task is easily solvable without machine learning, or if the desired infor‐\n",
      "mation might not be contained in the data. Additionally, inspecting your data is a good way to find abnormalities and peculiari‐\n",
      "ties. Maybe some of your irises were measured using inches and not centimeters, for\n",
      "example. In the real world, inconsistencies in the data and unexpected measurements\n",
      "are very common. One of the best ways to inspect data is to visualize it.\n",
      "\n",
      "- Answer with loss 1.4846998453140259 proba 0.3061595559120178: \n",
      "Stemming  is  always  restricted  to  trimming  the  word  to  a  stem, so  \"was\"  becomes\n",
      "\"wa\",  while  lemmatization  can  retrieve  the  correct  base  verb  form,  \"be\". Similarly,\n",
      "lemmatization  can  normalize  \"worse\"  to  \"bad\",  while  stemming  produces  \"wors\". Another major difference is that stemming reduces both occurrences of \"meeting\" to\n",
      "\"meet\". Using  lemmatization,  the  first  occurrence  of  \"meeting\"  is  recognized  as  a noun and left as is, while the second occurrence is recognized as a verb and reduced\n",
      "to  \"meet\".\n",
      "\n",
      "- Answer with loss 1.4867775440216064 proba 0.3016873300075531: \n",
      "dual_coef_ attribute, 98 for binary classification, 276-296\n",
      "for multiclass classification, 296-299\n",
      "metric selection, 275\n",
      "model selection and, 300\n",
      "regression metrics, 299\n",
      "testing production systems, 359 F\n",
      "f(x)=y formula, 18\n",
      "facial recognition, 147, 157\n",
      "factor analysis (FA), 163\n",
      "false positive rate (FPR), 292\n",
      "false positive/false negative errors, 277\n",
      "feature extraction/feature engineering, 211-250\n",
      "(see also data representation; text data) augmenting data with, 211\n",
      "automatic feature selection, 236-241\n",
      "categorical features, 212-220\n",
      "continuous vs. discrete features, 211\n",
      "defined, 4, 34, 211\n",
      "interaction features, 224-232\n",
      "with non-negative matrix factorization, 156\n",
      "overview of, 250\n",
      "polynomial features, 224-232\n",
      "with principal component analysis, 147\n",
      "univariate nonlinear transformations,\n",
      "\n",
      "- Answer with loss 1.4888122081756592 proba 0.3048485219478607: \n",
      "254 Stratified k-Fold Cross-Validation and Other Strategies 254 Grid Search 260 Simple Grid Search 261 The Danger of Overfitting the Parameters and the Validation Set                      261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring 275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification 276\n",
      "Metrics for Multiclass Classification\n",
      "\n",
      "- Answer with loss 1.4963616132736206 proba 0.3045556843280792: \n",
      "Regression Metrics 299 Using Evaluation Metrics in Model Selection 300\n",
      "Summary and Outlook                                                                                                  302 305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315\n",
      "\n",
      "- Answer with loss 1.501011848449707 proba 0.29153597354888916: \n",
      "As  one  would  expect,  the  training  score  is  higher  than  the  test  score  for  all  dataset\n",
      "sizes, for both ridge and linear regression. Because ridge is regularized, the training\n",
      "score of ridge is lower than the training score for linear regression across the board. However, the test score for ridge is better, particularly for small subsets of the data. For less than 400 data points, linear regression is not able to learn anything. As more\n",
      "and  more  data  becomes  available  to  the  model,  both  models  improve,  and  linear\n",
      "regression catches up with ridge in the end.\n",
      "\n",
      "- Answer with loss 1.5032694339752197 proba 0.2943960130214691: \n",
      "that  contains  only  8 benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 1.5090171098709106 proba 0.3033045530319214: \n",
      "306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters 317 Grid-Searching Which Model To Use                                                                         319\n",
      "Summary and Outlook                                                                                                  320\n",
      "\n",
      "- Answer with loss 1.519224762916565 proba 0.29973724484443665: \n",
      "c =y_pred, s=60, cmap='Paired') plt.scatter(kmeans.cluster_centers _ [:, 0], kmeans.cluster_centers_ [:, 1], s=60,\n",
      "            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired')\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "print(\"Cluster memberships:\\n{}\".format(y_pred)) Cluster memberships: [9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\n",
      " 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\n",
      " 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\n",
      " 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\n",
      " 3 5 0\n",
      "\n",
      "- Answer with loss 1.520074725151062 proba 0.29528918862342834: \n",
      "is unsuper‐\n",
      "vised learning  algorithms. Unsupervised learning  subsumes  all  kinds  of  machine\n",
      "learning where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data.\n",
      "\n",
      "- Answer with loss 1.522072672843933 proba 0.29369738698005676: \n",
      "where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data. A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a  new  way  to  represent  this  data\n",
      "\n",
      "- Answer with loss 1.5302399396896362 proba 0.2958301305770874: \n",
      "119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230, MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292 RandomForestClassifier, 84-86, 238, 290, RandomForestRegressor, 84, 231, 240\n",
      "RFE, 240-241\n",
      "Ridge, 49, 67, 112, 231, 234, 310, 317-319\n",
      "RobustScaler, 133\n",
      "roc_auc_score, 294-301\n",
      "\n",
      "- Answer with loss 1.5318048000335693 proba 0.29117199778556824: \n",
      "but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 1.5346310138702393 proba 0.2906014621257782: \n",
      "The  price  paid  for  this  efficiency is  that  naive  Bayes  models  often  provide\n",
      "generalization  performance that  is  slightly  worse  than  that  of  linear  classifiers  like\n",
      "LogisticRegression and LinearSVC. The reason that naive Bayes models are so efficient is that they learn parameters by\n",
      "looking  at each  feature  individually  and  collect  simple  per-class  statistics  from  each\n",
      "feature. There  are  three  kinds  of  naive  Bayes  classifiers implemented  in  scikit- learn: GaussianNB, BernoulliNB, and MultinomialNB.\n",
      "\n",
      "- Answer with loss 1.5381189584732056 proba 0.28877609968185425: \n",
      "benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 1.53960382938385 proba 0.29495668411254883: \n",
      "bag-of-words representation, 327-334\n",
      "examples of, 323\n",
      "model coefficients, 338\n",
      "overview of, 355\n",
      "rescaling data with tf-idf, 336-338\n",
      "sentiment analysis example, 325\n",
      "stopwords, 334\n",
      "topic modeling and document clustering, time series predictions, 363\n",
      "tokenization, 328, 344-347\n",
      "top nodes, 72\n",
      "topic modeling, with LDA, 347-355\n",
      "training data, 17\n",
      "train_test_split function, 254\n",
      "transform method, 135, 312, 334\n",
      "transformations\n",
      "selecting, 235\n",
      "univariate nonlinear, 232-236\n",
      "unsupervised, 131 agglomerative clustering, 182-187\n",
      "clustering, 168-207\n",
      "DBSCAN, 187-190\n",
      "k-means clustering, 168-181\n",
      "manifold learning with t-SNE, 163-168\n",
      "non-negative matrix factorization,\n",
      "\n",
      "- Answer with loss 1.5435479879379272 proba 0.29159078001976013: \n",
      "This  often  leads  to  clusters that  are  rela‐\n",
      "tively equally sized. ward works on most datasets, and we will use it in our examples. If the clusters have\n",
      "very  dissimilar  numbers  of  members  (if  one  is  much  bigger  than  all  the  others,  for\n",
      "example), average or complete might work better. Initially,  each  point  is  its  own  cluster. Then,  in  each  step,  the  two  clusters  that  are\n",
      "closest  are  merged. In  the  first  four  steps,  two  single-point  clusters  are  picked  and\n",
      "these  are  joined  into  two-point  clusters.\n",
      "\n",
      "- Answer with loss 1.5467209815979004 proba 0.30003947019577026: \n",
      "feature_names attribute, 33\n",
      "feed-forward neural networks, 104\n",
      "fit method, 21, 68, 119, 135\n",
      "fit_transform method, 138\n",
      "floating-point numbers, 26 get_dummies function, 218\n",
      "get_support method of feature selection, 237\n",
      "gradient boosted regression trees\n",
      "for feature selection, 220-224 learning_rate parameter, 89\n",
      "parameters, 91\n",
      "vs. random forests, 88\n",
      "strengths and weaknesses, 91\n",
      "training set accuracy, 90 accessing pipeline attributes, 315\n",
      "alternate strategies for, 272\n",
      "avoiding overfitting, 261\n",
      "model selection with, 319\n",
      "nested cross-validation, 272\n",
      "parallelizing with cross-validation, 274\n",
      "pipeline preprocessing, 317\n",
      "searching non-grid spaces, 271\n",
      "simple example of, 261\n",
      "tuning parameters with, 260\n",
      "using pipelines in, 309-311\n",
      "with cross-validation, 263-275\n",
      "\n",
      "- Answer with loss 1.5520906448364258 proba 0.29387158155441284: \n",
      "F\n",
      "f(x)=y formula, 18\n",
      "facial recognition, 147, 157\n",
      "factor analysis (FA), 163\n",
      "false positive rate (FPR), 292\n",
      "false positive/false negative errors, 277\n",
      "feature extraction/feature engineering, 211-250\n",
      "(see also data representation; text data) augmenting data with, 211\n",
      "automatic feature selection, 236-241\n",
      "categorical features, 212-220\n",
      "continuous vs. discrete features, 211\n",
      "defined, 4, 34, 211\n",
      "interaction features, 224-232\n",
      "with non-negative matrix factorization, 156\n",
      "overview of, 250\n",
      "polynomial features, 224-232\n",
      "with principal component analysis, 147\n",
      "univariate nonlinear transformations,\n",
      "\n",
      "- Answer with loss 1.5544265508651733 proba 0.29407399892807007: \n",
      "is  96%:  slightly  lower  than  before,  probably\n",
      "because we used less data to train the model (X_train is smaller now because we split\n",
      "our dataset twice). However, the score on the test set—the score that actually tells us\n",
      "how well we generalize—is even lower, at 92%. So we can only claim to classify new\n",
      "data 92% correctly, not 97% correctly as we thought before! The distinction between the training set, validation set, and test set is fundamentally\n",
      "important  to  applying  machine  learning  methods  in  practice.\n",
      "\n",
      "- Answer with loss 1.5554211139678955 proba 0.28891438245773315: \n",
      "all  clusters  increases  the  least. This  often  leads  to  clusters that  are  rela‐\n",
      "tively equally sized. ward works on most datasets, and we will use it in our examples. If the clusters have\n",
      "very  dissimilar  numbers  of  members  (if  one  is  much  bigger  than  all  the  others,  for\n",
      "example), average or complete might work better. Initially,  each  point  is  its  own  cluster. Then,  in  each  step,  the  two  clusters  that  are\n",
      "closest  are  merged. In  the  first  four  steps,  two  single-point  clusters  are  picked  and\n",
      "these  are  joined  into  two-point  clusters.\n",
      "\n",
      "- Answer with loss 1.5570296049118042 proba 0.2892630696296692: \n",
      "Often this is hard, as assess‐\n",
      "ing the business impact of a particular model might require putting it in production\n",
      "in a real-life system. In the early stages of development, and for adjusting parameters, it is often infeasible\n",
      "to put models into production just for testing purposes, because of the high business\n",
      "or  personal  risks that  can  be  involved. Imagine  evaluating  the  pedestrian  avoidance\n",
      "capabilities  of  a  self-driving  car  by  just  letting  it  drive  around,  without  verifying  it\n",
      "first; if your model is bad, pedestrians will be in trouble!\n",
      "\n",
      "- Answer with loss 1.5627899169921875 proba 0.2911891043186188: \n",
      "However,  you  should\n",
      "not test different parameter ranges on the final test set—as we discussed earlier, eval‐ In  some  cases,  trying  all  possible  combinations  of  all  parameters  as  GridSearchCV usually  does,  is  not  a  good  idea. For  example,  SVC  has a  kernel  parameter,  and\n",
      "depending  on  which  kernel  is  chosen,  other  parameters  will  be  relevant. If  ker\n",
      "nel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\n",
      "both the C and gamma parameters are used (but not other parameters like degree).\n",
      "\n",
      "- Answer with loss 1.5648670196533203 proba 0.29553624987602234: \n",
      "print(\"Number of features used: {}\".format(np.sum(lasso001.coef _ != 0))) A lower alpha allowed us to fit a more complex model, which worked better on the\n",
      "training and test data. The performance is slightly better than using Ridge, and we are\n",
      "using only 33 of the 105 features. This makes this model potentially easier to under‐\n",
      "stand. lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train) print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train))) print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
      "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef\n",
      "\n",
      "- Answer with loss 1.5670558214187622 proba 0.291016161441803: \n",
      "Unsupervised learning  subsumes  all  kinds  of  machine\n",
      "learning where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data.\n",
      "\n",
      "- Answer with loss 1.5690134763717651 proba 0.2937840521335602: \n",
      "363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366 Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   367\n",
      "\n",
      "- Answer with loss 1.5709551572799683 proba 0.28984278440475464: \n",
      "we  have  possible  values  of  \"Government\n",
      "Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\n",
      "rated\". To encode these four possible values, we create four new features, called \"Gov\n",
      "ernment Employee\",  \"Private Employee\",  \"Self Employed\", and  \"Self Employed\n",
      "Incorporated\". A  feature  is  1  if  workclass  for  this  person has  the  corresponding\n",
      "value and 0 otherwise, so exactly one of the four new features will be 1 for each data\n",
      "point. This is why this is called one-hot or one-out-of-N encoding.\n",
      "\n",
      "- Answer with loss 1.573655605316162 proba 0.28865981101989746: \n",
      "For  this  kind  of  data,  linear  models  might  be  more  appropriate. Random forests usually work well even on very large datasets, and training can easily\n",
      "be parallelized over many CPU cores within a powerful computer. However, random\n",
      "forests require more memory and are slower to train and to predict than linear mod‐\n",
      "els. If time and memory are important in an application, it might make sense to use a\n",
      "linear model instead. The  important  parameters  to  adjust  are  n_estimators,  max_features,  and  possibly\n",
      "pre-pruning options like max_depth.\n",
      "\n",
      "- Answer with loss 1.5760432481765747 proba 0.28788191080093384: \n",
      "Kernelized support vector machines are powerful models and perform well on a vari‐\n",
      "ety of datasets. SVMs allow for complex decision boundaries, even if the data has only\n",
      "a few features. They work well on low-dimensional and high-dimensional data (i.e.,\n",
      "few and many features), but don’t scale very well with the number of samples. Run‐\n",
      "ning an SVM on data with up to 10,000 samples might work well, but working with\n",
      "datasets  of  size  100,000  or  more  can  become  challenging  in  terms  of  runtime  and\n",
      "memory usage.\n",
      "\n",
      "- Answer with loss 1.577042818069458 proba 0.29271209239959717: \n",
      "One of the simplest and most widely used algorithms for all of these is principal com‐\n",
      "ponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\n",
      "zation  (NMF),  which  is  commonly  used  for  feature  extraction,  and  t-SNE,  which is\n",
      "commonly used for visualization using two-dimensional scatter plots. Principal Component Analysis (PCA)\n",
      "Principal component analysis is a method that rotates the dataset in a way such that the  rotated  features  are  statistically  uncorrelated.\n",
      "\n",
      "- Answer with loss 1.5789291858673096 proba 0.2919086217880249: \n",
      "359 Testing Production Systems 359 Building Your Own Estimator 360 Where to Go from Here                                                                                                361\n",
      "Theory 361\n",
      "Other Machine Learning Frameworks and Packages 362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364\n",
      "\n",
      "- Answer with loss 1.5820977687835693 proba 0.2837706506252289: \n",
      "In fact, every other\n",
      "  movie in the world is better than this one.' You  can  see  that  text_train  is  a  list  of  length  25,000,  where  each  entry  is  a  string\n",
      "containing  a  review. We  printed  the  review  with  index  1. You can  also  see  that  the\n",
      "review contains some HTML line breaks (<br />). While these are unlikely to have a The type of the entries of text_train will depend on your Python version. In Python\n",
      "3, they will be of type bytes which represents a binary encoding of the string data.\n",
      "\n",
      "- Answer with loss 1.5828723907470703 proba 0.28934237360954285: \n",
      "make_blobs, 92, 119, 136, 173-183, 188, 286\n",
      "make_circles, 119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230, MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292 RandomForestClassifier, 84-86, 238, 290, RandomForestRegressor, 84, 231, 240\n",
      "RFE, 240-241\n",
      "Ridge, 49, 67, 112, 231, 234, 310, 317-319\n",
      "RobustScaler, 133\n",
      "roc_auc_score, 294-301\n",
      "\n",
      "- Answer with loss 1.5860215425491333 proba 0.28656885027885437: \n",
      "that  are  in  “crowded”  regions  of the  feature\n",
      "space,  where  many  data  points  are  close  together. These  regions  are  referred  to  as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points), and they\n",
      "are defined as follows. There are two parameters in DBSCAN: min_samples and eps. If there are at least min_samples many data points within a distance of eps to a given\n",
      "data point, that data point is classified as a core sample.\n",
      "\n",
      "- Answer with loss 1.5862780809402466 proba 0.291732519865036: \n",
      "param_C param_gamma params mean_test_score\n",
      "0   0.001 0.001         {'C': 0.001, 'gamma': 0.001}       0.366 1   0.001 0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347\n",
      "\n",
      "- Answer with loss 1.5975629091262817 proba 0.28799161314964294: \n",
      "be  a  NumPy  array  or  SciPy  sparse  matrix that  has  continuous (floating-point) entries. Supervised algorithms also require a y argument, which is a\n",
      "one-dimensional  NumPy  array  containing  target  values  for  regression  or classifica‐\n",
      "tion (i.e., the known output labels or responses). There are two main ways to apply a learned model in scikit-learn. To create a pre‐\n",
      "diction in the form of a new output like y, you use the predict method. To create a\n",
      "new  representation  of  the  input  data  X,  you  use  the  transform  method.\n",
      "\n",
      "- Answer with loss 1.5981013774871826 proba 0.28411194682121277: \n",
      "* 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\n",
      "weights, for a whopping total of 1,101,000—50 times larger than the model with two\n",
      "hidden layers of size 100.\n",
      "\n",
      "- Answer with loss 1.5982128381729126 proba 0.29045090079307556: \n",
      "You can think of that as each point being\n",
      "represented using only a single component, which is given by the cluster center. This\n",
      "view of k-means as a decomposition method, where each point is represented using a\n",
      "single component, is called vector quantization. Let’s do a side-by-side comparison of PCA, NMF, and k-means, showing the compo‐\n",
      "nents  extracted  (Figure  3-30),  as  well  as  reconstructions  of  faces  from  the  test  set\n",
      "using  100  components (Figure  3-31). For  k-means,  the  reconstruction  is  the  closest\n",
      "cluster center found on the training set:\n",
      "\n",
      "- Answer with loss 1.6001085042953491 proba 0.2848096787929535: \n",
      "*  1  =  100  weights  between  the  hidden  layer  and  the  output  layer,  for  a  total  of\n",
      "around 10,100 weights. If you add a second hidden layer with 100 hidden units, there\n",
      "will be another 100 * 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000.\n",
      "\n",
      "- Answer with loss 1.6012779474258423 proba 0.2852762043476105: \n",
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n",
      "  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1} The  bag-of-words  representation  is  stored  in  a  SciPy  sparse  matrix that  only  stores\n",
      "the entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\n",
      "row  for  each  of  the  two  data  points  and  one feature  for  each  of  the  words  in  the\n",
      "vocabulary. A sparse matrix is used as most documents only contain a small subset of\n",
      "the words in the vocabulary, meaning most entries in the feature array are 0.\n",
      "\n",
      "- Answer with loss 1.6031019687652588 proba 0.28825998306274414: \n",
      "It param_C param_gamma params mean_test_score\n",
      "0   0.001 0.001         {'C': 0.001, 'gamma': 0.001}       0.366 1   0.001 0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347\n",
      "\n",
      "- Answer with loss 1.603731632232666 proba 0.2845151126384735: \n",
      "Another  important  parameter  is  max_depth  (or  alternatively  max_leaf_nodes),  to\n",
      "reduce  the  complexity  of  each  tree. Usually  max_depth is  set  very  low  for  gradient\n",
      "boosted models, often not deeper than five splits. Kernelized Support Vector Machines The  next  type  of  supervised  model we  will  discuss is  kernelized  support  vector\n",
      "machines. We explored the use of linear support vector machines for classification in “Linear  models  for  classification”  on  page  56. Kernelized  support  vector  machines\n",
      "(often just referred to as SVMs) are an extension that allows for more complex mod‐\n",
      "els that are not defined simply by hyperplanes in the input space.\n",
      "\n",
      "- Answer with loss 1.6099474430084229 proba 0.2805047631263733: \n",
      "If  you  are  not  married  to\n",
      "Python, you might also consider using R, another lingua franca of data scientists. R is\n",
      "a language designed specifically for statistical analysis and is famous for its excellent\n",
      "visualization capabilities and the availability of many (often highly specialized) statis‐ tical modeling packages. Another  popular  machine  learning  package  is  vowpal  wabbit  (often  called  vw  to\n",
      "avoid possible tongue twisting), a highly optimized machine learning package written\n",
      "in C++ with a command-line interface.\n",
      "\n",
      "- Answer with loss 1.6129266023635864 proba 0.2912904918193817: \n",
      "305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters 317 Grid-Searching Which Model To Use                                                                         319\n",
      "Summary and Outlook                                                                                                  320\n",
      "\n",
      "- Answer with loss 1.6146067380905151 proba 0.28274229168891907: \n",
      "The most important parameters are the num‐\n",
      "ber of layers and the number of hidden units per layer. You should start with one or\n",
      "two hidden layers, and possibly expand from there. The number of nodes per hidden\n",
      "layer is often similar to the number of input features, but rarely higher than in the low\n",
      "to mid-thousands. A helpful measure when thinking about the model complexity of a neural network is\n",
      "the number of weights or coefficients that are learned. If you have a binary classifica‐\n",
      "tion  dataset  with  100  features,\n",
      "\n",
      "- Answer with loss 1.6205955743789673 proba 0.28991350531578064: \n",
      "This\n",
      "is  a  consequence  of  the  default  parameter  settings  for  eps  and  min_samples, which\n",
      "are  not  tuned  for  small  toy  datasets. The  cluster  assignments  for  different  values  of\n",
      "min_samples and eps are shown below, and visualized in Figure 3-37: min_samples: 2 eps: 1.000000 cluster: [-1  0  0 -1 0 -1 1 1  0  1 -1 -1] min_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\n",
      "min_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\n",
      "min_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "min_samples:\n",
      "\n",
      "- Answer with loss 1.6223864555358887 proba 0.2830179035663605: \n",
      "we  know  are  not  interested  in  buying  a  boat.2 The\n",
      "goal  is  to  send  out  promotional  emails  to  people who  are  likely  to  actually  make  a\n",
      "purchase, but not bother those customers who won’t be interested. After looking at the data for a while, our novice data scientist comes up with the fol‐\n",
      "lowing rule: “If the customer is older than 45, and has less than 3 children or is not\n",
      "divorced, then they want to buy a boat.” When asked how well this rule of his does,\n",
      "our data scientist answers, “It’s 100 percent accurate!”\n",
      "\n",
      "- Answer with loss 1.623115062713623 proba 0.2816961109638214: \n",
      "The  k-NN  algorithm  is  arguably  the  simplest  machine  learning  algorithm. Building the  model  consists  only  of  storing  the  training  dataset. To  make  a  prediction  for  a\n",
      "new data point, the algorithm finds the closest data points in the training dataset—its\n",
      "“nearest neighbors.” In its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\n",
      "bor, which is the closest training data point to the point we want to make a prediction\n",
      "for. The prediction is then simply the known output for this training point.\n",
      "\n",
      "- Answer with loss 1.6263201236724854 proba 0.28984344005584717: \n",
      "= -np.inf # iterate over parameters\n",
      "        for parameters in parameter_grid: # accumulate score over inner splits cv_scores = []\n",
      "            # iterate over inner cross-validation\n",
      "            for inner_train, inner_test in inner_cv.split(\n",
      "                    X[training_samples], y[training_samples]) :\n",
      "                # build classifier given parameters and training data clf = Classifier(**parameters)\n",
      "                clf.fit(X[inner_train], y[inner_train]) # evaluate on inner test set\n",
      "                score = clf.score(X[inner_test],\n",
      "\n",
      "- Answer with loss 1.6363933086395264 proba 0.2777261734008789: \n",
      "model using a particular parameter setting on a particular cross-validation split can\n",
      "be  done  completely  independently  from  the  other  parameter  settings  and  models. This makes grid search and cross-validation ideal candidates for parallelization over\n",
      "multiple  CPU  cores  or  over  a  cluster. You  can  make  use  of  multiple  cores  in  Grid\n",
      "SearchCV  and  cross_val_score  by  setting  the  n_jobs  parameter  to the  number  of\n",
      "CPU cores you want to use. You can set n_jobs=-1 to use all available cores.\n",
      "\n",
      "- Answer with loss 1.6443743705749512 proba 0.2855250835418701: \n",
      "The only feature that we are using in our prediction task is the date and time when a\n",
      "particular number of rentals occurred. So, the input feature is the date and time—say,\n",
      "2015-08-01 00:00:00—and the  output  is  the  number  of  rentals  in the  following three hours (three in this case, according to our DataFrame). A  (surprisingly)  common  way that  dates  are  stored  on  computers is  using  POSIX\n",
      "time, which is the number of seconds since January 1970 00:00:00 (aka the beginning\n",
      "of Unix time). As a first try, we can use this single integer feature as our data\n",
      "\n",
      "- Answer with loss 1.6458543539047241 proba 0.28014659881591797: \n",
      "* x[3])\n",
      "h[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3]) ŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2] Here,  w  are  the  weights  between  the  input x and  the  hidden  layer  h,  and  v  are  the\n",
      "weights between the hidden layer h and the output ŷ. The weights v and w are learned\n",
      "from data , x are the input features, ŷ is the computed output, and h are intermediate\n",
      "computations. An important parameter that needs to be set by the user is the number\n",
      "of nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\n",
      "sets and as big as 10,000 for very complex data.\n",
      "\n",
      "- Answer with loss 1.646801471710205 proba 0.2836877703666687: \n",
      "predict method, 22, 37, 68\n",
      "Python 2 vs. Python 3, 12\n",
      "random_state parameter, 18\n",
      "scaling mechanisms in, 139\n",
      "score method, 23, 37, 43\n",
      "transform method, 135\n",
      "user guide, 6\n",
      "versions used, 12 accuracy_score, 193\n",
      "adjusted_rand_score, 191 AgglomerativeClustering, 182, 191, 203-207\n",
      "average_precision_score, 292 BaseEstimator, 360\n",
      "classification_report, 284-288, 298\n",
      "confusion_matrix, 279-299\n",
      "CountVectorizer, 329-355\n",
      "cross_val_score, 253, 256, 300, 307, 360\n",
      "DBSCAN, 187-190\n",
      "DecisionTreeClassifier, 75, 278\n",
      "DecisionTreeRegressor, 75, 80\n",
      "DummyClassifier, 278\n",
      "ElasticNet class,\n",
      "\n",
      "- Answer with loss 1.6476200819015503 proba 0.2767494320869446: \n",
      "As you can see, the random forest gives nonzero importance to many more features\n",
      "than the single tree. Similarly to the single decision tree, the random forest also gives\n",
      "a lot of importance to the “worst radius” feature, but it actually chooses “worst perim‐\n",
      "eter” to be the most informative feature overall. The randomness in building the ran‐\n",
      "dom  forest  forces  the  algorithm  to  consider  many  possible  explanations, the  result\n",
      "being that the random forest captures a much broader picture of the data than a sin‐\n",
      "gle tree.\n",
      "\n",
      "- Answer with loss 1.6493797302246094 proba 0.2851918041706085: \n",
      "275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification 276\n",
      "Metrics for Multiclass Classification 296 Regression Metrics 299 Using Evaluation Metrics in Model Selection 300\n",
      "Summary and Outlook                                                                                                  302 305\n",
      "Parameter Selection with Preprocessing 306 Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface\n",
      "\n",
      "- Answer with loss 1.652733325958252 proba 0.2844977080821991: \n",
      "360 Where to Go from Here                                                                                                361\n",
      "Theory 361\n",
      "Other Machine Learning Frameworks and Packages 362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366\n",
      "\n",
      "- Answer with loss 1.6531124114990234 proba 0.27794742584228516: \n",
      "Comparing and Evaluating Clustering Algorithms One of the challenges in applying clustering algorithms is that it is very hard to assess\n",
      "how  well  an  algorithm worked,  and  to  compare  outcomes  between  different algo‐\n",
      "rithms. After talking about the algorithms behind k-means, agglomerative clustering,\n",
      "and DBSCAN, we will now compare them on some real-world datasets. There  are  metrics that  can  be  used  to  assess  the  outcome  of  a  clustering  algorithm\n",
      "relative to a ground truth clustering, the most important ones being the adjusted rand\n",
      "index (ARI) and normalized mutual information (NMI), which both provide a quanti‐\n",
      "tative measure between 0 and 1.\n",
      "\n",
      "- Answer with loss 1.654880404472351 proba 0.2835075557231903: \n",
      "This  makes  AUC  a\n",
      "much better metric for imbalanced classification problems than accuracy. The AUC\n",
      "can be interpreted as evaluating the ranking of positive samples. It’s equivalent to the\n",
      "probability that a randomly picked point of the positive class will have a higher score\n",
      "according to the classifier than a randomly picked point from the negative class. So, a\n",
      "perfect AUC of 1 means that all positive points have a higher score than all negative\n",
      "points. For  classification  problems  with  imbalanced  classes,\n",
      "\n",
      "- Answer with loss 1.6630477905273438 proba 0.27717795968055725: \n",
      "The  application  of  machine\n",
      "learning methods has in recent years become ubiquitous in everyday life. From auto‐\n",
      "matic  recommendations  of  which  movies  to  watch,  to  what  food  to  order  or  which\n",
      "products  to  buy,  to  personalized  online  radio  and  recognizing  your  friends  in your\n",
      "photos, many modern websites and devices have machine learning algorithms at their\n",
      "core. When  you  look  at  a  complex  website  like  Facebook,  Amazon,  or  Netflix,  it  is\n",
      "very likely that every part of the site contains multiple machine learning models.\n",
      "\n",
      "- Answer with loss 1.664270043373108 proba 0.2829286456108093: \n",
      "22 Evaluating the Model 22\n",
      "Summary and Outlook                                                                                                    23 2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Classification and Regression 25\n",
      "Generalization, Overfitting, and Underfitting                                                             26\n",
      "Relation of Model Complexity to Dataset Size                                                         29\n",
      "Supervised Machine Learning Algorithms                                                                   29\n",
      "\n",
      "- Answer with loss 1.6646244525909424 proba 0.2805406153202057: \n",
      "To create a dataset for building a machine learning model,\n",
      "you  need  to  collect  many  envelopes. Then  you  can  read  the  zip  codes  yourself\n",
      "and store the digits as your desired outcomes. Here the input is the image, and the output is whether the tumor is benign. To\n",
      "create a dataset for building a model, you need a database of medical images. You\n",
      "also  need  an  expert  opinion,  so  a  doctor  needs  to  look  at  all  of  the  images  and\n",
      "decide which tumors are benign and which are not. It might even be necessary to\n",
      "\n",
      "- Answer with loss 1.6720916032791138 proba 0.27779287099838257: \n",
      "that we  did  not  cover  in  this  book. The  first  is  ranking,  in\n",
      "which we want to retrieve answers to a particular query, ordered by their relevance. You’ve  probably  already used  a  ranking  system  today;  this  is  how  search  engines\n",
      "operate. You input a search query and obtain a sorted list of answers, ranked by how\n",
      "relevant they are. A great introduction to ranking is provided in Manning, Raghavan,\n",
      "and Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\n",
      "mender  systems,  which  provide  suggestions  to  users  based  on  their  preferences.\n",
      "\n",
      "- Answer with loss 1.6724094152450562 proba 0.28198274970054626: \n",
      "There was also one digit\n",
      "3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\n",
      "as 2 (thrid column, fourth row). 0       1.00 1.00 1.00 37\n",
      "          1 0.89      0.91 0.90 43 2 0.95 0.93 0.94 44 3 0.90 0.96 0.92 45\n",
      "          4 0.97 1.00 0.99 38\n",
      "          5 0.98 0.98 0.98        48\n",
      "          6 0.96 1.00 0.98 52\n",
      "          7 1.00 0.94 0.97 48\n",
      "          8 0.93 0.90 0.91        48\n",
      "          9 0.96 0.94 0.95 47 Unsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\n",
      "sions with this class.\n",
      "\n",
      "- Answer with loss 1.6741182804107666 proba 0.2781088650226593: \n",
      "points that  are  very\n",
      "different  from the  rest  (like  measurement  errors). These odd  data points are  also\n",
      "called outliers, and can lead to trouble for other scaling techniques. The MinMaxScaler, on the other hand, shifts the data such that all features are exactly\n",
      "between 0 and 1. For the two-dimensional dataset this means all of the data is con‐ 1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of the numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\n",
      "smaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\n",
      "\n",
      "- Answer with loss 1.6840077638626099 proba 0.2815481126308441: \n",
      "DBSCAN Another  very  useful clustering  algorithm  is  DBSCAN  ( which  stands  for  “density-\n",
      "based spatial clustering of applications with noise”). The main benefits of DBSCAN\n",
      "are that it does not require the user to set the number of clusters a priori , it can cap‐\n",
      "ture  clusters  of  complex  shapes, and  it  can  identify  points that  are  not  part  of any\n",
      "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
      "still scales to relatively large datasets. DBSCAN  works  by  identifying  points\n",
      "\n",
      "- Answer with loss 1.685151219367981 proba 0.2759726941585541: \n",
      "If you add a second hidden layer with 100 hidden units, there\n",
      "will be another 100 * 100 = 10,000 weights from the first hidden layer to the second\n",
      "hidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n",
      "1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\n",
      "the hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\n",
      "total of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\n",
      "weights, for a whopping total of 1,101,000—50 times larger than the model with two\n",
      "hidden layers of size 100.\n",
      "\n",
      "- Answer with loss 1.6880346536636353 proba 0.2732926309108734: \n",
      "We can also evaluate the model using the score method, which for regressors returns\n",
      "the R2 score. The R2 score, also known as the coefficient of determination, is a meas‐\n",
      "ure of goodness of a prediction for a regression model, and yields a score between 0\n",
      "and 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds\n",
      "to a constant model that just predicts the mean of the training set responses, y_train: For  our  one-dimensional  dataset,  we  can  see  what the  predictions  look  like  for  all\n",
      "possible feature values (Figure 2-10).\n",
      "\n",
      "- Answer with loss 1.6891686916351318 proba 0.2709212601184845: \n",
      "We  adjusted  two  parameters  here:  the  C  parameter  and  the  gamma  parameter,\n",
      "which we will now discuss in detail. The gamma parameter is the one shown in the formula given in the previous section,\n",
      "which  controls  the  width  of  the  Gaussian  kernel. It  determines  the  scale  of  what  it\n",
      "means for points to be close together. The C parameter is a regularization parameter,\n",
      "similar  to  that  used  in  the  linear  models. It  limits  the  importance  of each  point  (or more precisely, their dual_coef_).\n",
      "\n",
      "- Answer with loss 1.6925426721572876 proba 0.2799774408340454: \n",
      "252\n",
      "Cross-Validation in scikit-learn                                                                                253\n",
      "Benefits of Cross-Validation 254 Stratified k-Fold Cross-Validation and Other Strategies 254 Grid Search 260 Simple Grid Search 261 The Danger of Overfitting the Parameters and the Validation Set                      261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring 275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification\n",
      "\n",
      "- Answer with loss 1.6929962635040283 proba 0.27367669343948364: \n",
      "In\n",
      "this  case,  we’ll  define  10  bins  equally  spaced  between  –3  and  3. We  use  the\n",
      "np.linspace function for this, creating 11 entries, which will create 10 bins—they are\n",
      "the spaces in between two consecutive boundaries: Here, the first bin contains all data points with feature values –3 to –2.68, the second\n",
      "bin contains all points with feature values from –2.68 to –2.37, and so on. What we did here is transform the single continuous input feature in the wave dataset\n",
      "into a categorical feature that encodes which bin a data point is in.\n",
      "\n",
      "- Answer with loss 1.695144534111023 proba 0.27318307757377625: \n",
      "This is the direction (or vector) in the data that contains most\n",
      "of the information, or in other words, the direction along which the features are most\n",
      "correlated with each other. Then, the algorithm finds the direction that contains the\n",
      "most information while being orthogonal (at a right angle) to the first direction. In\n",
      "two dimensions, there is only one possible orientation that is at a right angle, but in\n",
      "higher-dimensional  spaces  there  would  be  (infinitely)  many  orthogonal  directions. Although the two components are drawn as arrows, it doesn’t really matter where the\n",
      "head and the tail are; we could have drawn the first component from the center up to\n",
      "\n",
      "- Answer with loss 1.7043554782867432 proba 0.2733440399169922: \n",
      "The  algorithm  proceeds  by  first  finding  the  direction  of  maximum  variance,\n",
      "labeled “Component 1.” This is the direction (or vector) in the data that contains most\n",
      "of the information, or in other words, the direction along which the features are most\n",
      "correlated with each other. Then, the algorithm finds the direction that contains the\n",
      "most information while being orthogonal (at a right angle) to the first direction. In\n",
      "two dimensions, there is only one possible orientation that is at a right angle, but in\n",
      "higher-dimensional  spaces  there  would  be  (infinitely)  many  orthogonal  directions.\n",
      "\n",
      "- Answer with loss 1.7061578035354614 proba 0.27344590425491333: \n",
      "the  feature\n",
      "space,  where  many  data  points  are  close  together. These  regions  are  referred  to  as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points), and they\n",
      "are defined as follows. There are two parameters in DBSCAN: min_samples and eps. If there are at least min_samples many data points within a distance of eps to a given\n",
      "data point, that data point is classified as a core sample.\n",
      "\n",
      "- Answer with loss 1.707215666770935 proba 0.27058878540992737: \n",
      "Because it is such a common task, there\n",
      "are standard methods in scikit-learn to help you with it. The most commonly used\n",
      "method is grid search, which basically means trying all possible combinations of the\n",
      "parameters of interest. Consider  the  case  of  a  kernel  SVM  with  an  RBF  (radial  basis  function)  kernel,  as\n",
      "implemented in the SVC class. As we discussed in Chapter 2, there are two important\n",
      "parameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\n",
      "want to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\n",
      "same for gamma.\n",
      "\n",
      "- Answer with loss 1.7144616842269897 proba 0.27404671907424927: \n",
      "Another  interesting  aspect  of Figure 2-13 is the decrease in training performance for linear regression. If more data\n",
      "is added, it becomes harder for a model to overfit, or memorize the data. An  alternative  to  Ridge  for  regularizing  linear  regression  is  Lasso. As  with  ridge\n",
      "regression,  using  the  lasso  also  restricts  coefficients  to  be  close  to  zero,  but  in  a\n",
      "slightly different way, called L1 regularization.8 The consequence of L1 regularization\n",
      "is that when using the lasso, some coefficients are exactly zero.\n",
      "\n",
      "- Answer with loss 1.7146631479263306 proba 0.2766285836696625: \n",
      "=y_pred, s=60, cmap='Paired') plt.scatter(kmeans.cluster_centers _ [:, 0], kmeans.cluster_centers_ [:, 1], s=60,\n",
      "            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired')\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "print(\"Cluster memberships:\\n{}\".format(y_pred)) Cluster memberships: [9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\n",
      " 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\n",
      " 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\n",
      " 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\n",
      " 3 5 0\n",
      "\n",
      "- Answer with loss 1.7182514667510986 proba 0.27469319105148315: \n",
      "how  well\n",
      "KNeighborsClassifier does here: from sklearn.neighbors import KNeighborsClassifier # split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(\n",
      "    X_people, y_people, stratify= y_people, random_state=0) # build a KNeighborsClassifier using one neighbor\n",
      "knn = KNeighborsClassifier(n_neighbors=1) knn.fit(X_train, y_train) print(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test))) We obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\n",
      "cation problem (random guessing would give you around 1/62 = 1.5% accuracy), but\n",
      "is also not great.\n",
      "\n",
      "- Answer with loss 1.7195684909820557 proba 0.27413275837898254: \n",
      "means they  are  classified  as  “rest”  by  the  binary  classifier  for  class  2. The  points belonging to class 0 are to the left of the line corresponding to class 1, which means\n",
      "the  binary  classifier  for  class 1 also  classifies  them  as  “rest.” Therefore,  any  point  in\n",
      "this area will be classified as class 0 by the final classifier (the result of the classifica‐\n",
      "tion  confidence  formula  for  classifier  0  is  greater  than  zero,  while  it  is  smaller  than\n",
      "zero for the other two classes).\n",
      "\n",
      "- Answer with loss 1.719632863998413 proba 0.2725839912891388: \n",
      "to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data. A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a  new  way  to  represent  this  data\n",
      "\n",
      "- Answer with loss 1.7223944664001465 proba 0.27239227294921875: \n",
      "In  this  setting,\n",
      "which is known as supervised learning, the user provides the algorithm with pairs of\n",
      "inputs and desired outputs, and the algorithm finds a way to produce the desired out‐\n",
      "put given an input. In particular, the algorithm is able to create an output for an input\n",
      "it has never seen before without any help from a human. Going back to our example\n",
      "of spam classification, using machine learning, the user provides the algorithm with a\n",
      "large  number  of  emails  (which  are  the  input),  together  with  information  about\n",
      "whether  any  of  these  emails  are  spam  (which  is  the  desired  output).\n",
      "\n",
      "- Answer with loss 1.7253392934799194 proba 0.27581870555877686: \n",
      "Precision, recall, and f-score. There are several other ways to summarize the confusion\n",
      "matrix,  with  the  most  common  ones  being  precision  and  recall. Precision  measures how many of the samples predicted as positive are actually positive: Precision  is  used  as  a  performance  metric when  the  goal  is  to  limit  the  number  of\n",
      "false  positives. As  an  example,  imagine  a  model  for  predicting  whether  a  new  drug\n",
      "will  be  effective  in  treating  a  disease  in  clinical  trials.\n",
      "\n",
      "- Answer with loss 1.7263301610946655 proba 0.2800966799259186: \n",
      "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030])) print(\"Every 700th feature:\\n{}\".format(feature_names[::700])) First 50 features:\n",
      "['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08',\n",
      " '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107',\n",
      " '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120',\n",
      " '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16',\n",
      " '160', '1600', '16mm', '16s', '16th']\n",
      "Features 20010 to 20030:\n",
      "\n",
      "- Answer with loss 1.7319731712341309 proba 0.2768782377243042: \n",
      "Generalization, Overfitting, and Underfitting In  supervised  learning,  we  want  to  build  a  model  on  the  training  data and  then  be\n",
      "able to make accurate predictions on new, unseen data that has the same characteris‐\n",
      "tics as the training set that we used. If a model is able to make accurate predictions on\n",
      "unseen  data,  we  say  it  is  able  to  generalize  from the  training  set  to  the  test  set. We\n",
      "want to build a model that is able to generalize as accurately as possible. Usually we build a model in such a way that it can make accurate predictions on the\n",
      "training  set.\n",
      "\n",
      "- Answer with loss 1.737654209136963 proba 0.26883840560913086: \n",
      "16.795  creates  a  node that  contains  only  8 benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 1.738182783126831 proba 0.26796501874923706: \n",
      "Points that are within a dense region are called core samples (or core points), and they\n",
      "are defined as follows. There are two parameters in DBSCAN: min_samples and eps. If there are at least min_samples many data points within a distance of eps to a given\n",
      "data point, that data point is classified as a core sample. Core samples that are closer\n",
      "to each other than the distance eps are put into the same cluster by DBSCAN. The  algorithm  works  by  picking  an  arbitrary  point  to start  with. It then  finds  all\n",
      "points  with  distance\n",
      "\n",
      "- Answer with loss 1.7392646074295044 proba 0.27430248260498047: \n",
      "D\n",
      "data points, defined, 4\n",
      "data representation, 211-250 (see also feature\n",
      "extraction/feature engineering; text data)\n",
      "automatic feature selection, 236-241\n",
      "binning and, 220-224\n",
      "categorical features, 212-220\n",
      "effect on model performance, 211\n",
      "integer features, 218\n",
      "model complexity vs. dataset size, 29\n",
      "overview of, 250\n",
      "table analogy, 4\n",
      "in training vs. test sets, 217\n",
      "understanding your data, 4\n",
      "univariate nonlinear transformations, analyzing, 76\n",
      "building, 71\n",
      "controlling complexity of, 74\n",
      "data representation and, 220-224\n",
      "feature importance in, 77\n",
      "if/else structure of, 70\n",
      "parameters,\n",
      "\n",
      "- Answer with loss 1.7394078969955444 proba 0.2762564718723297: \n",
      "Even faster than linear models, good for very large data‐\n",
      "sets and high-dimensional data. Often less accurate than linear models. Nearly always perform better than a single decision tree, very robust and power‐\n",
      "ful. Don’t need scaling of data. Not good for very high-dimensional sparse data. Often  slightly  more  accurate  than  random  forests. Slower  to  train but  faster  to\n",
      "predict than random forests, and smaller in memory. Need more parameter tun‐\n",
      "ing than random forests. Can build very complex models, particularly for large datasets.\n",
      "\n",
      "- Answer with loss 1.7395074367523193 proba 0.2697072923183441: \n",
      "Because average precision is the\n",
      "area  under  a  curve  that  goes  from  0  to  1,  average  precision  always  returns  a  value\n",
      "between  0  (worst) and  1  (best). The  average  precision  of  a  classifier that  assigns decision_function at random is the fraction of positive samples in the dataset. There is another tool that is commonly used to analyze the behavior of classifiers at\n",
      "different  thresholds:  the  receiver  operating  characteristics  curve,  or  ROC curve  for\n",
      "short. Similar  to  the  precision-recall  curve,  the  ROC  curve\n",
      "\n",
      "- Answer with loss 1.7427879571914673 proba 0.2700510025024414: \n",
      "and  then  be\n",
      "able to make accurate predictions on new, unseen data that has the same characteris‐\n",
      "tics as the training set that we used. If a model is able to make accurate predictions on\n",
      "unseen  data,  we  say  it  is  able  to  generalize  from the  training  set  to  the  test  set. We\n",
      "want to build a model that is able to generalize as accurately as possible. Usually we build a model in such a way that it can make accurate predictions on the\n",
      "training  set. If  the  training  and  test  sets  have  enough  in  common,  we  expect  the\n",
      "model  to  also  be  accurate  on  the  test  set.\n",
      "\n",
      "- Answer with loss 1.7467825412750244 proba 0.270693302154541: \n",
      "Another way that we can get rid of uninformative words is by discarding words that\n",
      "are too frequent to be informative. There are two main approaches: using a language-\n",
      "specific  list  of  stopwords,  or  discarding  words  that  appear  too  frequently. scikit- learn has  a  built-in  list  of  English  stopwords  in the  feature_extraction.text\n",
      "module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\n",
      "\n",
      "- Answer with loss 1.7497586011886597 proba 0.27128729224205017: \n",
      "and  it  can  identify  points that  are  not  part  of any\n",
      "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
      "still scales to relatively large datasets. DBSCAN  works  by  identifying  points that  are  in  “crowded”  regions  of the  feature\n",
      "space,  where  many  data  points  are  close  together. These  regions  are  referred  to  as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty.\n",
      "\n",
      "- Answer with loss 1.7506381273269653 proba 0.2700655162334442: \n",
      "We then fit the scaler using the fit method, applied to the training data. For the Min\n",
      "MaxScaler, the fit method computes the minimum and maximum value of each fea‐\n",
      "ture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\n",
      "scaler is only provided with the data (X_train) when fit is called, and y_train is not\n",
      "used: To apply the transformation that we just learned—that is, to actually scale the training\n",
      "data—we use the transform method of the scaler. The transform method is used in\n",
      "scikit-learn whenever a model returns a new representation of the data:\n",
      "\n",
      "- Answer with loss 1.7517368793487549 proba 0.26710665225982666: \n",
      "In  the  next\n",
      "chapter, we will go into more depth about the different kinds of supervised models in\n",
      "scikit-learn and how to apply them successfully. As we mentioned earlier, supervised machine learning is one of the most commonly\n",
      "used and successful types of machine learning. In this chapter, we will describe super‐\n",
      "vised  learning  in  more  detail and  explain  several  popular  supervised  learning algo‐\n",
      "rithms. We already saw an application of supervised machine learning in Chapter 1:\n",
      "classifying  iris  flowers  into  several  species  using  physical  measurements  of  the\n",
      "flowers.\n",
      "\n",
      "- Answer with loss 1.755953073501587 proba 0.2640829086303711: \n",
      "Instead of a simple split, we can replace each of\n",
      "these splits with cross-validation. The most commonly used form (as described ear‐\n",
      "lier) is a training/test split for evaluation, and using cross-validation on the training\n",
      "set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring\n",
      "function used for model selection and model evaluation. The theory of how to make\n",
      "business  decisions  from the  predictions  of  a  machine  learning  model  is  somewhat\n",
      "beyond the scope of this book.7\n",
      "\n",
      "- Answer with loss 1.756648302078247 proba 0.2657136023044586: \n",
      "the\n",
      "Internet (in particular, ads) will not result in a click. You might need to show a user\n",
      "100  ads  or  articles  before they  find  something  interesting  enough  to  click  on. This\n",
      "results in a dataset where for each 99 “no click” data points, there is 1 “clicked” data\n",
      "point ; in other words, 99% of the samples belong to the “no click” class. Datasets in\n",
      "which  one  class  is  much  more  frequent  than  the  other are  often  called  imbalanced\n",
      "datasets, or datasets with imbalanced classes. In reality, imbalanced data is the norm,\n",
      "and it is rare that the events of interest have equal or even similar frequency in the\n",
      "data.\n",
      "\n",
      "- Answer with loss 1.758396029472351 proba 0.2706466317176819: \n",
      "The output of predict_proba is a probability for each class, and is often more easily\n",
      "understood than the output of decision_function. It is always of shape (n_samples,\n",
      "2) for binary classification: The first entry in each row is the estimated probability of the first class, and the sec‐\n",
      "ond entry is the estimated probability of the second class. Because it is a probability,\n",
      "the output of predict_proba is always between 0 and 1, and the sum of the entries\n",
      "for both classes is always 1: Because the probabilities for the two classes sum to 1, exactly one of the classes will\n",
      "be above 50% certainty.\n",
      "\n",
      "- Answer with loss 1.7628138065338135 proba 0.2638919949531555: \n",
      "There are two basic methods: starting with no fea‐\n",
      "tures  and  adding  features  one  by  one  until  some  stopping  criterion  is  reached,  or\n",
      "starting with all features and removing features one by one until some stopping crite‐\n",
      "rion is reached. Because a series of models are built, these methods are much more\n",
      "computationally expensive than the methods we discussed previously. One particular\n",
      "method  of  this  kind  is  recursive  feature  elimination  (RFE),  which  starts  with  all fea‐ tures,  builds  a  model,  and  discards  the  least  important  feature  according  to  the\n",
      "model.\n",
      "\n",
      "- Answer with loss 1.7640550136566162 proba 0.26787570118904114: \n",
      "For linear models for regression, the output, ŷ, is a linear function of the features: a\n",
      "line, plane, or hyperplane (in higher dimensions). For linear models for classification,\n",
      "the decision boundary is a linear function of the input. In other words, a (binary) lin‐ ear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\n",
      "plane. We will see examples of that in this section. Different  algorithms  choose  different  ways  to  measure what  “fitting  the  training  set\n",
      "well” means.\n",
      "\n",
      "- Answer with loss 1.764315128326416 proba 0.26984062790870667: \n",
      "we  will  discuss is unsuper‐\n",
      "vised learning  algorithms. Unsupervised learning  subsumes  all  kinds  of  machine\n",
      "learning where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning\n",
      "\n",
      "- Answer with loss 1.764862298965454 proba 0.26773858070373535: \n",
      "how  well our  model  generalizes  to  new,  previously  unseen  data. We  are  not  interested  in how  well  our  model  fit  the  training  set,  but rather  in how well it can make predictions for data that was not observed during training. In this chapter, we will expand on two aspects of this evaluation. We will first intro‐\n",
      "duce  cross-validation,  a  more  robust  way to  assess  generalization  performance,  and discuss methods to evaluate classification and regression performance that go beyond\n",
      "the default measures of accuracy and R2 provided by the score method.\n",
      "\n",
      "- Answer with loss 1.7685073614120483 proba 0.269966721534729: \n",
      "decision trees, 70-83\n",
      "gradient boosting, 88-91\n",
      "k-nearest neighbors, 40\n",
      "Lasso, 53-55\n",
      "linear regression (OLS), 47, 220-229\n",
      "neural networks, 104-119\n",
      "random forests, 84-88\n",
      "Ridge, 49-55, 67, 112, 231, 234, 310, BernoulliNB, 68\n",
      "bigrams, 339\n",
      "binary classification, 25, 56, 276-296\n",
      "binning, 144, 220-224\n",
      "bootstrap samples, 84\n",
      "Boston Housing dataset, 34\n",
      "boundary points, 188\n",
      "Bunch objects, 33\n",
      "business metric, 275, 358 categorical data, defined, 324\n",
      "defined, 211\n",
      "encoded as numbers, 218\n",
      "example of, 212\n",
      "representation in training and test sets, 217\n",
      "representing using one-hot-encoding, 213\n",
      "categorical variables (see categorical features)\n",
      "chaining (see algorithm chains and pipelines)\n",
      "class labels, 25\n",
      "classification problems\n",
      "\n",
      "- Answer with loss 1.7688956260681152 proba 0.27155426144599915: \n",
      "In  practice,  the  number  of\n",
      "higher n-grams that actually appear in the data is much smaller, because of the struc‐\n",
      "ture of the (English) language, though it is still large. cv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\n",
      "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
      "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names())) Vocabulary size: 39\n",
      "Vocabulary:\n",
      "['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think',\n",
      " 'doth think he', 'fool', 'fool doth', ' fool doth think', 'he', 'he is',\n",
      " 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise',\n",
      " 'knows', 'knows himself', 'knows himself to', 'man', '\n",
      "\n",
      "- Answer with loss 1.7689099311828613 proba 0.27151182293891907: \n",
      "Your Own Estimator 360 Where to Go from Here                                                                                                361\n",
      "Theory 361\n",
      "Other Machine Learning Frameworks and Packages 362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366\n",
      "\n",
      "- Answer with loss 1.7695761919021606 proba 0.27102187275886536: \n",
      "params mean_test_score\n",
      "0   0.001 0.001         {'C': 0.001, 'gamma': 0.001}       0.366 1   0.001 0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347 0.363 2 22              0.375           0.347\n",
      "\n",
      "- Answer with loss 1.7708524465560913 proba 0.2658310830593109: \n",
      "the mean is 0 and the variance is 1, bringing all features to the same magni‐\n",
      "tude. However, this scaling does not ensure any particular minimum and maximum\n",
      "values for the features. The RobustScaler works similarly to the StandardScaler in\n",
      "that it ensures statistical properties for each feature that guarantee that they are on the\n",
      "same  scale. However,  the  RobustScaler  uses  the  median and  quartiles,1  instead  of\n",
      "mean  and  variance. This  makes  the  RobustScaler  ignore  data points that  are  very\n",
      "different  from\n",
      "\n",
      "- Answer with loss 1.7726534605026245 proba 0.2672608196735382: \n",
      "feature we  have  possible  values  of  \"Government\n",
      "Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\n",
      "rated\". To encode these four possible values, we create four new features, called \"Gov\n",
      "ernment Employee\",  \"Private Employee\",  \"Self Employed\", and  \"Self Employed\n",
      "Incorporated\". A  feature  is  1  if  workclass  for  this  person has  the  corresponding\n",
      "value and 0 otherwise, so exactly one of the four new features will be 1 for each data\n",
      "point. This is why this is called one-hot or one-out-of-N encoding.\n",
      "\n",
      "- Answer with loss 1.7731192111968994 proba 0.27326110005378723: \n",
      "set\", s=60)\n",
      "axes[1].set_title(\"Scaled Data\") # rescale the test set separately # so test set min is 0 and test set max is 1\n",
      "# DO NOT DO THIS! For illustration purposes only. test_scaler = MinMaxScaler()\n",
      "test_scaler.fit(X_test) X_test_scaled_badly = test_scaler.transform(X_test) # visualize wrongly scaled data axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=mglearn.cm2(0), label=\"training set\", s=60) axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\n",
      "\n",
      "- Answer with loss 1.7732373476028442 proba 0.27161651849746704: \n",
      "categorical data, defined, 324\n",
      "defined, 211\n",
      "encoded as numbers, 218\n",
      "example of, 212\n",
      "representation in training and test sets, 217\n",
      "representing using one-hot-encoding, 213\n",
      "categorical variables (see categorical features)\n",
      "chaining (see algorithm chains and pipelines)\n",
      "class labels, 25\n",
      "classification problems binary vs. multiclass, 25\n",
      "examples of, 26\n",
      "goals for, 25\n",
      "iris classification example, 14\n",
      "k-nearest neighbors, 35\n",
      "linear models, 56\n",
      "naive Bayes classifiers, 68\n",
      "vs. regression problems, 26 agglomerative clustering, 182-187\n",
      "applications for, 131\n",
      "comparing on faces dataset, 195-207\n",
      "DBSCAN, 187-190\n",
      "evaluating with ground truth, 191-193\n",
      "evaluating without ground truth, 193-195\n",
      "goals of, 168\n",
      "k-means clustering, 168-181\n",
      "summary of, 207\n",
      "\n",
      "- Answer with loss 1.7735252380371094 proba 0.2666846215724945: \n",
      "Iterative Feature Selection In univariate testing we used no model, while in model-based selection we used a sin‐\n",
      "gle model to select features. In iterative feature selection, a series of models are built,\n",
      "with varying numbers of features. There are two basic methods: starting with no fea‐\n",
      "tures  and  adding  features  one  by  one  until  some  stopping  criterion  is  reached,  or\n",
      "starting with all features and removing features one by one until some stopping crite‐\n",
      "rion is reached. Because a series of models are built, these methods are much more\n",
      "computationally expensive than the methods we discussed previously.\n",
      "\n",
      "- Answer with loss 1.7780258655548096 proba 0.2644572854042053: \n",
      "Non-Negative Matrix Factorization (NMF)\n",
      "Non-negative  matrix factorization  is  another  unsupervised  learning  algorithm that\n",
      "aims  to  extract  useful  features. It  works  similarly  to  PCA and  can  also  be  used  for\n",
      "dimensionality  reduction. As  in  PCA,  we  are  trying  to  write  each  data  point  as  a\n",
      "weighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\n",
      "we wanted components that were orthogonal and that explained as much variance of\n",
      "the data as possible, in NMF, we want the components and the coefficients to be non-\n",
      "negative; that is, we want both the components and the coefficients to be greater than\n",
      "or equal to zero.\n",
      "\n",
      "- Answer with loss 1.7791484594345093 proba 0.27268508076667786: \n",
      "feature importance, 77\n",
      "features, defined, 4 feature_names attribute, 33\n",
      "feed-forward neural networks, 104\n",
      "fit method, 21, 68, 119, 135\n",
      "fit_transform method, 138\n",
      "floating-point numbers, 26 get_dummies function, 218\n",
      "get_support method of feature selection, 237\n",
      "gradient boosted regression trees\n",
      "for feature selection, 220-224 learning_rate parameter, 89\n",
      "parameters, 91\n",
      "vs. random forests, 88\n",
      "strengths and weaknesses, 91\n",
      "training set accuracy, 90 accessing pipeline attributes, 315\n",
      "alternate strategies for, 272\n",
      "avoiding overfitting, 261\n",
      "model selection with, 319\n",
      "nested cross-validation, 272\n",
      "parallelizing with cross-validation, 274\n",
      "pipeline preprocessing, 317\n",
      "searching non-grid spaces, 271\n",
      "simple example of, 261\n",
      "tuning parameters with, 260\n",
      "using pipelines in, 309-311\n",
      "with cross-validation, 263-275\n",
      "\n",
      "- Answer with loss 1.7834467887878418 proba 0.2681097388267517: \n",
      "Lemmatization and stemming can sometimes help in building better (or\n",
      "at least more compact) models, so we suggest you give these techniques a try when\n",
      "trying to squeeze out the last bit of performance on a particular task. Topic Modeling and Document Clustering One particular technique that is often applied to text data is topic modeling, which is\n",
      "an umbrella term describing the task of assigning each document to one or multiple\n",
      "topics,  usually  without  supervision. A  good  example  for  this  is  news  data,  which\n",
      "might be categorized into topics like “politics,” “sports,” “finance,” and so on.\n",
      "\n",
      "- Answer with loss 1.783599853515625 proba 0.2648138403892517: \n",
      "meant  merging  some  very\n",
      "far-apart points. We  see  this  again  at  the  top  of  the  chart,  where  merging  the  two\n",
      "remaining clusters into a single cluster again bridges a relatively large distance. Unfortunately,  agglomerative  clustering  still  fails  at  separating  complex  shapes  like\n",
      "the two_moons dataset. But the same is not true for the next algorithm we will look at,\n",
      "DBSCAN. DBSCAN Another  very  useful clustering  algorithm  is  DBSCAN  ( which  stands  for  “density-\n",
      "based spatial clustering of applications with noise”).\n",
      "\n",
      "- Answer with loss 1.786440372467041 proba 0.26677602529525757: \n",
      "The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of the numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\n",
      "smaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x. Finally,  the  Normalizer  does  a  very  different  kind  of  rescaling. It  scales each  data point such  that the  feature vector  has  a  Euclidean  length  of  1. In  other  words,  it\n",
      "projects a data point on the circle (or sphere, in the case of higher dimensions) with a\n",
      "radius  of  1.\n",
      "\n",
      "- Answer with loss 1.7876043319702148 proba 0.26477962732315063: \n",
      "DBSCAN  works  by  identifying  points that  are  in  “crowded”  regions  of the  feature\n",
      "space,  where  many  data  points  are  close  together. These  regions  are  referred  to  as\n",
      "dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n",
      "regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points), and they\n",
      "are defined as follows. There are two parameters in DBSCAN: min_samples and eps. If there are at least min_samples many data points within a distance of eps to a given\n",
      "data point, that data point is classified as a core sample.\n",
      "\n",
      "- Answer with loss 1.788974642753601 proba 0.27185580134391785: \n",
      "Example Application: Sentiment Analysis of Movie Reviews 325 Representing Text Data as a Bag of Words                                                                 327 Applying Bag-of-Words to a Toy Dataset 329 Bag-of-Words for Movie Reviews 330 Stopwords 334 Rescaling the Data with tf–idf                                                                                      336\n",
      "Investigating Model Coefficients                                                                                  338\n",
      "Bag-of-Words with More Than One Word (n-Grams)                                            339\n",
      "Advanced Tokenization, Stemming, and Lemmatization                                        344\n",
      "\n",
      "- Answer with loss 1.789293646812439 proba 0.26495540142059326: \n",
      "In scikit-learn, the NumPy array is the fundamental data structure. scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\n",
      "verted  to  a  NumPy  array. The  core  functionality  of  NumPy  is  the  ndarray  class,  a\n",
      "multidimensional  (n-dimensional)  array. All  elements  of the  array must  be  of  the\n",
      "same type. A NumPy array looks like this: SciPy\n",
      "SciPy  is  a  collection  of  functions  for  scientific  computing  in  Python. It  provides,\n",
      "among  other  functionality,  advanced  linear  algebra  routines,  mathematical  function\n",
      "optimization, signal processing, special mathematical functions, and statistical distri‐\n",
      "butions.\n",
      "\n",
      "- Answer with loss 1.792224645614624 proba 0.2676166296005249: \n",
      "print(\"f1_score of random forest: {:.3f}\".format(\n",
      "    f1_score(y_test, rf.predict(X_test))))\n",
      "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test)))) Comparing two precision-recall curves provides a lot of detailed insight, but is a fairly\n",
      "manual process. For automatic model comparison, we might want to summarize the\n",
      "information contained in the curve, without limiting ourselves to a particular thresh‐\n",
      "old or operating point. One particular way to summarize the precision-recall curve is\n",
      "by computing the integral or area under the curve of the precision-recall curve, also\n",
      "known as the average\n",
      "\n",
      "- Answer with loss 1.7928403615951538 proba 0.2688163220882416: \n",
      "This prediction rule is common to all linear models for classifica‐\n",
      "tion. Again, there are many different ways to find the coefficients (w) and the inter‐\n",
      "cept (b). For linear models for regression, the output, ŷ, is a linear function of the features: a\n",
      "line, plane, or hyperplane (in higher dimensions). For linear models for classification,\n",
      "the decision boundary is a linear function of the input. In other words, a (binary) lin‐ ear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\n",
      "plane.\n",
      "\n",
      "- Answer with loss 1.7933337688446045 proba 0.268814355134964: \n",
      "254 Grid Search 260 Simple Grid Search 261 The Danger of Overfitting the Parameters and the Validation Set                      261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring 275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification 276\n",
      "Metrics for Multiclass Classification 296 Regression Metrics 299 Using Evaluation Metrics in Model Selection\n",
      "\n",
      "- Answer with loss 1.7943965196609497 proba 0.2690432071685791: \n",
      "132 Preprocessing and Scaling 132 Different Kinds of Preprocessing                                                                             133 Applying Data Transformations                                                                               134 Scaling Training and Test Data the Same Way 136 The Effect of Preprocessing on Supervised Learning 138\n",
      "Dimensionality Reduction, Feature Extraction, and Manifold Learning 140\n",
      "Principal Component Analysis (PCA) 140\n",
      "Non-Negative Matrix Factorization (NMF)\n",
      "\n",
      "- Answer with loss 1.7978687286376953 proba 0.25931698083877563: \n",
      "this  is  only  the  first  step  in  evaluating  an  algorithm,\n",
      "though. The next step is usually online testing or live testing, where the consequences\n",
      "of employing the algorithm in the overall system are evaluated. Changing the recom‐\n",
      "mendations  or  search  results  users  are  shown  by  a  website  can  drastically  change\n",
      "their  behavior  and  lead  to  unexpected  consequences. To  protect  against  these  sur‐\n",
      "prises,  most  user-facing  services employ  A/B  testing,  a  form  of  blind  user  study.\n",
      "\n",
      "- Answer with loss 1.798211932182312 proba 0.26190048456192017: \n",
      "pendently of the training and test sizes, which can sometimes be helpful. It also allows\n",
      "for  using  only  part  of  the  data  in  each  iteration,  by  providing  train_size  and\n",
      "test_size settings that don’t add up to one. Subsampling the data in this way can be\n",
      "useful for experimenting with large datasets. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\n",
      "plit, which can provide more reliable results for classification tasks. Another  very  common  setting  for  cross-validation  is  when  there  are  groups  in  the\n",
      "data\n",
      "\n",
      "- Answer with loss 1.8029959201812744 proba 0.26510486006736755: \n",
      "what these  topics\n",
      "are, or how many topics there might be. Therefore, there are no known outputs. Given a set of customer records, you might want to identify which customers are\n",
      "similar, and whether there are groups of customers with similar preferences. For\n",
      "a shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\n",
      "don’t know in advance what these groups might be, or even how many there are, you have no known outputs. To identify abuse or bugs, it is often helpful to find access patterns that are differ‐\n",
      "ent  from  the  norm.\n",
      "\n",
      "- Answer with loss 1.8082737922668457 proba 0.26321011781692505: \n",
      "Considering more and more neigh‐\n",
      "bors leads to a smoother decision boundary. A smoother boundary corresponds to a\n",
      "simpler model. In other words, using few neighbors corresponds to high model com‐ plexity (as shown on the right side of Figure 2-1), and using many neighbors corre‐ sponds  to  low  model  complexity  (as  shown  on  the  left  side  of  Figure  2-1). If  you\n",
      "consider the extreme case where the number of neighbors is the number of all data\n",
      "points in the training set, each test point would have exactly the same neighbors (all\n",
      "training points) and all predictions would be the same: the class that is most frequent\n",
      "in the training set.\n",
      "\n",
      "- Answer with loss 1.8092283010482788 proba 0.26412367820739746: \n",
      "roc_curve, 293-296 SCORERS, 301\n",
      "SelectFromModel, 238 SelectPercentile, 236, 310\n",
      "ShuffleSplit, 258, 258\n",
      "silhouette_score, 193\n",
      "StandardScaler, 114, 133, 138, 144, 150, SciPy, 8\n",
      "score method, 23, 37, 43, 267, 308\n",
      "sensitivity, 283\n",
      "sentiment analysis example, 325\n",
      "shapes, defined, 16\n",
      "shuffle-split cross-validation, 258\n",
      "sin function, 232\n",
      "soft voting strategy, 84 spark computing environment, 362\n",
      "sparse coding (dictionary learning), 163\n",
      "sparse datasets, 44\n",
      "splits, 252\n",
      "Stan language, 364\n",
      "statsmodel package, 362\n",
      "stemming, 344-347\n",
      "stopwords, 334\n",
      "stratified k-fold cross-validation, 254-256\n",
      "string-encoded categorical data, 214\n",
      "supervised learning, 25-129 (see also classifica‐\n",
      "\n",
      "- Answer with loss 1.8113393783569336 proba 0.26014748215675354: \n",
      "The main benefits of DBSCAN\n",
      "are that it does not require the user to set the number of clusters a priori , it can cap‐\n",
      "ture  clusters  of  complex  shapes, and  it  can  identify  points that  are  not  part  of any\n",
      "cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\n",
      "still scales to relatively large datasets. DBSCAN  works  by  identifying  points that  are  in  “crowded”  regions  of the  feature\n",
      "space,  where  many  data  points  are  close  together. These  regions  are  referred  to  as\n",
      "dense regions in feature space.\n",
      "\n",
      "- Answer with loss 1.8115335702896118 proba 0.2674482464790344: \n",
      "data transformation application, 134\n",
      "effect on supervised learning, 138\n",
      "kinds of, 133\n",
      "parameter selection with, 306\n",
      "pipelines and, 317\n",
      "purpose of, 132\n",
      "scaling training and test data, 136\n",
      "principal component analysis (PCA) drawbacks of, 146\n",
      "example of, 140\n",
      "feature extraction with, 147\n",
      "unsupervised nature of, 145\n",
      "visualizations with, 142\n",
      "whitening option, 150\n",
      "probabilistic modeling, 363 building your own estimators, 360\n",
      "business metrics and, 358\n",
      "initial approach to, 357\n",
      "resources, 361-366\n",
      "simple vs. complicated cases, 358\n",
      "steps of, 358\n",
      "testing your system, 359\n",
      "tool choice, 359\n",
      "production systems\n",
      "testing, 359\n",
      "tool choice, 359\n",
      "\n",
      "- Answer with loss 1.8133432865142822 proba 0.2666560709476471: \n",
      "Building Your Own Estimator 360 Where to Go from Here                                                                                                361\n",
      "Theory 361\n",
      "Other Machine Learning Frameworks and Packages 362 Ranking, Recommender Systems, and Other Kinds of Learning 363\n",
      "Probabilistic Modeling, Inference, and Probabilistic Programming 363 Neural Networks 364 Scaling to Larger Datasets                                                                                          364 Honing Your Skills 365 Conclusion                                                                                                                      366\n",
      "\n",
      "- Answer with loss 1.8152467012405396 proba 0.26526403427124023: \n",
      "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
      "        # for each combination of parameters, train an SVC\n",
      "        svm = SVC(gamma= gamma, C=C) svm.fit(X_train, y_train) # evaluate the SVC on the test set\n",
      "        score = svm.score(X_valid, y_valid) # if we got a better score, store the score and parameters\n",
      "        if score > best_score: best_score = score best_parameters = {'C' : C, 'gamma' : gamma} # rebuild a model on the combined training and validation set,\n",
      "# and evaluate it on the test set\n",
      "svm = SVC(**best_parameters)\n",
      "\n",
      "- Answer with loss 1.8158411979675293 proba 0.2691282331943512: \n",
      ".3f}\".format(logreg.score(X_test, y_test))) The  default  value  of  C=1 provides  quite  good  performance,  with  95%  accuracy  on both the training and the test set. But as training and test set performance are very\n",
      "close, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\n",
      "model: logreg100 = LogisticRegression(C=100).fit(X_train, y_train) print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train))) print(\"Test set score: {: .3f}\".format(logreg100.score(X_test, y_test)))\n",
      "\n",
      "- Answer with loss 1.8186547756195068 proba 0.26837748289108276: \n",
      "marker='^', c=mglearn.cm2(1), label=\"Test set\", s=60) axes[0].legend(loc='upper left') axes[0].set_title(\"Original Data\") # scale the data using MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # visualize the properly scaled data\n",
      "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
      "                c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
      "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
      "                c=mglearn.cm2(1), label=\"Test\n",
      "\n",
      "- Answer with loss 1.8231180906295776 proba 0.26655369997024536: \n",
      "1 Why Machine Learning? 1 Problems Machine Learning Can Solve                                                                       2 Knowing Your Task and Knowing Your Data                                                             4 Why Python? 5 scikit-learn 5 Installing scikit-learn                                                                                                      6 Essential Libraries and Tools 7 Jupyter Notebook 7 NumPy 7 SciPy 8\n",
      "matplotlib 9\n",
      "pandas 10\n",
      "mglearn 11 Python 2 Versus Python 3                                                                                               12\n",
      "Versions Used in this Book\n",
      "\n",
      "- Answer with loss 1.8235079050064087 proba 0.2666512131690979: \n",
      "h[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\n",
      "h[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\n",
      "h[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3]) ŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2] Here,  w  are  the  weights  between  the  input x and  the  hidden  layer  h,  and  v  are  the\n",
      "weights between the hidden layer h and the output ŷ. The weights v and w are learned\n",
      "from data , x are the input features, ŷ is the computed output, and h are intermediate\n",
      "computations.\n",
      "\n",
      "- Answer with loss 1.8237872123718262 proba 0.26090627908706665: \n",
      "You might notice the strange-looking trailing underscore at the end\n",
      "of  coef_  and  intercept _. scikit-learn  always  stores anything that is derived from the training data in attributes that end with a\n",
      "trailing underscore. That is to separate them from parameters that\n",
      "are set by the user. The intercept_ attribute is always a single float number, while the coef_ attribute is\n",
      "a NumPy array with one entry per input feature. As we only have a single input fea‐ ture in the wave dataset, lr.coef_ only has a single entry.\n",
      "\n",
      "- Answer with loss 1.8239847421646118 proba 0.26323455572128296: \n",
      "these  topics\n",
      "are, or how many topics there might be. Therefore, there are no known outputs. Given a set of customer records, you might want to identify which customers are\n",
      "similar, and whether there are groups of customers with similar preferences. For\n",
      "a shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\n",
      "don’t know in advance what these groups might be, or even how many there are, you have no known outputs. To identify abuse or bugs, it is often helpful to find access patterns that are differ‐\n",
      "ent  from  the  norm.\n",
      "\n",
      "- Answer with loss 1.8247511386871338 proba 0.26075395941734314: \n",
      "which species each iris belongs to. Let’s assume\n",
      "that these are the only species our hobby botanist will encounter in the wild. Our goal is to build a machine learning model that can learn from the measurements\n",
      "of  these  irises  whose  species  is  known, so  that  we  can  predict  the  species  for  a  new\n",
      "iris. Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised  learning  problem. In  this  problem,  we  want  to  predict  one  of  several\n",
      "options (the species of iris).\n",
      "\n",
      "- Answer with loss 1.8266072273254395 proba 0.26512300968170166: \n",
      "print(\"AUC scoring: {}\".format(roc_auc)) # we provide a somewhat bad grid to illustrate the point: param_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n",
      "# using the default scoring of accuracy: grid = GridSearchCV(SVC(), param_grid= param_grid)\n",
      "grid.fit(X_train, y_train) print(\"Grid-Search with accuracy\") print(\"Best parameters:\", grid.best_params_) print(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_)) print(\"Test set AUC: {:.3f}\".format (\n",
      "    roc_auc_score(y_test, grid.decision_function(X_test))))\n",
      "\n",
      "- Answer with loss 1.8266181945800781 proba 0.25611504912376404: \n",
      "You might start off by asking whether the animal has feathers, a\n",
      "question that narrows down your possible animals to just two. If the answer is “yes,”\n",
      "you  can  ask  another  question  that  could  help you  distinguish  between  hawks  and\n",
      "penguins. For  example,  you  could  ask  whether  the  animal  can  fly. If  the  animal\n",
      "doesn’t have feathers, your possible animal choices are dolphins and bears, and you\n",
      "will  need  to  ask  a  question  to  distinguish  between  these  two  animals—for  example,\n",
      "asking whether the animal has fins.\n",
      "\n",
      "- Answer with loss 1.8290694952011108 proba 0.2624255418777466: \n",
      "This  book  is  for  current  and  aspiring  machine  learning  practitioners  looking  to\n",
      "implement solutions to real-world machine learning problems. This is an introduc‐\n",
      "tory book requiring no previous knowledge of machine learning or artificial intelli‐\n",
      "gence  (AI). We  focus  on  using  Python and  the  scikit-learn  library,  and  work\n",
      "through all the steps to create a successful machine learning application. The meth‐\n",
      "ods we introduce will be helpful for scientists and researchers, as well as data\n",
      "\n",
      "- Answer with loss 1.8342301845550537 proba 0.2583479583263397: \n",
      "The  animal  on  the  cover  of  Introduction  to  Machine  Learning  with  Python  is  a  hell‐\n",
      "bender salamander (Cryptobranchus alleganiensis), an amphibian native to the eastern\n",
      "United States (ranging from New York to Georgia). It has many colorful nicknames,\n",
      "including “Allegheny alligator,” “snot otter,” and “mud-devil.” The origin of the name\n",
      "“hellbender”  is  unclear: one  theory  is  that  early  settlers  found  the  salamander’s\n",
      "appearance  unsettling  and  supposed it to  be  a  demonic  creature  trying  to  return  to\n",
      "hell.\n",
      "\n",
      "- Answer with loss 1.8354947566986084 proba 0.2619669735431671: \n",
      "the number of false negatives and false positives —there are many more false positives\n",
      "than  true  positives! The  predictions  made  by  the  decision  tree  make  much  more\n",
      "sense  than  the  dummy  predictions,  even  though  the  accuracy  was  nearly  the  same. Finally, we can see that logistic regression does better than pred_tree in all aspects: it\n",
      "has more true positives and true negatives while having fewer false positives and false\n",
      "negatives. From this comparison, it is clear that only the decision tree and the logistic\n",
      "regression give reasonable results, and that the logistic regression works better than\n",
      "\n",
      "- Answer with loss 1.8357481956481934 proba 0.25586190819740295: \n",
      "There  were  three  possible\n",
      "species, setosa, versicolor, or virginica, which made the task a three-class classification\n",
      "problem. The possible species are called classes in the classification problem, and the\n",
      "species of a single iris is called its label. The  Iris  dataset  consists  of  two  NumPy  arrays:  one  containing  the  data,  which  is\n",
      "referred to as X in scikit-learn, and one containing the correct or desired outputs, which is called y. The array X is a two-dimensional array of features, with one row per\n",
      "data point and one column per feature.\n",
      "\n",
      "- Answer with loss 1.8382681608200073 proba 0.2609221041202545: \n",
      "so  that  we  can  predict  the  species  for  a  new\n",
      "iris. Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised  learning  problem. In  this  problem,  we  want  to  predict  one  of  several\n",
      "options (the species of iris). This is an example of a classification problem. The possi‐\n",
      "ble  outputs  (different  species  of  irises)  are  called  classes. Every  iris  in the  dataset\n",
      "belongs to one of three classes, so this problem is a three-class classification problem.\n",
      "\n",
      "- Answer with loss 1.8391218185424805 proba 0.26546910405158997: \n",
      "0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347 0.363 2 22              0.375           0.347 0.363 3 22              0.375           0.347 0.363 4 22              0.375           0.347           0.363 split3_test_score split4_test_score std_test_score\n",
      "0           0.363              0.380\n",
      "\n",
      "- Answer with loss 1.8406479358673096 proba 0.26555299758911133: \n",
      "We  already  saw  one  way to  summarize  the  result  in  the  confu‐\n",
      "sion matrix—by computing accuracy, which can be expressed as: In other words, accuracy is the number of correct predictions (TP and TN) divided\n",
      "by the number of all samples (all entries of the confusion matrix summed up). Precision, recall, and f-score. There are several other ways to summarize the confusion\n",
      "matrix,  with  the  most  common  ones  being  precision  and  recall. Precision  measures how many of the samples predicted as positive are actually positive:\n",
      "\n",
      "- Answer with loss 1.8415449857711792 proba 0.2629757821559906: \n",
      "def __init__(self, first_parameter=1, second_parameter=2): # All parameters must be specified in the __init__ function\n",
      "        self.first_parameter = 1\n",
      "        self.second_parameter = 2 def fit(self , X, y=None): # fit should only take X and y as parameters\n",
      "        # Even if your model is unsupervised, you need to accept a y argument! Implementing a classifier or regressor works similarly, only instead of  Transformer\n",
      "Mixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead\n",
      "of implementing transform, you would implement predict.\n",
      "\n",
      "- Answer with loss 1.8436810970306396 proba 0.25456976890563965: \n",
      "On the other hand, if your model is too simple—say, “Everybody who owns a\n",
      "house buys a boat”—then you might not be able to capture all the aspects of and vari‐\n",
      "ability in the data, and your model will do badly even on the training set. Choosing\n",
      "too simple a model is called underfitting. The more complex we allow our model to be, the better we will be able to predict on\n",
      "the training data. However, if our model becomes too complex, we start focusing too\n",
      "much on each individual data point in our training set, and the model will not gener‐\n",
      "alize well to new data.\n",
      "\n",
      "- Answer with loss 1.8450857400894165 proba 0.25940394401550293: \n",
      "learn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\n",
      "any  continuous  data,  while  BernoulliNB  assumes  binary  data  and  MultinomialNB\n",
      "assumes  count  data  (that  is,  that  each  feature  represents  an  integer  count  of some‐\n",
      "thing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\n",
      "are mostly used in text data classification. Here, we have four data points, with four binary features each. There are two classes,\n",
      "0 and 1. For class 0 (the first and third data points), the first feature is zero two times\n",
      "and nonzero zero times, the second feature is zero one time and nonzero one time,\n",
      "and  so  on.\n",
      "\n",
      "- Answer with loss 1.8482327461242676 proba 0.2597922086715698: \n",
      "cast          wonderful     soldiers      rock superman\n",
      "play truly         military      band alien\n",
      "actors superb        army soundtrack    world performances actors tarzan        singing       evil played        brilliant     soldier       voice humans supporting    recommend america       singer aliens director quite         country       sing          human\n",
      "oscar         performance   americans musical       creatures roles performances  during        roll miike\n",
      "actress perfect men           fan monsters excellent     drama\n",
      "\n",
      "- Answer with loss 1.8496410846710205 proba 0.2567748725414276: \n",
      "Then,  all  neighbors  (within  eps)  of  the  point  are  visited. If  they  have  not  been\n",
      "assigned a cluster yet, they are assigned the new cluster label that was just created. If\n",
      "they  are  core  samples,  their  neighbors are  visited  in  turn,  and  so  on. The  cluster\n",
      "grows until there are no more core samples within distance eps of the cluster. Then\n",
      "another  point that  hasn’t  yet  been  visited  is  picked,  and  the  same  procedure is\n",
      "repeated. In the end, there are three kinds of points: core points, points that are within distance\n",
      "eps of core points (called boundary points), and noise.\n",
      "\n",
      "- Answer with loss 1.851049780845642 proba 0.2594335377216339: \n",
      "Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning algo‐\n",
      "rithms to understand compared to the original representation of the data. A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a  new  way  to  represent  this  data that  summarizes  the  essential  characteristics  with\n",
      "fewer  features.\n",
      "\n",
      "- Answer with loss 1.8524751663208008 proba 0.2607323229312897: \n",
      "plt.ylim(-10, 15) plt.xlim(-10, 8)\n",
      "plt.xlabel(\"Feature 0\") plt.ylabel(\"Feature 1\") plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n",
      "            'Line class 2'], loc=(1.01, 0.3)) You can see that all the points belonging to class 0 in the training data are above the\n",
      "line corresponding to class 0, which means they are on the “class 0” side of this binary\n",
      "classifier. The  points  in  class  0  are  above  the  line  corresponding  to  class  2,  which means they  are  classified  as  “rest”  by  the  binary  classifier  for  class  2.\n",
      "\n",
      "- Answer with loss 1.8530572652816772 proba 0.2623195946216583: \n",
      "L\n",
      "L1 regularization, 53\n",
      "L2 regularization, 49, 60, 67\n",
      "Lasso model, 53 Latent Dirichlet Allocation (LDA), 348-355\n",
      "leafs, 71\n",
      "leakage, 310\n",
      "learn from the past approach, 243 learning_rate parameter, 89\n",
      "leave-one-out cross-validation, 257\n",
      "lemmatization, 344-347\n",
      "linear functions, 56\n",
      "linear models classification, 56\n",
      "data representation and, 220-224\n",
      "vs. k-nearest neighbors, 46\n",
      "Lasso, 53\n",
      "linear SVMs, 56\n",
      "logistic regression, 56\n",
      "multiclass classification, 63\n",
      "ordinary least squares, 47\n",
      "parameters, 67\n",
      "predictions with, 45\n",
      "regression, 45\n",
      "ridge regression, 49\n",
      "strengths and weaknesses, 67\n",
      "\n",
      "- Answer with loss 1.8535888195037842 proba 0.2607971727848053: \n",
      "actor         amazing       world         songs planet cast          wonderful     soldiers      rock superman\n",
      "play truly         military      band alien\n",
      "actors superb        army soundtrack    world performances actors tarzan        singing       evil played        brilliant     soldier       voice humans supporting    recommend america       singer aliens director quite         country       sing          human\n",
      "oscar         performance   americans musical       creatures roles performances  during        roll miike\n",
      "actress\n",
      "\n",
      "- Answer with loss 1.8558379411697388 proba 0.2604551613330841: \n",
      "RandomForestClassifier, 84-86, 238, 290, RandomForestRegressor, 84, 231, 240\n",
      "RFE, 240-241\n",
      "Ridge, 49, 67, 112, 231, 234, 310, 317-319\n",
      "RobustScaler, 133\n",
      "roc_auc_score, 294-301 roc_curve, 293-296 SCORERS, 301\n",
      "SelectFromModel, 238 SelectPercentile, 236, 310\n",
      "ShuffleSplit, 258, 258\n",
      "silhouette_score, 193\n",
      "StandardScaler, 114, 133, 138, 144, 150, SciPy, 8\n",
      "score method, 23, 37, 43, 267, 308\n",
      "sensitivity, 283\n",
      "sentiment analysis example, 325\n",
      "shapes, defined, 16\n",
      "shuffle-split cross-validation, 258\n",
      "sin function, 232\n",
      "soft voting strategy, 84\n",
      "\n",
      "- Answer with loss 1.8563523292541504 proba 0.25969159603118896: \n",
      "KFold, 256, 260\n",
      "KMeans, 174-181\n",
      "KNeighborsClassifier, 21-24, 37-43\n",
      "KNeighborsRegressor, 42-47\n",
      "Lasso, 53-55 LatentDirichletAllocation, 348\n",
      "LeaveOneOut, 257\n",
      "LinearRegression, 47-56, 81, 247\n",
      "LinearSVC, 56-59, 65, 67, 68\n",
      "load_boston, 34, 230, 317\n",
      "load_breast_cancer, 32, 38, 59, 75, 134, 144, make_blobs, 92, 119, 136, 173-183, 188, 286\n",
      "make_circles, 119\n",
      "make_moons, 85, 108, 175, 190-195\n",
      "make_pipeline, 313-319\n",
      "MinMaxScaler, 102, 133, 135-139, 190, 230, MLPClassifier, 107-119\n",
      "NMF, 140, 159-163, 179-182, 348\n",
      "Normalizer, 134\n",
      "OneHotEncoder, 218, 247\n",
      "ParameterGrid, 274\n",
      "PCA, 140-166, 179, 195-206, 313-314, 348\n",
      "Pipeline, 305-319, 320\n",
      "PolynomialFeatures, 227-230, 248, 317\n",
      "precision_recall_curve, 289-292\n",
      "\n",
      "- Answer with loss 1.8586152791976929 proba 0.26250016689300537: \n",
      "def nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid): outer_scores = []\n",
      "    # for each split of the data in the outer cross-validation # (split method returns indices)\n",
      "    for training_samples, test_samples in outer_cv.split(X, y): # find best parameter using inner cross-validation\n",
      "        best_parms = {}\n",
      "        best_score = -np.inf # iterate over parameters\n",
      "        for parameters in parameter_grid: # accumulate score over inner splits cv_scores = []\n",
      "            # iterate over inner cross-validation\n",
      "            for inner_train, inner_test in inner_cv.split(\n",
      "                    X[training_samples], y[training_samples])\n",
      "\n",
      "- Answer with loss 1.8590573072433472 proba 0.25848808884620667: \n",
      "NumPy\n",
      "NumPy  is  one  of  the  fundamental  packages  for  scientific  computing  in  Python. It\n",
      "contains  functionality  for  multidimensional  arrays,  high-level  mathematical  func‐\n",
      "tions such as linear algebra operations and the Fourier transform, and pseudorandom\n",
      "number generators. In scikit-learn, the NumPy array is the fundamental data structure. scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\n",
      "verted  to  a  NumPy  array. The  core  functionality  of  NumPy  is  the  ndarray  class,  a\n",
      "multidimensional  (n-dimensional)  array.\n",
      "\n",
      "- Answer with loss 1.8600828647613525 proba 0.2560504078865051: \n",
      "To  make  a  prediction  for  a\n",
      "new data point, the algorithm finds the closest data points in the training dataset—its\n",
      "“nearest neighbors.” In its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\n",
      "bor, which is the closest training data point to the point we want to make a prediction\n",
      "for. The prediction is then simply the known output for this training point. Figure 2-4\n",
      "illustrates this for the case of classification on the forge dataset: Here, we added three new data points, shown as stars.\n",
      "\n",
      "- Answer with loss 1.8646363019943237 proba 0.25476109981536865: \n",
      "Some of them, including t-SNE, com‐\n",
      "pute a new representation of the training data, but don’t allow transformations of new\n",
      "data. This means these algorithms cannot be applied to a test set: rather, they can only\n",
      "transform the data they were trained for. Manifold learning can be useful for explora‐\n",
      "tory data analysis, but is rarely used if the final goal is supervised learning. The idea\n",
      "behind t-SNE is to find a two-dimensional representation of the data that preserves\n",
      "the  distances  between  points  as  best  as  possible.\n",
      "\n",
      "- Answer with loss 1.8649091720581055 proba 0.25401726365089417: \n",
      "The other decision you have to make is whether you want to\n",
      "use L1 regularization or L2 regularization. If you assume that only a few of your fea‐\n",
      "tures are actually important, you should use L1. Otherwise, you should default to L2. L1 can also be useful if interpretability of the model is important. As L1 will use only\n",
      "a few features, it is easier to explain which features are important to the model, and\n",
      "what the effects of these features are. Linear models are very fast to train, and also fast to predict.\n",
      "\n",
      "- Answer with loss 1.8678370714187622 proba 0.2597772479057312: \n",
      "print(\"Best params:\\n{}\\n\".format(grid.best_params_)) print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_)) print(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test))) Best params:\n",
      "{'classifier': SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "     decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "     max_iter=-1, probability=False, random_state= None, shrinking= True,\n",
      "     tol=0.001 , verbose=False),\n",
      " 'preprocessing': StandardScaler(copy= True, with_mean= True, with_std\n",
      "\n",
      "- Answer with loss 1.8707555532455444 proba 0.25839751958847046: \n",
      "The most commonly used metric for imbalanced datasets in the multiclass setting is\n",
      "the multiclass version of the f-score. The idea behind the multiclass f-score is to com‐ pute one binary f-score per class, with that class being the positive class and the other\n",
      "classes making  up  the  negative  classes. Then,  these  per-class  f-scores  are  averaged\n",
      "using one of the following strategies: • \"micro\" averaging computes the total number of false positives, false negatives,\n",
      "and  true  positives  over all  classes,\n",
      "\n",
      "- Answer with loss 1.8708791732788086 proba 0.25395667552948: \n",
      "Thanks to Celia La and Brian Carlson for reading in the\n",
      "early days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\n",
      "to DTS, for your everlasting and endless support. Machine learning is about extracting knowledge from data. It is a research field at the\n",
      "intersection  of  statistics,  artificial  intelligence,  and  computer  science and  is  also\n",
      "known  as  predictive  analytics  or  statistical  learning. The  application  of  machine\n",
      "learning methods has in recent years become ubiquitous in everyday life.\n",
      "\n",
      "- Answer with loss 1.8718817234039307 proba 0.262288361787796: \n",
      "clf = Classifier(**parameters)\n",
      "                clf.fit(X[inner_train], y[inner_train]) # evaluate on inner test set\n",
      "                score = clf.score(X[inner_test], y[inner_test]) cv_scores.append(score) # compute mean score over inner folds mean_score = np.mean(cv_scores) if mean_score > best_score:\n",
      "                # if better than so far, remember parameters\n",
      "                best_score = mean_score best_params = parameters # build classifier on best parameters using outer training set clf = Classifier(**best_params)\n",
      "        clf.fit(X[training_samples], y[training_samples])\n",
      "\n",
      "- Answer with loss 1.8721811771392822 proba 0.2639106214046478: \n",
      "print(\"Test set score: {: .3f}\".format(logreg.score(X_test, y_test))) The  default  value  of  C=1 provides  quite  good  performance,  with  95%  accuracy  on both the training and the test set. But as training and test set performance are very\n",
      "close, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\n",
      "model: logreg100 = LogisticRegression(C=100).fit(X_train, y_train) print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train))) print(\"Test set score: {: .3f}\".format(logreg100.score(X_test, y_test)))\n",
      "\n",
      "- Answer with loss 1.8721835613250732 proba 0.2580704092979431: \n",
      "The  second  family  of  machine  learning  algorithms that we  will  discuss is unsuper‐\n",
      "vised learning  algorithms. Unsupervised learning  subsumes  all  kinds  of  machine\n",
      "learning where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning\n",
      "\n",
      "- Answer with loss 1.8770087957382202 proba 0.25964638590812683: \n",
      "to  make  a  linear\n",
      "model more flexible is by adding more features—for example, by adding interactions\n",
      "or polynomials of the input features. Now  let’s  expand  the  set  of  input features,  say  by  also  adding  feature1 **  2,  the\n",
      "square of the second feature, as a new feature. Instead of representing each data point\n",
      "as a two-dimensional point, (feature0, feature1), we now represent it as a three-\n",
      "dimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\n",
      "tion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\n",
      "\n",
      "- Answer with loss 1.8782689571380615 proba 0.25647836923599243: \n",
      "The  best  score  on  the  validation  set is  96%:  slightly  lower  than  before,  probably\n",
      "because we used less data to train the model (X_train is smaller now because we split\n",
      "our dataset twice). However, the score on the test set—the score that actually tells us\n",
      "how well we generalize—is even lower, at 92%. So we can only claim to classify new\n",
      "data 92% correctly, not 97% correctly as we thought before! The distinction between the training set, validation set, and test set is fundamentally\n",
      "important  to  applying  machine  learning  methods  in  practice.\n",
      "\n",
      "- Answer with loss 1.8809049129486084 proba 0.2556234300136566: \n",
      "Challenges in Unsupervised Learning A  major  challenge  in  unsupervised  learning is  evaluating  whether the  algorithm\n",
      "learned  something  useful. Unsupervised learning  algorithms are  usually  applied  to\n",
      "data that  does  not  contain  any  label  information, so we  don’t  know what the  right\n",
      "output  should  be. Therefore,  it  is  very  hard  to say  whether  a  model  “did  well.” For\n",
      "example,  our  hypothetical  clustering  algorithm  could  have  grouped  together  all  the\n",
      "pictures that show faces in profile and all the full-face pictures.\n",
      "\n",
      "- Answer with loss 1.8814024925231934 proba 0.25534331798553467: \n",
      "For eps=9 we still get many noise points, but we get\n",
      "one big cluster and some smaller clusters. Starting from eps=11, we get only one large\n",
      "cluster and noise. What is interesting to note is that there is never more than one large cluster. At most,\n",
      "there  is  one  large  cluster  containing  most  of  the  points,  and  some  smaller  clusters. This indicates that there are not two or three different kinds of face images in the data\n",
      "that are very distinct, but rather that all images are more or less equally similar to (or\n",
      "dissimilar from) the rest.\n",
      "\n",
      "- Answer with loss 1.881506085395813 proba 0.26018771529197693: \n",
      "the  feature_extraction.text\n",
      "module: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
      "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS))) print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10])) Number of stop words: 318 Every 10th stopword:\n",
      "['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n",
      " 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n",
      " 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n",
      " 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\n",
      "\n",
      "- Answer with loss 1.8828845024108887 proba 0.25503990054130554: \n",
      "9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\n",
      "linear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\n",
      "Allocation. We will learn a topic model with 10 topics, which is few enough that we can look at all\n",
      "of them. Similarly to the components in NMF, topics don’t have an inherent ordering,\n",
      "and  changing  the  number  of  topics  will  change  all  of  the  topics.10 We’ll  use  the\n",
      "\"batch\" learning method, which is somewhat slower than the default (\"online\")\n",
      "\n",
      "- Answer with loss 1.884602427482605 proba 0.25387442111968994: \n",
      "However,  the  RobustScaler  uses  the  median and  quartiles,1  instead  of\n",
      "mean  and  variance. This  makes  the  RobustScaler  ignore  data points that  are  very\n",
      "different  from the  rest  (like  measurement  errors). These odd  data points are  also\n",
      "called outliers, and can lead to trouble for other scaling techniques. The MinMaxScaler, on the other hand, shifts the data such that all features are exactly\n",
      "between 0 and 1. For the two-dimensional dataset this means all of the data is con‐ 1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\n",
      "\n",
      "- Answer with loss 1.8853031396865845 proba 0.25852546095848083: \n",
      "always\n",
      "applies exactly the same transformation to the training and the test set. This means\n",
      "the transform method always subtracts the training set minimum and divides by the\n",
      "training set range, which might be different from the minimum and range for the test\n",
      "set. Scaling Training and Test Data the Same Way It  is  important  to  apply  exactly  the  same  transformation  to  the  training  set  and  the\n",
      "test  set  for  the  supervised  model  to  work  on  the  test set. The  following  example (Figure 3-2) illustrates what would happen if we were to use the minimum and range\n",
      "of the test set instead:\n",
      "\n",
      "- Answer with loss 1.8883506059646606 proba 0.256211519241333: \n",
      "1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of the numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\n",
      "smaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x. Finally,  the  Normalizer  does  a  very  different  kind  of  rescaling. It  scales each  data point such  that the  feature vector  has  a  Euclidean  length  of  1. In  other  words,  it\n",
      "projects a data point on the circle (or sphere, in the case of higher dimensions) with a\n",
      "radius  of  1.\n",
      "\n",
      "- Answer with loss 1.8889039754867554 proba 0.25680041313171387: \n",
      "that we  will  discuss is unsuper‐\n",
      "vised learning  algorithms. Unsupervised learning  subsumes  all  kinds  of  machine\n",
      "learning where  there  is  no  known  output,  no  teacher to  instruct  the  learning  algo‐\n",
      "rithm. In unsupervised learning, the learning algorithm is just shown the input data\n",
      "and asked to extract knowledge from this data. Unsupervised transformations of a dataset are algorithms that create a new representa‐\n",
      "tion  of  the  data  which  might  be  easier  for  humans  or  other  machine  learning\n",
      "\n",
      "- Answer with loss 1.8895034790039062 proba 0.2553749680519104: \n",
      "There are other transformations that often prove useful for transforming certain\n",
      "features: in particular, applying mathematical functions like  log,  exp, or  sin. While\n",
      "tree-based  models  only  care  about  the  ordering  of  the  features,  linear  models  and neural networks are very tied to the scale and distribution of each feature, and if there\n",
      "is a nonlinear relation between the feature and the target, that becomes hard to model\n",
      "—particularly in regression. The functions log and exp can help by adjusting the rel‐\n",
      "ative scales in the data so that they can be captured better by a linear model or neural\n",
      "network.\n",
      "\n",
      "- Answer with loss 1.8904292583465576 proba 0.26055651903152466: \n",
      "# accumulate score over inner splits cv_scores = []\n",
      "            # iterate over inner cross-validation\n",
      "            for inner_train, inner_test in inner_cv.split(\n",
      "                    X[training_samples], y[training_samples]) :\n",
      "                # build classifier given parameters and training data clf = Classifier(**parameters)\n",
      "                clf.fit(X[inner_train], y[inner_train]) # evaluate on inner test set\n",
      "                score = clf.score(X[inner_test], y[inner_test]) cv_scores.append(score) # compute mean score over inner folds\n",
      "\n",
      "- Answer with loss 1.8940179347991943 proba 0.2578602731227875: \n",
      "Counting the nonzero entries per class in essence looks like this: counts = {}\n",
      "for label in np.unique(y):\n",
      "    # iterate over each class\n",
      "    # count (sum) entries of 1 per feature\n",
      "    counts[label] = X[y == label].sum(axis=0) print(\"Feature counts:\\n{}\".format(counts)) The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n",
      "ferent in what kinds of statistics they compute. MultinomialNB takes into account the\n",
      "average value of each feature for each class, while GaussianNB stores the average value\n",
      "as well as the standard deviation of each feature for each class.\n",
      "\n",
      "- Answer with loss 1.8981354236602783 proba 0.25169360637664795: \n",
      "The intuition of this method is to give high weight to any term that appears\n",
      "often in a particular document, but not in many documents in the corpus. If a word\n",
      "appears often in a particular document, but not in very many documents, it is likely\n",
      "to be very descriptive of the content of that document. scikit-learn implements the\n",
      "tf–idf  method  in  two  classes: TfidfTransformer,  which  takes  in  the  sparse  matrix\n",
      "output  produced  by  CountVectorizer and  transforms  it,  and  TfidfVectorizer,\n",
      "which  takes  in  the  text  data\n",
      "\n",
      "- Answer with loss 1.900824785232544 proba 0.2615585923194885: \n",
      "axes[0].legend(loc='upper left') axes[0].set_title(\"Original Data\") # scale the data using MinMaxScaler\n",
      "scaler = MinMaxScaler()\n",
      "scaler.fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # visualize the properly scaled data\n",
      "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
      "                c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
      "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
      "                c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
      "axes[1].set_title(\"Scaled Data\")\n",
      "\n",
      "- Answer with loss 1.9022045135498047 proba 0.2524448335170746: \n",
      "k-Nearest Neighbors The  k-NN  algorithm  is  arguably  the  simplest  machine  learning  algorithm. Building the  model  consists  only  of  storing  the  training  dataset. To  make  a  prediction  for  a\n",
      "new data point, the algorithm finds the closest data points in the training dataset—its\n",
      "“nearest neighbors.” In its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\n",
      "bor, which is the closest training data point to the point we want to make a prediction\n",
      "for. The prediction is then simply the known output for this training point.\n",
      "\n",
      "- Answer with loss 1.9063524007797241 proba 0.25752535462379456: \n",
      "Grid-Search with accuracy\n",
      "Best parameters: {'gamma': 0.0001} Best cross-validation score (accuracy)): 0.970 Test set AUC: 0.992 Test set accuracy: 0.973 # using AUC scoring instead:\n",
      "grid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\n",
      "grid.fit(X_train, y_train)\n",
      "print(\"\\nGrid-Search with AUC\") print(\"Best parameters:\", grid.best_params_) print(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_)) print(\"Test set AUC: {:.3f}\".format (\n",
      "    roc_auc_score(y_test, grid.decision_function(X_test))))\n",
      "\n",
      "- Answer with loss 1.9076582193374634 proba 0.2515769600868225: \n",
      "Another application of PCA that we mentioned earlier is feature extraction. The idea\n",
      "behind  feature  extraction  is  that it  is  possible  to  find  a  representation  of  your  data\n",
      "that is better suited to analysis than the raw representation you were given. A great\n",
      "example of an application where feature extraction is helpful is with images. Images\n",
      "are  made  up  of  pixels,  usually  stored  as  red,  green,  and  blue  (RGB)  intensities. Objects in images are usually made up of thousands of pixels, and only together are\n",
      "they meaningful.\n",
      "\n",
      "- Answer with loss 1.910061240196228 proba 0.25789719820022583: \n",
      "print(\"Best score on validation set: {:.2f}\".format(best_score)) print(\"Best parameters: \", best_parameters) print(\"Test set score with best parameters: {:.2f}\".format(test_score)) The  best  score  on  the  validation  set is  96%:  slightly  lower  than  before,  probably\n",
      "because we used less data to train the model (X_train is smaller now because we split\n",
      "our dataset twice). However, the score on the test set—the score that actually tells us\n",
      "how well we generalize—is even lower, at 92%. So we can only claim to classify new\n",
      "data 92% correctly, not 97% correctly as we thought before!\n",
      "\n",
      "- Answer with loss 1.9119633436203003 proba 0.2605381906032562: \n",
      "# print lemmas found by spacy\n",
      "    print(\"Lemmatization:\") print([token.lemma_ for token in doc_spacy]) # print tokens found by Porter stemmer\n",
      "    print(\"Stemming:\")\n",
      "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy]) Lemmatization: ['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n",
      " 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.'] Stemming: ['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n",
      " 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\n",
      "\n",
      "- Answer with loss 1.91206693649292 proba 0.2555290460586548: \n",
      "Examples of categorical features are the brand of a product, the color of a product, or\n",
      "the department (books, clothing, hardware) it is sold in. These are all properties that\n",
      "can describe a product, but they don’t vary in a continuous way. A product belongs\n",
      "either  in  the  clothing  department  or  in  the  books  department. There  is  no  middle\n",
      "ground between books and clothing, and no natural order for the different categories\n",
      "(books is not greater or less than clothing, hardware is not between books and cloth‐\n",
      "ing, etc.).\n",
      "\n",
      "- Answer with loss 1.9128886461257935 proba 0.25217404961586: \n",
      "who  are  likely  to  actually  make  a\n",
      "purchase, but not bother those customers who won’t be interested. After looking at the data for a while, our novice data scientist comes up with the fol‐\n",
      "lowing rule: “If the customer is older than 45, and has less than 3 children or is not\n",
      "divorced, then they want to buy a boat.” When asked how well this rule of his does,\n",
      "our data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is\n",
      "in  the  table,  the  rule  is  perfectly  accurate.\n",
      "\n",
      "- Answer with loss 1.9134433269500732 proba 0.25348690152168274: \n",
      "Changing the recom‐\n",
      "mendations  or  search  results  users  are  shown  by  a  website  can  drastically  change\n",
      "their  behavior  and  lead  to  unexpected  consequences. To  protect  against  these  sur‐\n",
      "prises,  most  user-facing  services employ  A/B  testing,  a  form  of  blind  user  study. In A/B testing, without their knowledge a selected portion of users will be provided with\n",
      "a  website  or  service  using  algorithm  A,  while  the  rest  of  the  users  will  be  provided\n",
      "with algorithm B.\n",
      "\n",
      "- Answer with loss 1.9147186279296875 proba 0.2545463740825653: \n",
      "Next,  we  create  a  parameter  grid. As  explained  in  Chapter  2,  the  regularization\n",
      "parameter to tune for LogisticRegression is the parameter C. We use a logarithmic\n",
      "grid  for  this  parameter,  searching  between  0.01  and  100. Because  we  used  the\n",
      "make_pipeline function, the name of the LogisticRegression step in the pipeline is\n",
      "the lowercased class name, logisticregression. To tune the parameter C, we there‐\n",
      "fore have to specify a parameter grid for logisticregression__C: X_train, X_test, y_train, y_test =\n",
      "\n",
      "- Answer with loss 1.9153172969818115 proba 0.25304633378982544: \n",
      "For these\n",
      "measurements, she can be certain of which species each iris belongs to. Let’s assume\n",
      "that these are the only species our hobby botanist will encounter in the wild. Our goal is to build a machine learning model that can learn from the measurements\n",
      "of  these  irises  whose  species  is  known, so  that  we  can  predict  the  species  for  a  new\n",
      "iris. Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised  learning  problem. In  this  problem,  we  want  to  predict  one  of  several\n",
      "options (the species of iris).\n",
      "\n",
      "- Answer with loss 1.9153656959533691 proba 0.2506234347820282: \n",
      "<= 16.795  creates  a  node that  contains  only  8 benign but  134\n",
      "malignant samples. The rest of this side of the tree then uses some finer distinctions\n",
      "to  split  off  these  8  remaining  benign  samples. Of  the  142  samples that  went  to the\n",
      "right in the initial split, nearly all of them (132) end up in the leaf to the very right. Taking a left at the root, for worst radius > 16.795 we end up with 25 malignant\n",
      "and 259 benign samples. Nearly all of the benign samples end up in the second leaf\n",
      "from the right, with most of the other leaves containing very few samples.\n",
      "\n",
      "- Answer with loss 1.9188650846481323 proba 0.2534486651420593: \n",
      "Precision  measures how many of the samples predicted as positive are actually positive: Precision  is  used  as  a  performance  metric when  the  goal  is  to  limit  the  number  of\n",
      "false  positives. As  an  example,  imagine  a  model  for  predicting  whether  a  new  drug\n",
      "will  be  effective  in  treating  a  disease  in  clinical  trials. Clinical  trials  are  notoriously\n",
      "expensive, and a pharmaceutical company will only want to run an experiment if it is\n",
      "very sure that the drug will actually work.\n",
      "\n",
      "- Answer with loss 1.9190993309020996 proba 0.2563377022743225: \n",
      "# evaluate on inner test set\n",
      "                score = clf.score(X[inner_test], y[inner_test]) cv_scores.append(score) # compute mean score over inner folds mean_score = np.mean(cv_scores) if mean_score > best_score:\n",
      "                # if better than so far, remember parameters\n",
      "                best_score = mean_score best_params = parameters # build classifier on best parameters using outer training set clf = Classifier(**best_params)\n",
      "        clf.fit(X[training_samples], y[training_samples]) # evaluate\n",
      "        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n",
      "\n",
      "- Answer with loss 1.9193830490112305 proba 0.25450247526168823: \n",
      "You  should  set  any  parameters  of  the  model  when  constructing  the  model  object. These  parameters  include  regularization,  complexity  control,  number  of  clusters  to\n",
      "find, etc. All estimators have a fit method, which is used to build the model. The fit\n",
      "method always requires as its first argument the data X, represented as a NumPy array\n",
      "or a SciPy sparse matrix, where each row represents a single data point. The data X is\n",
      "always  assumed  to be  a  NumPy  array  or  SciPy  sparse  matrix\n",
      "\n",
      "- Answer with loss 1.9202886819839478 proba 0.25076940655708313: \n",
      "The type of the entries of text_train will depend on your Python version. In Python\n",
      "3, they will be of type bytes which represents a binary encoding of the string data. In\n",
      "Python 2,  text_train contains strings. We won’t go into the details of the different\n",
      "string  types  in  Python  here, but  we  recommend  that you  read  the  Python 2 and/or Python 3 documentation regarding strings and Unicode. reviews_test = load_files(\"data/aclImdb/test/\") text_test, y_test = reviews_test.data, reviews_test.target\n",
      "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
      "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
      "\n",
      "- Answer with loss 1.9253495931625366 proba 0.2520715594291687: \n",
      "This is an example of a classification problem. The possi‐\n",
      "ble  outputs  (different  species  of  irises)  are  called  classes. Every  iris  in the  dataset\n",
      "belongs to one of three classes, so this problem is a three-class classification problem. The desired output for a single data point (an iris) is the species of this flower. For a\n",
      "particular data point, the species it belongs to is called its label. Meet the Data The data we will use for this example is the Iris dataset, a classical dataset in machine\n",
      "learning  and  statistics.\n",
      "\n",
      "- Answer with loss 1.9267438650131226 proba 0.2494628131389618: \n",
      "We can change the range\n",
      "of tokens that are considered as features by changing the ngram_range parameter of\n",
      "CountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐ sisting of the minimum length and the maximum length of the sequences of tokens\n",
      "that are considered. Here is an example on the toy data we used earlier: The default is to create one feature per sequence of tokens that is at least one token\n",
      "long  and  at  most  one  token  long,  or  in  other  words exactly  one  token  long  (\n",
      "\n",
      "- Answer with loss 1.9308228492736816 proba 0.25179699063301086: \n",
      "gets          director unfunny       evil saw barbra thing times         shot          long cast believe laughed       low interesting\n",
      "short         am            comedies      fulci few\n",
      "serial actually      isn re            half The topics we extracted this time seem to be more specific, though many are hard to\n",
      "interpret. Topic  7  seems  to  be  about  horror  movies  and  thrillers;  topics  16 and  54\n",
      "seem  to  capture  bad  reviews,  while  topic  63  mostly seems  to  be  capturing  positive\n",
      "reviews of comedies.\n",
      "\n",
      "- Answer with loss 1.9310908317565918 proba 0.24817855656147003: \n",
      "classified  as  class  0  on  the  bottom. In\n",
      "other words, any new data point that lies above the black line will be classified as class\n",
      "1 by the respective classifier, while any point that lies below the black line will be clas‐\n",
      "sified as class 0. The two models come up with similar decision boundaries. Note that both misclas‐\n",
      "sify two of the points. By default, both models apply an L2 regularization, in the same\n",
      "way that Ridge does for regression. For LogisticRegression and LinearSVC the trade-off parameter that determines the\n",
      "strength  of  the  regularization  is  called  C,  and  higher  values  of  C  correspond  to  less\n",
      "\n",
      "- Answer with loss 1.9328935146331787 proba 0.25484001636505127: \n",
      "but\n",
      "pipe_short  has  steps that  were  automatically  named. We  can  see  the  names  of  the\n",
      "steps by looking at the steps attribute: Pipeline steps:\n",
      "[('minmaxscaler', MinMaxScaler(copy= True, feature_range=(0, 1))),\n",
      " ('svc', SVC(C=100, cache_size=200, class_weight= None, coef0=0.0,\n",
      " \n",
      "     decision_function_shape =None, degree=3, gamma='auto',\n",
      "             kernel='rbf', max_iter=-1, probability=False,\n",
      "             random_state= None, shrinking= True, tol=0.001,\n",
      "             verbose=False))] The steps are named minmaxscaler and svc.\n",
      "\n",
      "- Answer with loss 1.9336198568344116 proba 0.2503470778465271: \n",
      "Representing Text Data as a Bag of Words\n",
      "One of the most simple but effective and commonly used ways to represent text for\n",
      "machine learning is using the bag-of-words representation. When using this represen‐\n",
      "tation,  we  discard  most  of  the  structure  of  the  input  text,  like  chapters,  paragraphs,\n",
      "sentences, and formatting, and only count how often each word appears in each text in There are some subtleties involved in step 1 and step 2, which we will discuss in more\n",
      "detail later in this chapter.\n",
      "\n",
      "- Answer with loss 1.934473991394043 proba 0.2495480626821518: \n",
      "forms is  used  (an\n",
      "explicit and human-verified system), and the role of the word in the sentence is taken\n",
      "into account, the process is referred to as lemmatization and the standardized form of the  word  is  referred  to  as the  lemma. Both  processing  methods,  lemmatization  and\n",
      "stemming,  are  forms  of  normalization that  try  to  extract  some  normal  form  of  a\n",
      "word. Another interesting case of normalization is spelling correction, which can be\n",
      "helpful in practice but is outside of the scope of this book.\n",
      "\n",
      "- Answer with loss 1.9345629215240479 proba 0.2478119432926178: \n",
      "We build a\n",
      "machine learning model from these input/output pairs, which comprise our training\n",
      "set. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\n",
      "vised learning  often  requires  human  effort to  build  the  training  set, but  afterward\n",
      "automates and often speeds up an otherwise laborious or infeasible task. In classification, the goal is to predict a class label, which is a choice from a predefined\n",
      "list of possibilities. In Chapter 1 we used the example of classifying irises into one of\n",
      "three possible species.\n",
      "\n",
      "- Answer with loss 1.9364018440246582 proba 0.25527113676071167: \n",
      "Relation to accuracy. We  already  saw  one  way to  summarize  the  result  in  the  confu‐\n",
      "sion matrix—by computing accuracy, which can be expressed as: In other words, accuracy is the number of correct predictions (TP and TN) divided\n",
      "by the number of all samples (all entries of the confusion matrix summed up). Precision, recall, and f-score. There are several other ways to summarize the confusion\n",
      "matrix,  with  the  most  common  ones  being  precision  and  recall. Precision  measures how many of the samples predicted as positive are actually positive:\n",
      "\n",
      "- Answer with loss 1.9370015859603882 proba 0.25282183289527893: \n",
      "kernel trick, 97\n",
      "linear models and nonlinear features, 92\n",
      "vs. linear support vector machines, 92\n",
      "mathematics of, 92\n",
      "parameters, 104\n",
      "predictions with, 98\n",
      "preprocessing data for, 102\n",
      "strengths and weaknesses, 104\n",
      "tuning SVM parameters, 99\n",
      "understanding, 98 L\n",
      "L1 regularization, 53\n",
      "L2 regularization, 49, 60, 67\n",
      "Lasso model, 53 Latent Dirichlet Allocation (LDA), 348-355\n",
      "leafs, 71\n",
      "leakage, 310\n",
      "learn from the past approach, 243 learning_rate parameter, 89\n",
      "leave-one-out cross-validation, 257\n",
      "lemmatization, 344-347\n",
      "linear functions, 56\n",
      "linear models\n",
      "\n",
      "- Answer with loss 1.9381405115127563 proba 0.25263580679893494: \n",
      "You  do  not\n",
      "need to contact us for permission unless you’re reproducing a significant portion of\n",
      "the code. For example, writing a program that uses several chunks of code from this\n",
      "book  does  not  require  permission. Selling  or  distributing  a  CD-ROM  of  examples\n",
      "from  O’Reilly  books does  require  permission. Answering a  question  by  citing  this\n",
      "book and quoting example code does not require permission. Incorporating a signifi‐ ca nt amount of example code from this book into your product’s documentation\n",
      "\n",
      "- Answer with loss 1.939699411392212 proba 0.2495463341474533: \n",
      "A less complex model means worse performance on the training\n",
      "set,  but  better  generalization. As  we  are  only  interested  in  generalization perfor‐\n",
      "mance, we should choose the Ridge model over the LinearRegression model. The  Ridge  model  makes  a  trade-off  between  the  simplicity  of  the  model  (near-zero\n",
      "coefficients) and  its  performance  on  the  training  set. How  much  importance the\n",
      "model  places  on  simplicity  versus  training set  performance  can  be  specified  by  the\n",
      "user, using the alpha parameter.\n",
      "\n",
      "- Answer with loss 1.9399299621582031 proba 0.2506394684314728: \n",
      "that  are  very\n",
      "different  from the  rest  (like  measurement  errors). These odd  data points are  also\n",
      "called outliers, and can lead to trouble for other scaling techniques. The MinMaxScaler, on the other hand, shifts the data such that all features are exactly\n",
      "between 0 and 1. For the two-dimensional dataset this means all of the data is con‐ 1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of the numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\n",
      "smaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\n",
      "\n",
      "- Answer with loss 1.940069556236267 proba 0.2553650438785553: \n",
      "print(\"argmax combined with classes _: {}\".format (\n",
      "        logreg.classes_[argmax_dec_func][:10])) unique classes in training data: ['setosa' 'versicolor' 'virginica']\n",
      "predictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n",
      " 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor'] argmax of decision function: [1 0 2 1 1 0 1 2 1 1]\n",
      "argmax combined with classes _: ['versicolor' 'setosa' 'virginica' 'versicolor'\n",
      " 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\n",
      "\n",
      "- Answer with loss 1.9418678283691406 proba 0.24888962507247925: \n",
      "Therefore, there are no known outputs. Given a set of customer records, you might want to identify which customers are\n",
      "similar, and whether there are groups of customers with similar preferences. For\n",
      "a shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\n",
      "don’t know in advance what these groups might be, or even how many there are, you have no known outputs. To identify abuse or bugs, it is often helpful to find access patterns that are differ‐\n",
      "ent  from  the  norm. Each  abnormal  pattern  might  be  very  different,\n",
      "\n",
      "- Answer with loss 1.9481964111328125 proba 0.25634321570396423: \n",
      "['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n",
      " 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.'] Stemming: ['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n",
      " 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.'] Stemming  is  always  restricted  to  trimming  the  word  to  a  stem, so  \"was\"  becomes\n",
      "\"wa\",  while  lemmatization  can  retrieve  the  correct  base  verb  form,  \"be\". Similarly,\n",
      "lemmatization  can  normalize  \"worse\"  to  \"bad\",  while  stemming  produces  \"wors\".\n",
      "\n",
      "- Answer with loss 1.948622226715088 proba 0.2509717643260956: \n",
      "As we just discussed, changing the threshold that is used to make a classification deci‐\n",
      "sion in a model is a way to adjust the trade-off of precision and recall for a given clas‐\n",
      "sifier. Maybe you want to miss less than 10% of positive samples, meaning a desired\n",
      "recall  of  90%. This  decision  depends  on  the  application,  and  it  should  be  driven  by\n",
      "business goals. Once a particular goal is set—say, a particular recall or precision value\n",
      "for a class—a threshold can be set appropriately. It is always possible to set a thresh‐\n",
      "old to fulfill a particular target, like 90% recall.\n",
      "\n",
      "- Answer with loss 1.951250433921814 proba 0.2521292269229889: \n",
      "We  fix  the  random_state  in  the  tree,  which is  used  for  tie-\n",
      "breaking internally: cancer = load_breast_cancer() X_train, X_test, y_train, y_test = train_test_split (\n",
      "    cancer.data, cancer.target, stratify= cancer.target, random_state=42) tree = DecisionTreeClassifier(random_state=0)\n",
      "tree.fit(X_train, y_train) print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
      "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test))) As  expected,  the  accuracy  on  the  training  set  is  100%—because\n",
      "\n",
      "- Answer with loss 1.951568841934204 proba 0.25042444467544556: \n",
      "—there are many more false positives\n",
      "than  true  positives! The  predictions  made  by  the  decision  tree  make  much  more\n",
      "sense  than  the  dummy  predictions,  even  though  the  accuracy  was  nearly  the  same. Finally, we can see that logistic regression does better than pred_tree in all aspects: it\n",
      "has more true positives and true negatives while having fewer false positives and false\n",
      "negatives. From this comparison, it is clear that only the decision tree and the logistic\n",
      "regression give reasonable results, and that the logistic regression works better than\n",
      "\n",
      "- Answer with loss 1.9517526626586914 proba 0.24738168716430664: \n",
      "Let’s assume\n",
      "that these are the only species our hobby botanist will encounter in the wild. Our goal is to build a machine learning model that can learn from the measurements\n",
      "of  these  irises  whose  species  is  known, so  that  we  can  predict  the  species  for  a  new\n",
      "iris. Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised  learning  problem. In  this  problem,  we  want  to  predict  one  of  several\n",
      "options (the species of iris). This is an example of a classification problem.\n",
      "\n",
      "- Answer with loss 1.952532172203064 proba 0.2480446994304657: \n",
      "To  make  a  prediction  for  a  new  data  point,  the  algorithm\n",
      "finds the point in the training set that is closest to the new point. Then it assigns the\n",
      "label of this training point to the new data point. The k in k-nearest neighbors signifies that instead of using only the closest neighbor\n",
      "to the new data point, we can consider any fixed number k of neighbors in the train‐\n",
      "ing (for example, the closest three or five neighbors). Then, we can make a prediction\n",
      "using  the  majority  class  among  these  neighbors.\n",
      "\n",
      "- Answer with loss 1.9525550603866577 proba 0.24604403972625732: \n",
      "Each of these in turn has two sub‐\n",
      "folders, one called pos and one called neg: 2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\n",
      "alphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\n",
      "delimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult. The pos folder contains all the positive reviews, each as a separate text file, and simi‐\n",
      "larly for the neg folder.\n",
      "\n",
      "- Answer with loss 1.9529471397399902 proba 0.24330975115299225: \n",
      "However, often we don’t know which fea‐ tures  to  add,  and  adding  many  features  (like  all  possible  interactions  in  a  100-\n",
      "dimensional feature space) might make computation very expensive. Luckily, there is\n",
      "a clever mathematical trick that allows us to learn a classifier in a higher-dimensional\n",
      "space without actually computing the new, possibly very large representation. This is\n",
      "known as the kernel trick, and it works by directly computing the distance (more pre‐ cisely, the scalar products) of the data points for the expanded feature representation,\n",
      "without ever actually computing the expansion.\n",
      "\n",
      "- Answer with loss 1.953884482383728 proba 0.2504751980304718: \n",
      "beforehand what these  topics\n",
      "are, or how many topics there might be. Therefore, there are no known outputs. Given a set of customer records, you might want to identify which customers are\n",
      "similar, and whether there are groups of customers with similar preferences. For\n",
      "a shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\n",
      "don’t know in advance what these groups might be, or even how many there are, you have no known outputs. To identify abuse or bugs, it is often helpful to find access patterns that are differ‐\n",
      "ent  from  the  norm.\n",
      "\n",
      "- Answer with loss 1.9545172452926636 proba 0.2523757517337799: \n",
      "# print dataset properties before and after scaling\n",
      "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
      "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\n",
      "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
      "print(\"per-feature minimum after scaling:\\n {}\".format (\n",
      "    X_train_scaled.min(axis=0)) ) print(\"per-feature maximum after scaling:\\n {}\".format (\n",
      "    X_train_scaled.max(axis=0))) transformed shape: (426, 30)\n",
      "per-feature minimum before scaling:\n",
      " [   6.98    9.71   43.79\n",
      "\n",
      "- Answer with loss 1.95772385597229 proba 0.2538149952888489: \n",
      "Building Pipelines 308 Using Pipelines in Grid Searches 309 The General Pipeline Interface 312\n",
      "Convenient Pipeline Creation with make_pipeline 313 Accessing Step Attributes 314 Accessing Attributes in a Grid-Searched Pipeline                                                 315 Grid-Searching Preprocessing Steps and Model Parameters 317 Grid-Searching Which Model To Use                                                                         319\n",
      "Summary and Outlook                                                                                                  320\n",
      "\n",
      "- Answer with loss 1.9583086967468262 proba 0.2504366636276245: \n",
      "Manifold  learning  algorithms  are  mainly  aimed  at  visualization,  and  so  are  rarely\n",
      "used to generate more than two new features. Some of them, including t-SNE, com‐\n",
      "pute a new representation of the training data, but don’t allow transformations of new\n",
      "data. This means these algorithms cannot be applied to a test set: rather, they can only\n",
      "transform the data they were trained for. Manifold learning can be useful for explora‐\n",
      "tory data analysis, but is rarely used if the final goal is supervised learning.\n",
      "\n",
      "- Answer with loss 1.961745262145996 proba 0.24931801855564117: \n",
      "This\n",
      "constraint is an example of what is called regularization. Regularization means explic‐\n",
      "itly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\n",
      "sion is known as L2 regularization.7 ridge = Ridge().fit(X_train, y_train) print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train))) print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test))) As you can see, the training set score of Ridge is lower than for LinearRegression,\n",
      "while the test set score is higher.\n",
      "\n",
      "- Answer with loss 1.96242094039917 proba 0.25017309188842773: \n",
      "0.01         {'C': 0.001, 'gamma': 0.01}        0.366 2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366 3 0.001 1         {'C': 0.001, 'gamma': 1}           0.366\n",
      "4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366 rank_test_score split0_test_score split1_test_score split2_test_score\n",
      "0 22              0.375           0.347 0.363 1 22              0.375           0.347 0.363 2 22              0.375           0.347 0.363 3 22              0.375           0.347 0.363 4 22              0.375           0.347           0.363\n",
      "\n",
      "- Answer with loss 1.963194489479065 proba 0.24975579977035522: \n",
      "Here,  we  used  stratified  five-fold  cross-validation  in  both  the  inner  and  the  outer\n",
      "loop. As  our  param_grid  contains  36  combinations  of  parameters,  this  results  in  a\n",
      "whopping 36 * 5 * 5 = 900 models being built, making nested cross-validation a very\n",
      "expensive  procedure. Here,  we  used  the  same  cross-validation  splitter  in  the  inner\n",
      "and the outer loop; however, this is not necessary and you can use any combination\n",
      "of  cross-validation  strategies  in  the  inner  and  outer  loops.\n",
      "\n",
      "- Answer with loss 1.9660416841506958 proba 0.2516874670982361: \n",
      "Examples of continuous features\n",
      "that we  have  seen are  pixel  brightnesses  and  size  measurements  of  plant  flowers. Examples of categorical features are the brand of a product, the color of a product, or\n",
      "the department (books, clothing, hardware) it is sold in. These are all properties that\n",
      "can describe a product, but they don’t vary in a continuous way. A product belongs\n",
      "either  in  the  clothing  department  or  in  the  books  department. There  is  no  middle\n",
      "ground between books and clothing, and no natural order for the different categories\n",
      "(books is not greater or less than clothing, hardware is not between books and cloth‐\n",
      "ing, etc.).\n",
      "\n",
      "- Answer with loss 1.9664802551269531 proba 0.24483782052993774: \n",
      "Starting from eps=11, we get only one large\n",
      "cluster and noise. What is interesting to note is that there is never more than one large cluster. At most,\n",
      "there  is  one  large  cluster  containing  most  of  the  points,  and  some  smaller  clusters. This indicates that there are not two or three different kinds of face images in the data\n",
      "that are very distinct, but rather that all images are more or less equally similar to (or\n",
      "dissimilar from) the rest. The results for eps=7 look most interesting, with many small clusters.\n",
      "\n",
      "- Answer with loss 1.966552972793579 proba 0.2514924108982086: \n",
      "Cross-Validation 252\n",
      "Cross-Validation in scikit-learn                                                                                253\n",
      "Benefits of Cross-Validation 254 Stratified k-Fold Cross-Validation and Other Strategies 254 Grid Search 260 Simple Grid Search 261 The Danger of Overfitting the Parameters and the Validation Set                      261\n",
      "Grid Search with Cross-Validation                                                                          263\n",
      "Evaluation Metrics and Scoring 275 Keep the End Goal in Mind                                                                                      275\n",
      "Metrics for Binary Classification\n",
      "\n",
      "- Answer with loss 1.9720896482467651 proba 0.24622859060764313: \n",
      "= tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3]) ŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2] Here,  w  are  the  weights  between  the  input x and  the  hidden  layer  h,  and  v  are  the\n",
      "weights between the hidden layer h and the output ŷ. The weights v and w are learned\n",
      "from data , x are the input features, ŷ is the computed output, and h are intermediate\n",
      "computations. An important parameter that needs to be set by the user is the number\n",
      "of nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\n",
      "sets and as big as 10,000 for very complex data.\n",
      "\n",
      "- Answer with loss 1.9726953506469727 proba 0.25235214829444885: \n",
      "relates  to  the  decomposition  methods  from  Chapter  3. Each  of  the  components we\n",
      "learn then  corresponds  to  one  topic,  and  the  coefficients  of  the  components  in the\n",
      "representation of a document tell us how strongly related that document is to a par‐\n",
      "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\n",
      "lar decomposition method called Latent Dirichlet Allocation (often LDA for short).9 Latent Dirichlet Allocation Intuitively,  the  LDA  model  tries  to  find  groups  of  words\n",
      "\n",
      "- Answer with loss 1.9775981903076172 proba 0.24716004729270935: \n",
      "We apply the load_files function first to the training data: reviews_train = load_files(\"data/ aclImdb/train/\") # load_files returns a bunch, containing training texts and training labels text_train , y_train = reviews_train.data, reviews_train.target\n",
      "print(\"type of text_train: {}\".format(type(text_train))) print(\"length of text_train: {}\".format(len(text_train)))\n",
      "print(\"text_train[1]:\\n{}\".format(text_train[1])) type of text_train:  <class 'list'>\n",
      "length of text_train:  25000 text_train[1]: b'Words can\\'t describe how bad this movie is.\n",
      "\n",
      "- Answer with loss 1.9788670539855957 proba 0.2444784790277481: \n",
      "SciPy\n",
      "SciPy  is  a  collection  of  functions  for  scientific  computing  in  Python. It  provides,\n",
      "among  other  functionality,  advanced  linear  algebra  routines,  mathematical  function\n",
      "optimization, signal processing, special mathematical functions, and statistical distri‐\n",
      "butions. scikit-learn  draws  from  SciPy’s  collection  of  functions  for  implementing\n",
      "its algorithms. The most important part of SciPy for us is scipy.sparse : this provides\n",
      "sparse  matrices,  which  are  another  representation  that  is  used  for  data  in\n",
      "\n",
      "- Answer with loss 1.9824738502502441 proba 0.2480389028787613: \n",
      "is\n",
      "commonly used for visualization using two-dimensional scatter plots. Principal Component Analysis (PCA)\n",
      "Principal component analysis is a method that rotates the dataset in a way such that the  rotated  features  are  statistically  uncorrelated. This  rotation is  often  followed  by\n",
      "selecting only a subset of the new features, according to how important they are for\n",
      "explaining the data. The following example (Figure 3-3) illustrates the effect of PCA\n",
      "on a synthetic two-dimensional dataset: The first plot (top left) shows the original data points, colored to distinguish among\n",
      "them.\n",
      "\n",
      "- Answer with loss 1.9856263399124146 proba 0.2449849545955658: \n",
      "pandas\n",
      "pandas is a Python library for data wrangling and analysis. It is built around a data\n",
      "structure called the DataFrame that is modeled after the R DataFrame. Simply put, a\n",
      "pandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\n",
      "range of methods to modify and operate on this table; in particular, it allows SQL-like\n",
      "queries and joins of tables. In contrast to NumPy, which requires that all entries in an\n",
      "array  be  of  the  same  type,  pandas  allows  each  column  to  have  a  separate  type  (for\n",
      "example, integers, dates, floating-point numbers, and strings).\n",
      "\n",
      "- Answer with loss 1.9856781959533691 proba 0.2440074384212494: \n",
      "Ensembles of Decision Trees Ensembles  are  methods that  combine  multiple  machine  learning  models  to  create\n",
      "more  powerful  models. There  are  many  models  in  the  machine  learning  literature that belong to this category, but there are two ensemble models that have proven to\n",
      "be  effective  on  a  wide  range  of  datasets  for  classification  and  regression,  both  of\n",
      "which use decision trees as their building blocks: random forests and gradient boos‐\n",
      "ted decision trees. As we just observed, a main drawback of decision trees is that they tend to overfit the\n",
      "training data.\n",
      "\n",
      "- Answer with loss 1.987391710281372 proba 0.24230873584747314: \n",
      "Our goal is to build a machine learning model that can learn from the measurements\n",
      "of  these  irises  whose  species  is  known, so  that  we  can  predict  the  species  for  a  new\n",
      "iris. Because we have measurements for which we know the correct species of iris, this is a\n",
      "supervised  learning  problem. In  this  problem,  we  want  to  predict  one  of  several\n",
      "options (the species of iris). This is an example of a classification problem. The possi‐\n",
      "ble  outputs  (different  species  of  irises)  are  called  classes.\n",
      "\n",
      "- Answer with loss 1.9893651008605957 proba 0.24461427330970764: \n",
      "The most important values for the scoring parameter for classification are accuracy\n",
      "(the default); roc_auc for the area under the ROC curve; average_precision for the\n",
      "area under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for\n",
      "the binary f1-score and the different weighted variants. For regression, the most com‐\n",
      "monly  used  values  are  r2  for  the  R2  score,  mean_squared_error  for mean  squared\n",
      "error, and  mean_absolute_error for mean absolute error. You can find a full list of\n",
      "supported  arguments  in  the  documentation  or  by  looking  at\n",
      "\n",
      "- Answer with loss 1.9926283359527588 proba 0.24160967767238617: \n",
      "A common rule of thumb is to build “as many as you have time/memory for.” As  described  earlier,  max_features  determines  how  random each  tree  is,  and  a\n",
      "smaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\n",
      "the  default  values:  max_features=sqrt(n_features)  for  classification and  max_fea\n",
      "tures=log2(n_features)  for  regression. Adding  max_features  or  max_leaf_nodes\n",
      "might sometimes improve performance. It can also drastically reduce space and time\n",
      "requirements for training and prediction.\n",
      "\n",
      "- Answer with loss 1.9927324056625366 proba 0.24871957302093506: \n",
      "Let’s  go  back  to  the\n",
      "two_moons data. Using PCA or NMF, there is nothing much we can do to this data, as\n",
      "it  lives  in  only  two  dimensions. Reducing  it  to  one  dimension  with  PCA  or  NMF\n",
      "would completely destroy the structure of the data. But we can find a more expressive\n",
      "representation with k-means, by using more cluster centers (see Figure 3-32): plt.scatter(X[:, 0], X[:, 1], c =y_pred, s=60, cmap='Paired') plt.scatter(kmeans.cluster_centers _ [:, 0], kmeans.cluster_centers_ [:, 1], s=60,\n",
      "            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired')\n",
      "plt.xlabel(\"Feature 0\")\n",
      "plt.ylabel(\"Feature 1\")\n",
      "print(\"Cluster memberships:\\n{}\".format(y_pred))\n",
      "\n",
      "- Answer with loss 1.993348240852356 proba 0.23742175102233887: \n",
      "Overfitting\n",
      "occurs when you fit a model too closely to the particularities of the training set and\n",
      "obtain a model that works well on the training set but is not able to generalize to new\n",
      "data. On the other hand, if your model is too simple—say, “Everybody who owns a\n",
      "house buys a boat”—then you might not be able to capture all the aspects of and vari‐\n",
      "ability in the data, and your model will do badly even on the training set. Choosing\n",
      "too simple a model is called underfitting. The more complex we allow our model to be, the better we will be able to predict on\n",
      "the training data.\n",
      "\n",
      "- Answer with loss 1.9954197406768799 proba 0.24419738352298737: \n",
      "print(\"length of text_train: {}\".format(len(text_train)))\n",
      "print(\"text_train[1]:\\n{}\".format(text_train[1])) type of text_train:  <class 'list'>\n",
      "length of text_train:  25000 text_train[1]: b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing\n",
      "  only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many\n",
      "  clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here\n",
      "  that will just make you cry.\n",
      "\n",
      "- Answer with loss 1.9978110790252686 proba 0.24444393813610077: \n",
      "algo‐\n",
      "rithms to understand compared to the original representation of the data. A common\n",
      "application of unsupervised transformations is dimensionality reduction, which takes\n",
      "a high-dimensional representation of the data, consisting of many features, and finds\n",
      "a  new  way  to  represent  this  data that  summarizes  the  essential  characteristics  with\n",
      "fewer  features. A  common  application  for  dimensionality reduction  is  reduction  to\n",
      "two dimensions for visualization purposes. Another application for unsupervised transformations is finding the parts or compo‐\n",
      "nents that “make up” the data.\n",
      "\n",
      "- Answer with loss 1.9992573261260986 proba 0.24721939861774445: \n",
      "5\n",
      "building your own systems, vii\n",
      "data representation, 211-250\n",
      "examples of, 1, 13-23\n",
      "mathematics of, vii\n",
      "model evaluation and improvement, preprocessing and scaling, 132-140\n",
      "prerequisites to learning, vii\n",
      "resources, ix, 361-366\n",
      "scikit-learn and, 5-13\n",
      "supervised learning, 25-129\n",
      "understanding your data, 4\n",
      "unsupervised learning, 131-209\n",
      "working with text data, 323-356 tions, 232\n",
      "matplotlib, 9\n",
      "max_features parameter, 84\n",
      "meta-estimators for trees and forests, 266\n",
      "method chaining, 68\n",
      "metrics (see evaluation metrics and scoring)\n",
      "mglearn, 11\n",
      "mllib, 362\n",
      "model-based feature selection, 238\n",
      "models (see also algorithms)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_ = sorted(res, key=lambda u: u[2])\n",
    "for i, j, l, p in res_:\n",
    "    print('- Answer with loss {} proba {}: \\n{}\\n'.format(l, p, ' '.join(sens[i:j])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pages instead Of paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pdfs/sample.pdf\", \"rb\") as f:\n",
    "    pdf = pdftotext.PDF(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on question: how to store sparse matrices? ...\n",
      "processing phrase 0 - loss 5.59 - proba 0.06\n",
      "processing phrase 1 - loss 4.47 - proba 0.10\n",
      "processing phrase 2 - loss 5.61 - proba 0.06\n",
      "processing phrase 3 - loss 10.00 - proba 0.00\n",
      "processing phrase 4 - loss 10.00 - proba 0.00\n",
      "processing phrase 5 - loss 10.00 - proba 0.00\n",
      "processing phrase 6 - loss 10.00 - proba 0.00\n",
      "processing phrase 7 - loss 10.00 - proba 0.00\n",
      "processing phrase 8 - loss 10.00 - proba 0.00\n",
      "processing phrase 9 - loss 10.00 - proba 0.00\n",
      "processing phrase 10 - loss 3.32 - proba 0.15\n",
      "processing phrase 11 - loss 10.00 - proba 0.00\n",
      "processing phrase 12 - loss 10.00 - proba 0.00\n",
      "processing phrase 13 - loss 2.46 - proba 0.20\n",
      "processing phrase 14 - loss 10.00 - proba 0.00\n",
      "processing phrase 15 - loss 10.00 - proba 0.00\n",
      "processing phrase 16 - loss 10.00 - proba 0.00\n",
      "processing phrase 17 - loss 10.00 - proba 0.00\n",
      "processing phrase 18 - loss 10.00 - proba 0.00\n",
      "processing phrase 19 - loss 10.00 - proba 0.00\n",
      "processing phrase 20 - loss 10.00 - proba 0.00\n",
      "processing phrase 21 - loss 10.00 - proba 0.00\n",
      "processing phrase 22 - loss 10.00 - proba 0.00\n",
      "processing phrase 23 - loss 3.53 - proba 0.14\n",
      "processing phrase 24 - loss 10.00 - proba 0.00\n",
      "processing phrase 25 - loss 10.00 - proba 0.00\n",
      "processing phrase 26 - loss 10.00 - proba 0.00\n",
      "processing phrase 27 - loss 3.51 - proba 0.14\n",
      "processing phrase 28 - loss 10.00 - proba 0.00\n",
      "processing phrase 29 - loss 10.00 - proba 0.00\n",
      "processing phrase 30 - loss 10.00 - proba 0.00\n",
      "processing phrase 31 - loss 10.00 - proba 0.00\n",
      "processing phrase 32 - loss 10.00 - proba 0.00\n",
      "processing phrase 33 - loss 5.11 - proba 0.07\n",
      "processing phrase 34 - loss 10.00 - proba 0.00\n",
      "processing phrase 35 - loss 10.00 - proba 0.00\n",
      "processing phrase 36 - loss 10.00 - proba 0.00\n",
      "processing phrase 37 - loss 10.00 - proba 0.00\n",
      "processing phrase 38 - loss 10.00 - proba 0.00\n",
      "processing phrase 39 - loss 10.00 - proba 0.00\n",
      "processing phrase 40 - loss 10.00 - proba 0.00\n",
      "processing phrase 41 - loss 10.00 - proba 0.00\n",
      "processing phrase 42 - loss 3.50 - proba 0.14\n",
      "processing phrase 43 - loss 10.00 - proba 0.00\n",
      "processing phrase 44 - loss 4.14 - proba 0.11\n",
      "processing phrase 45 - loss 3.53 - proba 0.14\n",
      "processing phrase 46 - loss 10.00 - proba 0.00\n",
      "processing phrase 47 - loss 10.00 - proba 0.00\n",
      "processing phrase 48 - loss 2.99 - proba 0.17\n",
      "processing phrase 49 - loss 3.61 - proba 0.14\n",
      "processing phrase 50 - loss 10.00 - proba 0.00\n",
      "processing phrase 51 - loss 10.00 - proba 0.00\n",
      "processing phrase 52 - loss 10.00 - proba 0.00\n",
      "processing phrase 53 - loss 4.91 - proba 0.08\n",
      "processing phrase 54 - loss 5.10 - proba 0.07\n",
      "processing phrase 55 - loss 3.61 - proba 0.14\n",
      "processing phrase 56 - loss 10.00 - proba 0.00\n",
      "processing phrase 57 - loss 3.08 - proba 0.16\n",
      "processing phrase 58 - loss 2.37 - proba 0.22\n",
      "processing phrase 59 - loss 5.33 - proba 0.07\n",
      "processing phrase 60 - loss 10.00 - proba 0.00\n",
      "processing phrase 61 - loss 10.00 - proba 0.00\n",
      "processing phrase 62 - loss 10.00 - proba 0.00\n",
      "processing phrase 63 - loss 10.00 - proba 0.00\n",
      "processing phrase 64 - loss 3.96 - proba 0.12\n",
      "processing phrase 65 - loss 3.05 - proba 0.16\n",
      "processing phrase 66 - loss 10.00 - proba 0.00\n",
      "processing phrase 67 - loss 10.00 - proba 0.00\n",
      "processing phrase 68 - loss 3.92 - proba 0.12\n",
      "processing phrase 69 - loss 10.00 - proba 0.00\n",
      "processing phrase 70 - loss 10.00 - proba 0.00\n",
      "processing phrase 71 - loss 3.88 - proba 0.12\n",
      "processing phrase 72 - loss 10.00 - proba 0.00\n",
      "processing phrase 73 - loss 10.00 - proba 0.00\n",
      "processing phrase 74 - loss 6.35 - proba 0.04\n",
      "processing phrase 75 - loss 10.00 - proba 0.00\n",
      "processing phrase 76 - loss 5.39 - proba 0.07\n",
      "processing phrase 77 - loss 4.24 - proba 0.11\n",
      "processing phrase 78 - loss 10.00 - proba 0.00\n",
      "processing phrase 79 - loss 3.97 - proba 0.12\n",
      "processing phrase 80 - loss 2.99 - proba 0.17\n",
      "processing phrase 81 - loss 10.00 - proba 0.00\n",
      "processing phrase 82 - loss 10.00 - proba 0.00\n",
      "processing phrase 83 - loss 10.00 - proba 0.00\n",
      "processing phrase 84 - loss 3.23 - proba 0.16\n",
      "processing phrase 85 - loss 3.13 - proba 0.16\n",
      "processing phrase 86 - loss 3.02 - proba 0.17\n",
      "processing phrase 87 - loss 3.99 - proba 0.12\n",
      "processing phrase 88 - loss 10.00 - proba 0.00\n",
      "processing phrase 89 - loss 3.51 - proba 0.14\n",
      "processing phrase 90 - loss 10.00 - proba 0.00\n",
      "processing phrase 91 - loss 4.06 - proba 0.12\n",
      "processing phrase 92 - loss 5.08 - proba 0.07\n",
      "processing phrase 93 - loss 5.27 - proba 0.07\n",
      "processing phrase 94 - loss 10.00 - proba 0.00\n",
      "processing phrase 95 - loss 3.94 - proba 0.12\n",
      "processing phrase 96 - loss 10.00 - proba 0.00\n",
      "processing phrase 97 - loss 10.00 - proba 0.00\n",
      "processing phrase 98 - loss 10.00 - proba 0.00\n",
      "processing phrase 99 - loss 3.78 - proba 0.13\n",
      "processing phrase 100 - loss 3.26 - proba 0.15\n",
      "processing phrase 101 - loss 10.00 - proba 0.00\n",
      "processing phrase 102 - loss 10.00 - proba 0.00\n",
      "processing phrase 103 - loss 10.00 - proba 0.00\n",
      "processing phrase 104 - loss 3.78 - proba 0.13\n",
      "processing phrase 105 - loss 10.00 - proba 0.00\n",
      "processing phrase 106 - loss 3.70 - proba 0.13\n",
      "processing phrase 107 - loss 3.56 - proba 0.14\n",
      "processing phrase 108 - loss 10.00 - proba 0.00\n",
      "processing phrase 109 - loss 4.09 - proba 0.12\n",
      "processing phrase 110 - loss 2.43 - proba 0.21\n",
      "processing phrase 111 - loss 10.00 - proba 0.00\n",
      "processing phrase 112 - loss 3.62 - proba 0.14\n",
      "processing phrase 113 - loss 3.62 - proba 0.14\n",
      "processing phrase 114 - loss 10.00 - proba 0.00\n",
      "processing phrase 115 - loss 3.49 - proba 0.14\n",
      "processing phrase 116 - loss 10.00 - proba 0.00\n",
      "processing phrase 117 - loss 10.00 - proba 0.00\n",
      "processing phrase 118 - loss 3.66 - proba 0.14\n",
      "processing phrase 119 - loss 3.37 - proba 0.15\n",
      "processing phrase 120 - loss 3.07 - proba 0.17\n",
      "processing phrase 121 - loss 3.42 - proba 0.15\n",
      "processing phrase 122 - loss 3.73 - proba 0.13\n",
      "processing phrase 123 - loss 3.39 - proba 0.15\n",
      "processing phrase 124 - loss 3.15 - proba 0.16\n",
      "processing phrase 125 - loss 10.00 - proba 0.00\n",
      "processing phrase 126 - loss 3.53 - proba 0.14\n",
      "processing phrase 127 - loss 10.00 - proba 0.00\n",
      "processing phrase 128 - loss 10.00 - proba 0.00\n",
      "processing phrase 129 - loss 10.00 - proba 0.00\n",
      "processing phrase 130 - loss 10.00 - proba 0.00\n",
      "processing phrase 131 - loss 10.00 - proba 0.00\n",
      "processing phrase 132 - loss 10.00 - proba 0.00\n",
      "processing phrase 133 - loss 10.00 - proba 0.00\n",
      "processing phrase 134 - loss 10.00 - proba 0.00\n",
      "processing phrase 135 - loss 3.72 - proba 0.13\n",
      "processing phrase 136 - loss 10.00 - proba 0.00\n",
      "processing phrase 137 - loss 4.89 - proba 0.08\n",
      "processing phrase 138 - loss 10.00 - proba 0.00\n",
      "processing phrase 139 - loss 10.00 - proba 0.00\n",
      "processing phrase 140 - loss 10.00 - proba 0.00\n",
      "processing phrase 141 - loss 10.00 - proba 0.00\n",
      "processing phrase 142 - loss 7.49 - proba 0.03\n",
      "processing phrase 143 - loss 4.47 - proba 0.10\n",
      "processing phrase 144 - loss 10.00 - proba 0.00\n",
      "processing phrase 145 - loss 10.00 - proba 0.00\n",
      "processing phrase 146 - loss 2.92 - proba 0.18\n",
      "processing phrase 147 - loss 10.00 - proba 0.00\n",
      "processing phrase 148 - loss 10.00 - proba 0.00\n",
      "processing phrase 149 - loss 10.00 - proba 0.00\n",
      "processing phrase 150 - loss 10.00 - proba 0.00\n",
      "processing phrase 151 - loss 10.00 - proba 0.00\n",
      "processing phrase 152 - loss 10.00 - proba 0.00\n",
      "processing phrase 153 - loss 10.00 - proba 0.00\n",
      "processing phrase 154 - loss 3.90 - proba 0.12\n",
      "processing phrase 155 - loss 10.00 - proba 0.00\n",
      "processing phrase 156 - loss 7.54 - proba 0.03\n",
      "processing phrase 157 - loss 10.00 - proba 0.00\n",
      "processing phrase 158 - loss 3.97 - proba 0.12\n",
      "processing phrase 159 - loss 10.00 - proba 0.00\n",
      "processing phrase 160 - loss 10.00 - proba 0.00\n",
      "processing phrase 161 - loss 3.89 - proba 0.12\n",
      "processing phrase 162 - loss 10.00 - proba 0.00\n",
      "processing phrase 163 - loss 10.00 - proba 0.00\n",
      "processing phrase 164 - loss 3.09 - proba 0.16\n",
      "processing phrase 165 - loss 10.00 - proba 0.00\n",
      "processing phrase 166 - loss 4.34 - proba 0.10\n",
      "processing phrase 167 - loss 4.35 - proba 0.10\n",
      "processing phrase 168 - loss 4.23 - proba 0.11\n",
      "processing phrase 169 - loss 10.00 - proba 0.00\n",
      "processing phrase 170 - loss 3.42 - proba 0.15\n",
      "processing phrase 171 - loss 4.40 - proba 0.10\n",
      "processing phrase 172 - loss 3.89 - proba 0.12\n",
      "processing phrase 173 - loss 3.18 - proba 0.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing phrase 174 - loss 3.12 - proba 0.16\n",
      "processing phrase 175 - loss 10.00 - proba 0.00\n",
      "processing phrase 176 - loss 3.71 - proba 0.13\n",
      "processing phrase 177 - loss 3.21 - proba 0.16\n",
      "processing phrase 178 - loss 10.00 - proba 0.00\n",
      "processing phrase 179 - loss 3.70 - proba 0.13\n",
      "processing phrase 180 - loss 3.70 - proba 0.13\n",
      "processing phrase 181 - loss 3.23 - proba 0.16\n",
      "processing phrase 182 - loss 3.02 - proba 0.17\n",
      "processing phrase 183 - loss 3.89 - proba 0.12\n",
      "processing phrase 184 - loss 10.00 - proba 0.00\n",
      "processing phrase 185 - loss 3.46 - proba 0.15\n",
      "processing phrase 186 - loss 10.00 - proba 0.00\n",
      "processing phrase 187 - loss 4.49 - proba 0.10\n",
      "processing phrase 188 - loss 3.19 - proba 0.16\n",
      "processing phrase 189 - loss 4.14 - proba 0.11\n",
      "processing phrase 190 - loss 10.00 - proba 0.00\n",
      "processing phrase 191 - loss 8.83 - proba 0.01\n",
      "processing phrase 192 - loss 2.92 - proba 0.17\n",
      "processing phrase 193 - loss 10.00 - proba 0.00\n",
      "processing phrase 194 - loss 10.00 - proba 0.00\n",
      "processing phrase 195 - loss 2.73 - proba 0.19\n",
      "processing phrase 196 - loss 2.98 - proba 0.17\n",
      "processing phrase 197 - loss 4.56 - proba 0.09\n",
      "processing phrase 198 - loss 3.39 - proba 0.15\n",
      "processing phrase 199 - loss 4.24 - proba 0.11\n",
      "processing phrase 200 - loss 10.00 - proba 0.00\n",
      "processing phrase 201 - loss 10.00 - proba 0.00\n",
      "processing phrase 202 - loss 2.17 - proba 0.23\n",
      "processing phrase 203 - loss 10.00 - proba 0.00\n",
      "processing phrase 204 - loss 3.72 - proba 0.13\n",
      "processing phrase 205 - loss 10.00 - proba 0.00\n",
      "processing phrase 206 - loss 10.00 - proba 0.00\n",
      "processing phrase 207 - loss 10.00 - proba 0.00\n",
      "processing phrase 208 - loss 10.00 - proba 0.00\n",
      "processing phrase 209 - loss 10.00 - proba 0.00\n",
      "processing phrase 210 - loss 10.00 - proba 0.00\n",
      "processing phrase 211 - loss 10.00 - proba 0.00\n",
      "processing phrase 212 - loss 5.08 - proba 0.08\n",
      "processing phrase 213 - loss 10.00 - proba 0.00\n",
      "processing phrase 214 - loss 4.08 - proba 0.11\n",
      "processing phrase 215 - loss 7.41 - proba 0.03\n",
      "processing phrase 216 - loss 10.00 - proba 0.00\n",
      "processing phrase 217 - loss 10.00 - proba 0.00\n",
      "processing phrase 218 - loss 4.08 - proba 0.12\n",
      "processing phrase 219 - loss 10.00 - proba 0.00\n",
      "processing phrase 220 - loss 10.00 - proba 0.00\n",
      "processing phrase 221 - loss 3.40 - proba 0.15\n",
      "processing phrase 222 - loss 10.00 - proba 0.00\n",
      "processing phrase 223 - loss 4.47 - proba 0.10\n",
      "processing phrase 224 - loss 10.00 - proba 0.00\n",
      "processing phrase 225 - loss 10.00 - proba 0.00\n",
      "processing phrase 226 - loss 10.00 - proba 0.00\n",
      "processing phrase 227 - loss 10.00 - proba 0.00\n",
      "processing phrase 228 - loss 10.00 - proba 0.00\n",
      "processing phrase 229 - loss 10.00 - proba 0.00\n",
      "processing phrase 230 - loss 10.00 - proba 0.00\n",
      "processing phrase 231 - loss 10.00 - proba 0.00\n",
      "processing phrase 232 - loss 10.00 - proba 0.00\n",
      "processing phrase 233 - loss 3.58 - proba 0.14\n",
      "processing phrase 234 - loss 3.39 - proba 0.15\n",
      "processing phrase 235 - loss 10.00 - proba 0.00\n",
      "processing phrase 236 - loss 3.44 - proba 0.15\n",
      "processing phrase 237 - loss 10.00 - proba 0.00\n",
      "processing phrase 238 - loss 3.59 - proba 0.14\n",
      "processing phrase 239 - loss 3.46 - proba 0.15\n",
      "processing phrase 240 - loss 10.00 - proba 0.00\n",
      "processing phrase 241 - loss 4.24 - proba 0.11\n",
      "processing phrase 242 - loss 4.04 - proba 0.12\n",
      "processing phrase 243 - loss 10.00 - proba 0.00\n",
      "processing phrase 244 - loss 10.00 - proba 0.00\n",
      "processing phrase 245 - loss 10.00 - proba 0.00\n",
      "processing phrase 246 - loss 10.00 - proba 0.00\n",
      "processing phrase 247 - loss 4.32 - proba 0.10\n",
      "processing phrase 248 - loss 4.12 - proba 0.11\n",
      "processing phrase 249 - loss 10.00 - proba 0.00\n",
      "processing phrase 250 - loss 10.00 - proba 0.00\n",
      "processing phrase 251 - loss 10.00 - proba 0.00\n",
      "processing phrase 252 - loss 10.00 - proba 0.00\n",
      "processing phrase 253 - loss 10.00 - proba 0.00\n",
      "processing phrase 254 - loss 10.00 - proba 0.00\n",
      "processing phrase 255 - loss 10.00 - proba 0.00\n",
      "processing phrase 256 - loss 10.00 - proba 0.00\n",
      "processing phrase 257 - loss 10.00 - proba 0.00\n",
      "processing phrase 258 - loss 10.00 - proba 0.00\n",
      "processing phrase 259 - loss 4.20 - proba 0.11\n",
      "processing phrase 260 - loss 3.51 - proba 0.14\n",
      "processing phrase 261 - loss 2.87 - proba 0.18\n",
      "processing phrase 262 - loss 3.12 - proba 0.16\n",
      "processing phrase 263 - loss 4.08 - proba 0.11\n",
      "processing phrase 264 - loss 10.00 - proba 0.00\n",
      "processing phrase 265 - loss 10.00 - proba 0.00\n",
      "processing phrase 266 - loss 10.00 - proba 0.00\n",
      "processing phrase 267 - loss 10.00 - proba 0.00\n",
      "processing phrase 268 - loss 10.00 - proba 0.00\n",
      "processing phrase 269 - loss 10.00 - proba 0.00\n",
      "processing phrase 270 - loss 10.00 - proba 0.00\n",
      "processing phrase 271 - loss 10.00 - proba 0.00\n",
      "processing phrase 272 - loss 10.00 - proba 0.00\n",
      "processing phrase 273 - loss 2.87 - proba 0.18\n",
      "processing phrase 274 - loss 10.00 - proba 0.00\n",
      "processing phrase 275 - loss 10.00 - proba 0.00\n",
      "processing phrase 276 - loss 10.00 - proba 0.00\n",
      "processing phrase 277 - loss 10.00 - proba 0.00\n",
      "processing phrase 278 - loss 10.00 - proba 0.00\n",
      "processing phrase 279 - loss 10.00 - proba 0.00\n",
      "processing phrase 280 - loss 10.00 - proba 0.00\n",
      "processing phrase 281 - loss 10.00 - proba 0.00\n",
      "processing phrase 282 - loss 2.87 - proba 0.18\n",
      "processing phrase 283 - loss 10.00 - proba 0.00\n",
      "processing phrase 284 - loss 10.00 - proba 0.00\n",
      "processing phrase 285 - loss 10.00 - proba 0.00\n",
      "processing phrase 286 - loss 10.00 - proba 0.00\n",
      "processing phrase 287 - loss 10.00 - proba 0.00\n",
      "processing phrase 288 - loss 10.00 - proba 0.00\n",
      "processing phrase 289 - loss 10.00 - proba 0.00\n",
      "processing phrase 290 - loss 10.00 - proba 0.00\n",
      "processing phrase 291 - loss 10.00 - proba 0.00\n",
      "processing phrase 292 - loss 10.00 - proba 0.00\n",
      "processing phrase 293 - loss 3.57 - proba 0.14\n",
      "processing phrase 294 - loss 3.85 - proba 0.13\n",
      "processing phrase 295 - loss 10.00 - proba 0.00\n",
      "processing phrase 296 - loss 10.00 - proba 0.00\n",
      "processing phrase 297 - loss 10.00 - proba 0.00\n",
      "processing phrase 298 - loss 10.00 - proba 0.00\n",
      "processing phrase 299 - loss 10.00 - proba 0.00\n",
      "processing phrase 300 - loss 10.00 - proba 0.00\n",
      "processing phrase 301 - loss 10.00 - proba 0.00\n",
      "processing phrase 302 - loss 10.00 - proba 0.00\n",
      "processing phrase 303 - loss 10.00 - proba 0.00\n",
      "processing phrase 304 - loss 10.00 - proba 0.00\n",
      "processing phrase 305 - loss 10.00 - proba 0.00\n",
      "processing phrase 306 - loss 10.00 - proba 0.00\n",
      "processing phrase 307 - loss 3.48 - proba 0.14\n",
      "processing phrase 308 - loss 10.00 - proba 0.00\n",
      "processing phrase 309 - loss 3.61 - proba 0.13\n",
      "processing phrase 310 - loss 10.00 - proba 0.00\n",
      "processing phrase 311 - loss 10.00 - proba 0.00\n",
      "processing phrase 312 - loss 10.00 - proba 0.00\n",
      "processing phrase 313 - loss 10.00 - proba 0.00\n",
      "processing phrase 314 - loss 10.00 - proba 0.00\n",
      "processing phrase 315 - loss 10.00 - proba 0.00\n",
      "processing phrase 316 - loss 6.46 - proba 0.04\n",
      "processing phrase 317 - loss 4.47 - proba 0.10\n",
      "processing phrase 318 - loss 10.00 - proba 0.00\n",
      "processing phrase 319 - loss 10.00 - proba 0.00\n",
      "processing phrase 320 - loss 3.44 - proba 0.14\n",
      "processing phrase 321 - loss 10.00 - proba 0.00\n",
      "processing phrase 322 - loss 10.00 - proba 0.00\n",
      "processing phrase 323 - loss 4.32 - proba 0.10\n",
      "processing phrase 324 - loss 10.00 - proba 0.00\n",
      "processing phrase 325 - loss 10.00 - proba 0.00\n",
      "processing phrase 326 - loss 3.68 - proba 0.13\n",
      "processing phrase 327 - loss 10.00 - proba 0.00\n",
      "processing phrase 328 - loss 10.00 - proba 0.00\n",
      "processing phrase 329 - loss 10.00 - proba 0.00\n",
      "processing phrase 330 - loss 10.00 - proba 0.00\n",
      "processing phrase 331 - loss 4.06 - proba 0.12\n",
      "processing phrase 332 - loss 10.00 - proba 0.00\n",
      "processing phrase 333 - loss 10.00 - proba 0.00\n",
      "processing phrase 334 - loss 7.24 - proba 0.03\n",
      "processing phrase 335 - loss 4.47 - proba 0.10\n",
      "processing phrase 336 - loss 10.00 - proba 0.00\n",
      "processing phrase 337 - loss 10.00 - proba 0.00\n",
      "processing phrase 338 - loss 10.00 - proba 0.00\n",
      "processing phrase 339 - loss 10.00 - proba 0.00\n",
      "processing phrase 340 - loss 10.00 - proba 0.00\n",
      "processing phrase 341 - loss 3.13 - proba 0.16\n",
      "processing phrase 342 - loss 10.00 - proba 0.00\n",
      "processing phrase 343 - loss 10.00 - proba 0.00\n",
      "processing phrase 344 - loss 10.00 - proba 0.00\n",
      "processing phrase 345 - loss 10.00 - proba 0.00\n",
      "processing phrase 346 - loss 10.00 - proba 0.00\n",
      "processing phrase 347 - loss 10.00 - proba 0.00\n",
      "processing phrase 348 - loss 10.00 - proba 0.00\n",
      "processing phrase 349 - loss 10.00 - proba 0.00\n",
      "processing phrase 350 - loss 10.00 - proba 0.00\n",
      "processing phrase 351 - loss 10.00 - proba 0.00\n",
      "processing phrase 352 - loss 10.00 - proba 0.00\n",
      "processing phrase 353 - loss 10.00 - proba 0.00\n",
      "processing phrase 354 - loss 10.00 - proba 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing phrase 355 - loss 3.61 - proba 0.14\n",
      "processing phrase 356 - loss 10.00 - proba 0.00\n",
      "processing phrase 357 - loss 10.00 - proba 0.00\n",
      "processing phrase 358 - loss 10.00 - proba 0.00\n",
      "processing phrase 359 - loss 10.00 - proba 0.00\n",
      "processing phrase 360 - loss 10.00 - proba 0.00\n",
      "processing phrase 361 - loss 10.00 - proba 0.00\n",
      "processing phrase 362 - loss 10.00 - proba 0.00\n",
      "processing phrase 363 - loss 10.00 - proba 0.00\n",
      "processing phrase 364 - loss 10.00 - proba 0.00\n",
      "processing phrase 365 - loss 10.00 - proba 0.00\n",
      "processing phrase 366 - loss 10.00 - proba 0.00\n",
      "processing phrase 367 - loss 4.11 - proba 0.11\n",
      "processing phrase 368 - loss 10.00 - proba 0.00\n",
      "processing phrase 369 - loss 4.58 - proba 0.09\n",
      "processing phrase 370 - loss 10.00 - proba 0.00\n",
      "processing phrase 371 - loss 10.00 - proba 0.00\n",
      "processing phrase 372 - loss 10.00 - proba 0.00\n",
      "processing phrase 373 - loss 3.92 - proba 0.12\n",
      "processing phrase 374 - loss 10.00 - proba 0.00\n",
      "processing phrase 375 - loss 10.00 - proba 0.00\n",
      "processing phrase 376 - loss 10.00 - proba 0.00\n",
      "processing phrase 377 - loss 10.00 - proba 0.00\n",
      "processing phrase 378 - loss 10.00 - proba 0.00\n",
      "processing phrase 379 - loss 7.00 - proba 0.03\n",
      "processing phrase 380 - loss 10.00 - proba 0.00\n",
      "processing phrase 381 - loss 10.00 - proba 0.00\n",
      "processing phrase 382 - loss 10.00 - proba 0.00\n",
      "processing phrase 383 - loss 10.00 - proba 0.00\n",
      "processing phrase 384 - loss 10.00 - proba 0.00\n",
      "processing phrase 385 - loss 10.00 - proba 0.00\n",
      "processing phrase 386 - loss 10.00 - proba 0.00\n",
      "processing phrase 387 - loss 10.00 - proba 0.00\n",
      "processing phrase 388 - loss 10.00 - proba 0.00\n",
      "processing phrase 389 - loss 3.35 - proba 0.15\n",
      "processing phrase 390 - loss 10.00 - proba 0.00\n",
      "processing phrase 391 - loss 3.16 - proba 0.16\n",
      "Time lapse 134.97\n"
     ]
    }
   ],
   "source": [
    "question = 'how to store sparse matrices?'\n",
    "res = findBestAnswer(pdf, question, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l, p in res:\n",
    "    print('- Answer with loss {} proba {}: \\n{}\\n'.format(l, p, sens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Topic Modeling and Document Clustering One particular technique that is often applied to text data is topic modeling, which is\n",
    "an umbrella term describing the task of assigning each document to one or multiple\n",
    "topics,  usually  without  supervision. A  good  example  for  this  is  news  data,  which\n",
    "might be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\n",
    "document  is  assigned  a  single  topic,  this  is  the  task  of  clustering  the  documents,  as\n",
    "discussed  in  Chapter  3.'''\n",
    "loss, p = getLossAndProbas('what is topic modeling?', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.351862\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Out[39]:\n",
      "- X_train_lemma.shape:  (25000, 21596)\n",
      "- X_train.shape:  (25000, 27271)\n",
      "- As you can see from the output, lemmatization reduced the number of features from\n",
      "27,271  (with  the  standard  CountVectorizer  processing)  to  21,596.\n",
      "- Lemmatization\n",
      "can be seen as a kind of regularization, as it conflates certain features.\n",
      "- Therefore, we\n",
      "expect  lemmatization\n",
      "- to  improve  performance  most  when  the  dataset  is  small.\n",
      "- To\n",
      "illustrate\n",
      "- how  lemmatization  can  help,  we  will  use\n",
      "- StratifiedShuffleSplit\n",
      "- for\n",
      "cross-validation, using only 1% of the data as training data and the rest as test data\n",
      "- :\n",
      "\n",
      "In[40]:\n",
      "- # build a grid search using only 1% of the data as the training set\n",
      "from sklearn.model_selection import\n",
      "- StratifiedShuffleSplit\n",
      "- param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
      "- cv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\n",
      "                            train_size=0.01, random_state=0)\n",
      "- grid =\n",
      "- GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
      "- # perform grid search with standard CountVectorizer\n",
      "grid.fit(X_train, y_train)\n",
      "- print(\"Best cross-validation score \"\n",
      "      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
      "- # perform grid search with lemmatization\n",
      "- grid.fit(X_train_lemma, y_train)\n",
      "- print(\"Best cross-validation score \"\n",
      "      \"(lemmatization): {:.3f}\".format(grid.best_score_))\n",
      "- Out[40]:\n",
      "- Best cross-validation score (standard CountVectorizer): 0.721\n",
      "- Best cross-validation score (lemmatization): 0.731\n"
     ]
    }
   ],
   "source": [
    "raw_text = '''\n",
    "\f",
    "Out[39]:\n",
    "\n",
    "X_train_lemma.shape:  (25000, 21596)\n",
    "X_train.shape:  (25000, 27271)\n",
    "\n",
    "As you can see from the output, lemmatization reduced the number of features from\n",
    "27,271  (with  the  standard  CountVectorizer  processing)  to  21,596.  Lemmatization\n",
    "can be seen as a kind of regularization, as it conflates certain features. Therefore, we\n",
    "expect  lemmatization  to  improve  performance  most  when  the  dataset  is  small.  To\n",
    "illustrate  how  lemmatization  can  help,  we  will  use  StratifiedShuffleSplit  for\n",
    "cross-validation, using only 1% of the data as training data and the rest as test data:\n",
    "\n",
    "In[40]:\n",
    "\n",
    "# build a grid search using only 1% of the data as the training set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "cv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\n",
    "                            train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n",
    "# perform grid search with standard CountVectorizer\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    "      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "# perform grid search with lemmatization\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(\"Best cross-validation score \"\n",
    "      \"(lemmatization): {:.3f}\".format(grid.best_score_))\n",
    "\n",
    "Out[40]:\n",
    "\n",
    "Best cross-validation score (standard CountVectorizer): 0.721\n",
    "Best cross-validation score (lemmatization): 0.731\n",
    "\n",
    "'''\n",
    "phrases = splitToSentences(raw_text, nlp)\n",
    "for ph in phrases:\n",
    "    print('-', ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quang",
   "language": "python",
   "name": "quang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
