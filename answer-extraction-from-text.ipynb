{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\r\n",
      "To initialize your shell, run\r\n",
      "\r\n",
      "    $ conda init <SHELL_NAME>\r\n",
      "\r\n",
      "Currently supported shells are:\r\n",
      "  - bash\r\n",
      "  - fish\r\n",
      "  - tcsh\r\n",
      "  - xonsh\r\n",
      "  - zsh\r\n",
      "  - powershell\r\n",
      "\r\n",
      "See 'conda init --help' for more information and options.\r\n",
      "\r\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! conda activate quang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('models/tanda_roberta_large_asnq_wikiqa/ckpt/')\n",
    "model = RobertaForSequenceClassification.from_pretrained('models/tanda_roberta_large_asnq_wikiqa/ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7686, grad_fn=<NllLossBackward>) tensor([[ 1.8508, -1.8945]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"what is machine learning? Machine learning is a subbranch of artificial intelligence\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print(loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8977, grad_fn=<NllLossBackward>) tensor([[ 1.9230, -1.9542]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"what is machine learning? deep learning is fun but complicated\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print(loss, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- In machine learning, the hinge loss is a loss function used for training classifiers.\n",
      "- 0.1936516910791397-tensor([[0.2671, 0.6304]], grad_fn=<SigmoidBackward>)\n",
      "- The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).[1]\n",
      "- 3.6040492057800293-tensor([[0.8516, 0.1384]], grad_fn=<SigmoidBackward>)\n",
      "- For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as\n",
      "\n",
      "{\\displaystyle \\ell (y)=\\max(0,1-t\\cdot y)}\\ell(y) = \\max(0, 1-t \\cdot y)\n",
      "Note that {\\displaystyle y}y should be the \"raw\" output of the classifier's decision function, not the predicted class label.\n",
      "- 3.3854787349700928-tensor([[0.8378, 0.1533]], grad_fn=<SigmoidBackward>)\n",
      "- For instance, in linear SVMs, {\\displaystyle y=\\mathbf {w} \\cdot \\mathbf {x} +b}y = \\mathbf{w} \\cdot \\mathbf{x} + b, where {\\displaystyle (\\mathbf {w} ,b)}(\\mathbf{w},b) are the parameters of the hyperplane and {\\displaystyle \\mathbf {x} }\\mathbf {x}  is the input variable(s).\n",
      "- 3.5059237480163574-tensor([[0.8463, 0.1456]], grad_fn=<SigmoidBackward>)\n",
      "- When t and y have the same sign (meaning y predicts the right class) and {\\displaystyle |y|\\geq 1}|y| \\ge 1, the hinge loss {\\displaystyle \\ell (y)=0}\\ell(y) = 0.\n",
      "- 3.2984507083892822-tensor([[0.8290, 0.1568]], grad_fn=<SigmoidBackward>)\n",
      "- When they have opposite signs, {\\displaystyle \\ell (y)}\\ell(y) increases linearly with y, and similarly if {\\displaystyle |y|<1}{\\displaystyle |y|<1}, even if it has the same sign (correct prediction, but not by enough margin).\n",
      "- 3.909883737564087-tensor([[0.8749, 0.1251]], grad_fn=<SigmoidBackward>)\n",
      "- What do you want to do with this, this is bullshit\n",
      "- 3.8874337673187256-tensor([[0.8716, 0.1244]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "\n",
    "raw_text = '''\n",
    "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs).[1]\n",
    "\n",
    "For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as\n",
    "\n",
    "{\\displaystyle \\ell (y)=\\max(0,1-t\\cdot y)}\\ell(y) = \\max(0, 1-t \\cdot y)\n",
    "Note that {\\displaystyle y}y should be the \"raw\" output of the classifier's decision function, not the predicted class label. For instance, in linear SVMs, {\\displaystyle y=\\mathbf {w} \\cdot \\mathbf {x} +b}y = \\mathbf{w} \\cdot \\mathbf{x} + b, where {\\displaystyle (\\mathbf {w} ,b)}(\\mathbf{w},b) are the parameters of the hyperplane and {\\displaystyle \\mathbf {x} }\\mathbf {x}  is the input variable(s).\n",
    "\n",
    "When t and y have the same sign (meaning y predicts the right class) and {\\displaystyle |y|\\geq 1}|y| \\ge 1, the hinge loss {\\displaystyle \\ell (y)=0}\\ell(y) = 0. When they have opposite signs, {\\displaystyle \\ell (y)}\\ell(y) increases linearly with y, and similarly if {\\displaystyle |y|<1}{\\displaystyle |y|<1}, even if it has the same sign (correct prediction, but not by enough margin).\n",
    "What do you want to do with this, this is bullshit\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "question = 'what is hinge loss?'\n",
    "for sen in sentences:\n",
    "    inputs = tokenizer(question + ' ' + sen, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "    print('- {}\\n- {}-{}'.format(sen, loss, torch.sigmoid(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- In this case, lemmatization provided a modest improvement in performance.\n",
      "- 4.38135290145874-tensor([[0.8999, 0.1022]], grad_fn=<SigmoidBackward>)\n",
      "- As with\n",
      "many  of  the  different  feature  extraction  techniques,  the  result  varies  depending  on\n",
      "the dataset.\n",
      "- 3.9671108722686768-tensor([[0.8764, 0.1203]], grad_fn=<SigmoidBackward>)\n",
      "- Lemmatization and stemming can sometimes help in building better (or\n",
      "at least more compact) models, so we suggest you give these techniques a try when\n",
      "trying to squeeze out the last bit of performance on a particular task.\n",
      "- 4.316229820251465-tensor([[0.8975, 0.1059]], grad_fn=<SigmoidBackward>)\n",
      "- Topic Modeling and Document Clustering\n",
      "- 2.6041598320007324-tensor([[0.7490, 0.1925]], grad_fn=<SigmoidBackward>)\n",
      "- One particular technique that is often applied to text data is topic modeling, which is\n",
      "an umbrella term describing the task of assigning each document to one or multiple\n",
      "topics,  usually  without  supervision.\n",
      "- 0.3845011293888092-tensor([[0.3357, 0.5188]], grad_fn=<SigmoidBackward>)\n",
      "- A  good  example  for  this  is  news  data,  which\n",
      "might be categorized into topics like “politics,” “sports,” “finance,” and so on.\n",
      "- 3.2465505599975586-tensor([[0.8209, 0.1565]], grad_fn=<SigmoidBackward>)\n",
      "- If each\n",
      "document  is  assigned  a  single  topic,  this  is  the  task  of  clustering  the  documents,  as\n",
      "discussed  in  Chapter  3.\n",
      "- 3.0403759479522705-tensor([[0.7998, 0.1671]], grad_fn=<SigmoidBackward>)\n",
      "- If  each  document  can  have  more  than  one  topic,  the  task\n",
      "\n",
      "Topic Modeling and Document Clustering\n",
      "- 3.668184757232666-tensor([[0.8565, 0.1352]], grad_fn=<SigmoidBackward>)\n",
      "- |\n",
      "- 5.494536876678467-tensor([[0.9426, 0.0635]], grad_fn=<SigmoidBackward>)\n",
      "- 347\n",
      "\n",
      "\f",
      "relates  to  the  decomposition  methods  from  Chapter  3.\n",
      "- 4.537026882171631-tensor([[0.9073, 0.0958]], grad_fn=<SigmoidBackward>)\n",
      "- Each  of  the  components\n",
      "- 3.179126262664795-tensor([[0.8119, 0.1579]], grad_fn=<SigmoidBackward>)\n",
      "- we\n",
      "learn\n",
      "- 3.8770458698272705-tensor([[0.8696, 0.1236]], grad_fn=<SigmoidBackward>)\n",
      "- then  corresponds  to  one  topic,  and  the  coefficients  of  the  components  in\n",
      "- 3.522951364517212-tensor([[0.8436, 0.1409]], grad_fn=<SigmoidBackward>)\n",
      "- the\n",
      "representation of a document tell us how strongly related that document is to a par‐\n",
      "ticular topic.\n",
      "- 2.9542882442474365-tensor([[0.7867, 0.1686]], grad_fn=<SigmoidBackward>)\n",
      "- Often, when people talk about topic modeling, they refer to one particu‐\n",
      "lar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\n",
      "- 3.367800712585449-tensor([[0.8335, 0.1516]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "\n",
    "raw_text = '''\n",
    "In this case, lemmatization provided a modest improvement in performance. As with\n",
    "many  of  the  different  feature  extraction  techniques,  the  result  varies  depending  on\n",
    "the dataset. Lemmatization and stemming can sometimes help in building better (or\n",
    "at least more compact) models, so we suggest you give these techniques a try when\n",
    "trying to squeeze out the last bit of performance on a particular task.\n",
    "\n",
    "Topic Modeling and Document Clustering\n",
    "One particular technique that is often applied to text data is topic modeling, which is\n",
    "an umbrella term describing the task of assigning each document to one or multiple\n",
    "topics,  usually  without  supervision.  A  good  example  for  this  is  news  data,  which\n",
    "might be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\n",
    "document  is  assigned  a  single  topic,  this  is  the  task  of  clustering  the  documents,  as\n",
    "discussed  in  Chapter  3.  If  each  document  can  have  more  than  one  topic,  the  task\n",
    "\n",
    "Topic Modeling and Document Clustering \n",
    "\n",
    "| \n",
    "\n",
    "347\n",
    "\n",
    "\f",
    "relates  to  the  decomposition  methods  from  Chapter  3.  Each  of  the  components  we\n",
    "learn  then  corresponds  to  one  topic,  and  the  coefficients  of  the  components  in  the\n",
    "representation of a document tell us how strongly related that document is to a par‐\n",
    "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\n",
    "lar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "question = 'what is topic modeling?'\n",
    "for sen in sentences:\n",
    "    inputs = tokenizer(question + ' ' + sen, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "    print('- {}\\n- {}-{}'.format(sen, loss, torch.sigmoid(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- How to calculate a logistic sigmoid function in Python\n",
      "- 1.9963171482086182-tensor([[0.6743, 0.2455]], grad_fn=<SigmoidBackward>)\n",
      "- The logistic sigmoid function defined as (1/(1 + e^-x)) takes an input x of any real number and returns an output value in the range of -1 and 1.\n",
      "- 2.7299745082855225-tensor([[0.7670, 0.1868]], grad_fn=<SigmoidBackward>)\n",
      "- DEFINE A LOGISTIC SIGMOID FUNCTION\n",
      "- 3.7228972911834717-tensor([[0.8613, 0.1333]], grad_fn=<SigmoidBackward>)\n",
      "- Define a new logistic sigmoid function that takes input x and returns 1/(1 + math.exp(-x)).\n",
      "- 2.5126426219940186-tensor([[0.7408, 0.2013]], grad_fn=<SigmoidBackward>)\n",
      "- def sigmoid(x):\n",
      "  return 1 / (1 + math.exp(-x))\n",
      "\n",
      "print(sigmoid(0.5))\n",
      "OUTPUT\n",
      "0.6224593312018546\n",
      "- 3.4740891456604004-tensor([[0.8406, 0.1443]], grad_fn=<SigmoidBackward>)\n",
      "- A function is a block of organized, reusable code that is used to perform a single, related action.\n",
      "- 3.841078281402588-tensor([[0.8692, 0.1272]], grad_fn=<SigmoidBackward>)\n",
      "- Functions provide better modularity for your application and a high degree of code reusing.\n",
      "- 5.446642875671387-tensor([[0.9407, 0.0643]], grad_fn=<SigmoidBackward>)\n",
      "- As you already know, Python gives you many built-in functions like print(), etc.\n",
      "- 3.56270694732666-tensor([[0.8477, 0.1398]], grad_fn=<SigmoidBackward>)\n",
      "- but you can also create your own functions.\n",
      "- 2.707369565963745-tensor([[0.7638, 0.1877]], grad_fn=<SigmoidBackward>)\n",
      "- These functions are called user-defined functions.\n",
      "- 3.464784860610962-tensor([[0.8394, 0.1444]], grad_fn=<SigmoidBackward>)\n",
      "- Defining a Function\n",
      "- 4.030460834503174-tensor([[0.8797, 0.1168]], grad_fn=<SigmoidBackward>)\n",
      "- You can define functions to provide the required functionality.\n",
      "- 3.2961316108703613-tensor([[0.8242, 0.1527]], grad_fn=<SigmoidBackward>)\n",
      "- Here are simple rules to define a function in Python.\n",
      "- 2.5707755088806152-tensor([[0.7440, 0.1940]], grad_fn=<SigmoidBackward>)\n",
      "- Function blocks begin with the keyword def followed by the function name and parentheses ( ( ) ).\n",
      "- 3.3803226947784424-tensor([[0.8310, 0.1476]], grad_fn=<SigmoidBackward>)\n",
      "- Any input parameters or arguments should be placed within these parentheses.\n",
      "- 3.80684494972229-tensor([[0.8643, 0.1264]], grad_fn=<SigmoidBackward>)\n",
      "- You can also define parameters inside these parentheses.\n",
      "- 3.6359682083129883-tensor([[0.8522, 0.1351]], grad_fn=<SigmoidBackward>)\n",
      "- The first statement of a function can be an optional statement - the documentation string of the function or docstring.\n",
      "- 3.8855462074279785-tensor([[0.8730, 0.1260]], grad_fn=<SigmoidBackward>)\n",
      "- The code block within every function starts with a colon (:) and is indented.\n",
      "- 3.6545917987823486-tensor([[0.8547, 0.1351]], grad_fn=<SigmoidBackward>)\n",
      "- The statement return [expression] exits a function, optionally passing back an expression to the caller.\n",
      "- 3.753286123275757-tensor([[0.8624, 0.1308]], grad_fn=<SigmoidBackward>)\n",
      "- A return statement with no arguments is the same as return None.\n",
      "- 3.9739835262298584-tensor([[0.8786, 0.1218]], grad_fn=<SigmoidBackward>)\n",
      "- Syntax\n",
      "- 3.7123944759368896-tensor([[0.8581, 0.1314]], grad_fn=<SigmoidBackward>)\n",
      "- def functionname( parameters\n",
      "- 3.007530927658081-tensor([[0.7908, 0.1642]], grad_fn=<SigmoidBackward>)\n",
      "- ):\n",
      "   \"function_docstring\"\n",
      "   function_suite\n",
      "   return [expression]\n",
      "- 3.879805088043213-tensor([[0.8725, 0.1261]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "\n",
    "raw_text = '''\n",
    "How to calculate a logistic sigmoid function in Python\n",
    "The logistic sigmoid function defined as (1/(1 + e^-x)) takes an input x of any real number and returns an output value in the range of -1 and 1.\n",
    "\n",
    "DEFINE A LOGISTIC SIGMOID FUNCTION\n",
    "Define a new logistic sigmoid function that takes input x and returns 1/(1 + math.exp(-x)).\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "print(sigmoid(0.5))\n",
    "OUTPUT\n",
    "0.6224593312018546\n",
    "A function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and a high degree of code reusing.\n",
    "\n",
    "As you already know, Python gives you many built-in functions like print(), etc. but you can also create your own functions. These functions are called user-defined functions.\n",
    "\n",
    "Defining a Function\n",
    "You can define functions to provide the required functionality. Here are simple rules to define a function in Python.\n",
    "\n",
    "Function blocks begin with the keyword def followed by the function name and parentheses ( ( ) ).\n",
    "\n",
    "Any input parameters or arguments should be placed within these parentheses. You can also define parameters inside these parentheses.\n",
    "\n",
    "The first statement of a function can be an optional statement - the documentation string of the function or docstring.\n",
    "\n",
    "The code block within every function starts with a colon (:) and is indented.\n",
    "\n",
    "The statement return [expression] exits a function, optionally passing back an expression to the caller. A return statement with no arguments is the same as return None.\n",
    "\n",
    "Syntax\n",
    "def functionname( parameters ):\n",
    "   \"function_docstring\"\n",
    "   function_suite\n",
    "   return [expression]\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "question = 'how to write sigmoid function in python?'\n",
    "for sen in sentences:\n",
    "    inputs = tokenizer(question + ' ' + sen, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "    print('- {}\\n- {}-{}'.format(sen, loss, torch.sigmoid(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- This is a logistic sigmoid function:\n",
      "- 3.9002280235290527-tensor([[0.8737, 0.1251]], grad_fn=<SigmoidBackward>)\n",
      "- enter image description here\n",
      "- 4.813180446624756-tensor([[0.9175, 0.0835]], grad_fn=<SigmoidBackward>)\n",
      "- I know\n",
      "- 3.5941247940063477-tensor([[0.8480, 0.1362]], grad_fn=<SigmoidBackward>)\n",
      "- x.\n",
      "- 4.324370384216309-tensor([[0.8967, 0.1043]], grad_fn=<SigmoidBackward>)\n",
      "- How can I calculate F(x) in Python now?\n",
      "- 2.9022889137268066-tensor([[0.7852, 0.1752]], grad_fn=<SigmoidBackward>)\n",
      "- Let's say\n",
      "- 3.7665536403656006-tensor([[0.8614, 0.1283]], grad_fn=<SigmoidBackward>)\n",
      "- x = 0.458.\n",
      "- 3.945207357406616-tensor([[0.8763, 0.1226]], grad_fn=<SigmoidBackward>)\n",
      "- F(x) = ?\n",
      "- 3.519068717956543-tensor([[0.8435, 0.1413]], grad_fn=<SigmoidBackward>)\n",
      "- This should do it:\n",
      "- 3.3282132148742676-tensor([[0.8282, 0.1521]], grad_fn=<SigmoidBackward>)\n",
      "- import math\n",
      "- 3.1396384239196777-tensor([[0.8068, 0.1589]], grad_fn=<SigmoidBackward>)\n",
      "- def sigmoid(x):\n",
      "  return 1 / (1 + math.exp(-x))\n",
      "- 3.0630271434783936-tensor([[0.8029, 0.1665]], grad_fn=<SigmoidBackward>)\n",
      "- And now you can test it by calling:\n",
      "\n",
      ">\n",
      "- 4.847705841064453-tensor([[0.9215, 0.0849]], grad_fn=<SigmoidBackward>)\n",
      "- >> sigmoid(0.458)\n",
      "- 4.107057094573975-tensor([[0.8852, 0.1143]], grad_fn=<SigmoidBackward>)\n",
      "- 0.61253961344091512\n",
      "Update:\n",
      "- 7.993255138397217-tensor([[0.9844, 0.0209]], grad_fn=<SigmoidBackward>)\n",
      "- Note that the above was mainly intended as a straight one-to-one translation of the given expression into Python code.\n",
      "- 3.645684242248535-tensor([[0.8549, 0.1364]], grad_fn=<SigmoidBackward>)\n",
      "- It is not tested or known to be a numerically sound implementation.\n",
      "- 3.583373546600342-tensor([[0.8482, 0.1377]], grad_fn=<SigmoidBackward>)\n",
      "- If you know you need a very robust implementation, I'm sure there are others where people have actually given this problem some thought.\n",
      "- 4.751412868499756-tensor([[0.9156, 0.0864]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "\n",
    "raw_text = '''\n",
    "This is a logistic sigmoid function:\n",
    "\n",
    "enter image description here\n",
    "\n",
    "I know x. How can I calculate F(x) in Python now?\n",
    "\n",
    "Let's say x = 0.458.\n",
    "\n",
    "F(x) = ?\n",
    "This should do it:\n",
    "\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "And now you can test it by calling:\n",
    "\n",
    ">>> sigmoid(0.458)\n",
    "0.61253961344091512\n",
    "Update: Note that the above was mainly intended as a straight one-to-one translation of the given expression into Python code. It is not tested or known to be a numerically sound implementation. If you know you need a very robust implementation, I'm sure there are others where people have actually given this problem some thought.\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "question = 'how to write sigmoid function in python?'\n",
    "for sen in sentences:\n",
    "    inputs = tokenizer(question + ' ' + sen, return_tensors=\"pt\")\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss, logits = outputs[:2]\n",
    "    print('- {}\\n- {}-{}'.format(sen, loss, torch.sigmoid(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- If you know you need a very robust implementation, I'm sure there are others where people have actually given this problem some thought.\n",
      "- 2.9642016887664795-tensor([[0.7923, 0.1719]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "\n",
    "raw_text = '''\n",
    "This is a logistic sigmoid function:\n",
    "\n",
    "enter image description here\n",
    "\n",
    "I know x. How can I calculate F(x) in Python now?\n",
    "\n",
    "Let's say x = 0.458.\n",
    "\n",
    "F(x) = ?\n",
    "This should do it:\n",
    "\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "And now you can test it by calling:\n",
    "\n",
    ">>> sigmoid(0.458)\n",
    "0.61253961344091512\n",
    "Update: Note that the above was mainly intended as a straight one-to-one translation of the given expression into Python code. It is not tested or known to be a numerically sound implementation. If you know you need a very robust implementation, I'm sure there are others where people have actually given this problem some thought.\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "question = 'how to write sigmoid function in python?'\n",
    "\n",
    "inputs = tokenizer(question + ' ' + raw_text, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print('- {}\\n- {}-{}'.format(sen, loss, torch.sigmoid(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 2.3961355686187744-tensor([[0.7292, 0.2125]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "\n",
    "raw_text = '''\n",
    "How to calculate a logistic sigmoid function in Python\n",
    "The logistic sigmoid function defined as (1/(1 + e^-x)) takes an input x of any real number and returns an output value in the range of -1 and 1.\n",
    "\n",
    "DEFINE A LOGISTIC SIGMOID FUNCTION\n",
    "Define a new logistic sigmoid function that takes input x and returns 1/(1 + math.exp(-x)).\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "print(sigmoid(0.5))\n",
    "OUTPUT\n",
    "0.6224593312018546\n",
    "'''\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "question = 'how to write sigmoid function in python?'\n",
    "\n",
    "inputs = tokenizer(question + ' ' + raw_text, return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, logits = outputs[:2]\n",
    "print('- {}-{}'.format(loss, torch.sigmoid(logits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quang",
   "language": "python",
   "name": "quang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
